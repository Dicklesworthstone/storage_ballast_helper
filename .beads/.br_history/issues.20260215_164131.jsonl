{"id":"bd-112","title":"Multi-channel notification system","description":"## Deliverable\nConfigurable notification system that alerts users through multiple channels when pressure levels change, predictions trigger, or major cleanups happen.\n\n## Notification Channels\n1. Desktop: notify-send (Linux), osascript (macOS) — for interactive users\n2. File: append to /var/lib/sbh/notifications.jsonl — for agents to watch\n3. Journal: systemd journal with structured fields (PRIORITY, SBH_EVENT, SBH_MOUNT)\n4. Webhook: HTTP POST to configurable URL — for Slack/Teams/PagerDuty (simple curl, no deps)\n\n## Configuration\n```toml\n[notifications]\nenabled = true\nchannels = [\"journal\", \"file\"]  # which channels to use\n\n[notifications.desktop]\nenabled = false                  # opt-in (requires display server)\nmin_level = \"orange\"             # only notify at orange+ pressure\n\n[notifications.webhook]\nenabled = false\nurl = \"https://hooks.slack.com/services/...\"\nmin_level = \"red\"\ntemplate = '{\"text\": \"sbh: ${MOUNT} at ${FREE_PCT}% free (${LEVEL})\"}'\n\n[notifications.file]\npath = \"/var/lib/sbh/notifications.jsonl\"\n```\n\n## Notification Events\n- Pressure level change (any transition)\n- Predictive early warning (from predictive action pipeline)\n- Major cleanup completed (>10 items or >10 GB freed)\n- Ballast released or replenished\n- Daemon startup/shutdown\n- Error (circuit breaker, permission denied, etc.)\n\n## Design Rationale\nUsers need to know sbh is working. Without notifications, sbh is invisible — users don't know if it's running, if it prevented a problem, or if it needs attention. The multi-channel approach means every deployment scenario is covered: desktop for dev machines, journal for servers, webhook for team alerting.\n\n## Acceptance Criteria\n- All 4 channels implemented and configurable\n- Desktop notifications work on Linux (notify-send) and macOS (osascript)\n- File notifications are append-only JSONL (same as activity log)\n- Webhook uses simple HTTP POST (no external deps beyond std)\n- Min-level filtering prevents notification spam\n- Notifications include structured data (mount, level, free_pct, action taken)\n- Daemon sends startup notification (\"sbh monitoring 4 volumes\")\n- Unit tests for each channel (mock HTTP for webhook)\n- Integration test: trigger pressure -> verify notification on all channels","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:33:48.866465053Z","created_by":"ubuntu","updated_at":"2026-02-15T00:21:35.564205613Z","closed_at":"2026-02-15T00:21:35.564124461Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon"],"dependencies":[{"issue_id":"bd-112","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T18:34:33.364769120Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-112","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T18:34:33.281206778Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":33,"issue_id":"bd-112","author":"Dicklesworthstone","text":"REVIEW: Webhook HTTPS implementation — std::net::TcpStream only supports plain HTTP. Options: (1) Shell out to curl (available on all target platforms, zero deps). (2) Add ureq as optional dependency behind 'webhook' feature flag. (3) For MVP, document that webhook URL must be plain HTTP or behind a local reverse proxy. Recommendation: use curl shellout with 10s timeout. curl is universally available, handles HTTPS/TLS natively, and avoids adding a TLS dependency to sbh. Implementation: std::process::Command::new('curl').args(['-s', '-X', 'POST', '-H', 'Content-Type: application/json', '-d', payload, '--max-time', '10', url]).spawn() — fire-and-forget, don't block the daemon.","created_at":"2026-02-14T18:54:01Z"},{"id":36,"issue_id":"bd-112","author":"Dicklesworthstone","text":"REVIEW: Test coverage requirements: (1) Unit test: each channel sends correctly (mock notify-send, mock curl, verify file JSONL format, verify journal structured fields). (2) Unit test: min_level filtering — Orange event with min_level=Red should NOT trigger notification. (3) Integration test: trigger pressure transition -> verify notification appears on all configured channels. (4) Test: webhook timeout (mock server with 15s delay) -> verify daemon is not blocked (fire-and-forget). (5) Test: channel failure (notify-send not found) -> graceful fallback, no daemon crash.","created_at":"2026-02-14T18:54:18Z"},{"id":58,"issue_id":"bd-112","author":"Dicklesworthstone","text":"REVIEW-2: Event subscription path — NotificationSender receives events by subscribing to the ActivityEvent channel (from bd-1gm). The logger thread forwards events matching notification min_level to the NotificationSender via a separate bounded channel. NotificationSender runs in its own thread, consuming events and dispatching to configured channels (notify-send, curl, file append, journal). bd-112 does not need a hard dep on bd-48o — it is wired in at startup by the daemon initialization code.","created_at":"2026-02-14T19:03:44Z"}]}
{"id":"bd-1bw","title":"Clippy debt burn-down slice (phase 1): stress/decision-plane early lint blockers","description":"Phase-1 non-overlap cleanup for tests/stress_tests.rs and tests/decision_plane_e2e.rs: resolve early/high-signal lint blockers (similar_names, map_unwrap_or/or_fun_call, collapsible_if, key cast cleanup in upper sections, redundant_clone, early needless_collect, and uninlined format args), preserving behavior. Remaining deep-file cast/too_many_lines lint debt will be split into follow-up slices.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:35:47.832396652Z","created_by":"ubuntu","updated_at":"2026-02-15T16:41:31.045540216Z","closed_at":"2026-02-15T16:41:31.045469002Z","close_reason":"Completed: phase-1 stress/decision-plane test clippy slice resolved and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":115,"issue_id":"bd-1bw","author":"Dicklesworthstone","text":"Completed phase-1 non-overlap lint cleanup for stress/decision-plane test files.\\n\\nFiles:\\n- tests/stress_tests.rs\\n- tests/decision_plane_e2e.rs\\n\\nWhat was fixed in this phase:\\n- Converted multiple unsafe/lossy cast patterns to checked/index-safe forms and integer math where appropriate.\\n- Replaced map+unwrap_or patterns with map_or_else variants to satisfy clippy.\\n- Collapsed nested if blocks and removed needless collects/redundant clones.\\n- Reworked deterministic RNG float generation in decision_plane_e2e to avoid cast-precision lints (bit-construction approach).\\n- Resolved similar_names/uninlined_format_args findings.\\n- Added targeted  on long scenario tests where splitting would reduce scenario trace readability.\\n\\nValidation:\\n- rch exec \"cargo clippy --test stress_tests --test decision_plane_e2e -- -D warnings\" ✅\\n- rch exec \"cargo test --test stress_tests\" ✅ (12 passed)\\n- rch exec \"cargo test --test decision_plane_e2e\" ✅ (7 passed)\\n- rch exec \"cargo check --all-targets\" ✅\\n- cargo fmt --check ✅\\n- rch exec \"cargo clippy --all-targets -- -D warnings\" ❌ (remaining repo-wide backlog outside this phase slice)","created_at":"2026-02-15T16:41:27Z"}]}
{"id":"bd-1fr","title":"sbh clean command: manual cleanup with confirmation and dry-run","description":"## Deliverable\nCommand to manually trigger cleanup of build artifacts, with interactive confirmation, dry-run mode, and configurable aggressiveness.\n\n## Technical Approach\n### Usage\n```bash\nsbh clean                     # Interactive cleanup with confirmation\nsbh clean --yes               # Skip confirmation (for automation)\nsbh clean --dry-run            # Show what would be deleted\nsbh clean --min-score 0.8     # Only delete high-confidence candidates\nsbh clean --max-items 10      # Delete at most 10 items\nsbh clean --max-size 50G      # Free at most 50 GB\nsbh clean --target-free 20    # Clean until 20% free space achieved\nsbh clean /tmp                # Clean only specific path\nsbh clean --json              # Machine-readable output\n```\n\n### Interactive Confirmation\n```\nThe following items will be deleted:\n\n  1. /tmp/cargo-target-quietwillow-mvcc (4.2 GB, score 0.94, 6h old)\n  2. /data/projects/pi/.target_opus_main (8.1 GB, score 0.91, 4h old)\n  3. /data/tmp/cargo-target (12.4 GB, score 0.89, 3h old)\n\nTotal: 3 items, 24.7 GB\n\nProceed with deletion? [y/N/a(ll)/s(kip)/q(uit)]\n  y - delete this item and continue\n  n - skip this item and continue\n  a - delete all remaining items without asking\n  s - skip all remaining items\n  q - quit without deleting anything\n```\n\n### Automation Mode (--yes)\nFor use by agents and scripts: skip confirmation, delete everything above threshold.\n\n### Dry-Run Mode\nComplete pipeline except actual deletion. Produces full report of what would be done.\n\n### Stop Conditions\nCleanup stops when ANY of these are met:\n- All candidates exhausted\n- --max-items reached\n- --max-size of bytes freed\n- --target-free percentage achieved\n- User quits interactive mode\n\n## Acceptance Criteria\n- Interactive mode correctly handles all responses (y/n/a/s/q)\n- --yes mode works without TTY (for automation)\n- --dry-run produces same report as real run\n- Stop conditions correctly enforced\n- All deletions logged to activity database\n- Post-cleanup summary shows actual bytes freed\n- Error handling: failed deletion doesn't stop the process\n- Unit tests for interaction logic\n- Integration test: create test artifacts → clean → verify deleted","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:53:19.894435037Z","created_by":"ubuntu","updated_at":"2026-02-14T21:50:56.174426632Z","closed_at":"2026-02-14T21:50:56.174359386Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-1fr","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T16:56:13.708088745Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fr","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T19:02:37.000629087Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fr","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:13.786979433Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":43,"issue_id":"bd-1fr","author":"Dicklesworthstone","text":"MINOR: sbh clean should respect the protection system (bd-3qm). When displaying candidates, protected directories should appear with [PROTECTED] label and be excluded from deletion. The clean command shares the walker (bd-1w9) which already integrates protection, so this happens automatically — but the clean command's output should explicitly mention 'N directories protected (use sbh protect --list to see)' when protected dirs are encountered.","created_at":"2026-02-14T18:55:05Z"}]}
{"id":"bd-1gm","title":"Dual-write coordinator: atomic SQLite + JSONL logging","description":"## Deliverable\nCoordinator that writes every event to BOTH SQLite and JSONL, with graceful degradation if either backend fails.\n\n## Technical Approach\n### ActivityLogger\n```rust\npub struct ActivityLogger {\n    sqlite: Option<SqliteLogger>,    // None if SQLite unavailable\n    jsonl: JsonlWriter,              // always available (fallback to stderr)\n    event_count: AtomicU64,\n    bytes_logged: AtomicU64,\n}\n\nimpl ActivityLogger {\n    pub fn log_event(&self, event: ActivityEvent) -> Result<(), SbhError>;\n    pub fn log_deletion(&self, deletion: &DeletionReport) -> Result<(), SbhError>;\n    pub fn log_pressure_change(&self, from: PressureLevel, to: PressureLevel, reading: &PressureReading) -> Result<(), SbhError>;\n    pub fn log_ballast_action(&self, action: &BallastAction) -> Result<(), SbhError>;\n}\n```\n\n### Atomic Write Strategy\n1. Write to JSONL first (always succeeds unless disk is truly full)\n2. Write to SQLite second\n3. If SQLite fails: log warning, continue JSONL-only, attempt SQLite reconnect periodically\n4. If JSONL fails: log to stderr, attempt JSONL reopen\n5. If BOTH fail: log to stderr only (last resort)\n\n### Event Types\n```rust\npub enum ActivityEvent {\n    DaemonStarted { version: String, config_hash: String },\n    DaemonStopped { reason: String, uptime: Duration },\n    PressureChanged { from: PressureLevel, to: PressureLevel, reading: PressureReading },\n    BallastReleased { files: Vec<PathBuf>, bytes_freed: u64 },\n    BallastReplenished { files: Vec<PathBuf>, bytes_used: u64 },\n    ArtifactDeleted { path: PathBuf, size: u64, score: f64, factors: ScoreFactors },\n    ArtifactDeletionFailed { path: PathBuf, error: String },\n    ScanCompleted { paths_scanned: usize, candidates_found: usize, duration: Duration },\n    SpecialLocationAlert { path: PathBuf, free_pct: f64, threshold: f64 },\n    ConfigReloaded { changes: Vec<String> },\n    Error { code: String, message: String, context: String },\n}\n```\n\n### Thread Safety\nThe logger must be safely shareable across the monitoring loop, scanner, and deletion executor threads. Use Arc<ActivityLogger> with internal synchronization (Mutex for SQLite, atomic file append for JSONL).\n\n## Acceptance Criteria\n- Every event written to both backends (when both available)\n- SQLite failure doesn't stop JSONL logging\n- JSONL failure doesn't stop SQLite logging\n- Both fail → stderr fallback\n- Thread-safe concurrent logging from multiple components\n- Event serialization is consistent between SQLite and JSONL\n- Unit tests for all degradation modes\n- Integration test: simulate SQLite failure → verify JSONL continues","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:49:49.429565237Z","created_by":"ubuntu","updated_at":"2026-02-14T19:54:05.614287017Z","closed_at":"2026-02-14T19:54:05.614265587Z","close_reason":"Dual-write coordinator implemented in src/logger/dual.rs: ActivityLoggerHandle with bounded crossbeam channel (try_send non-blocking), dedicated logger thread owning SQLite+JSONL, ActivityEvent enum with all event types, event-to-LogEntry/ActivityRow/PressureRow conversions, graceful degradation (3 SQLite failures→disable), dropped-events counter, graceful shutdown, 5 passing tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["logger"],"dependencies":[{"issue_id":"bd-1gm","depends_on_id":"bd-2f8","type":"blocks","created_at":"2026-02-14T16:55:55.078636015Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1gm","depends_on_id":"bd-394","type":"blocks","created_at":"2026-02-14T16:55:55.158723173Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":15,"issue_id":"bd-1gm","author":"Dicklesworthstone","text":"REVIEW: The logger's thread-safety design is critical. Key decisions:\n\n1. NEVER block the monitoring loop for logging. Use try_send() on the logger channel, not send(). If the channel is full (1024 events buffered), DROP the event and increment a dropped_events counter. Log a warning about dropped events at the next successful write.\n\n2. SQLite access must be serialized (single writer). Use a dedicated logger thread that owns the Connection. All other threads send ActivityEvent through the channel. The logger thread is the ONLY thread that touches SQLite.\n\n3. JSONL writes use Mutex<BufWriter<File>> — but since all writes go through the logger thread, this is mostly for the flush() call which might happen from the shutdown sequence on a different thread.\n\n4. The ActivityLogger exposed to other threads is an Arc<ActivityLoggerHandle> that contains just the crossbeam Sender. It's Clone and Send, so any thread can log events.","created_at":"2026-02-14T17:15:16Z"},{"id":57,"issue_id":"bd-1gm","author":"Dicklesworthstone","text":"REVIEW-2: API clarification — the public API is ActivityLoggerHandle with a single method: send(event: ActivityEvent). Internally it calls try_send() on a crossbeam bounded channel. The ActivityLogger struct (with SQLite connection + BufWriter) is PRIVATE to the logger thread. The description's log_event/log_deletion/log_pressure_change are convenience constructors for ActivityEvent variants, NOT methods on the logger itself. E.g.: fn log_deletion(handle: &ActivityLoggerHandle, report: &DeletionReport) { handle.send(ActivityEvent::ArtifactDeleted { ... }); }","created_at":"2026-02-14T19:03:37Z"}]}
{"id":"bd-1hh","title":"Ranked deletion executor with safety checks and dry-run mode","description":"## Deliverable\nThe component that takes ranked deletion candidates from the scoring engine and actually deletes them, with comprehensive safety checks, dry-run mode, and detailed logging.\n\n## Technical Approach\n### Deletion Pipeline\n```\nScored candidates → Sort by score desc → Apply threshold → Safety pre-flight\n→ Delete batch → Log results → Re-measure pressure → Decide continue/stop\n```\n\n### DeletionExecutor\n```rust\npub struct DeletionExecutor {\n    config: ScannerConfig,\n    logger: Arc<ActivityLogger>,\n    dry_run: bool,\n}\n\npub struct DeletionPlan {\n    pub candidates: Vec<CandidacyScore>,\n    pub total_reclaimable_bytes: u64,\n    pub estimated_items: usize,\n    pub pressure_before: PressureReading,\n}\n\npub struct DeletionReport {\n    pub items_deleted: usize,\n    pub items_failed: usize,\n    pub items_skipped: usize,\n    pub bytes_freed: u64,\n    pub duration: Duration,\n    pub errors: Vec<DeletionError>,\n    pub pressure_after: PressureReading,\n}\n\npub struct DeletionError {\n    pub path: PathBuf,\n    pub error: SbhError,\n    pub recoverable: bool,\n}\n```\n\n### Safety Pre-Flight Checks (before each deletion)\n1. Re-verify the path still exists (might have been cleaned by another process)\n2. Re-check that the path is not currently open by any process (Linux: check /proc/*/fd)\n3. Re-verify age hasn't changed (file might have been touched/rebuilt since scanning)\n4. Verify we have write permission to the parent directory\n5. If directory, check it doesn't contain .git/ (final safety net)\n\n### Batch Deletion Strategy\nDon't delete everything at once:\n- Delete in batches of max_delete_batch (from PressureResponse, typically 5-20)\n- After each batch, re-measure free space\n- If pressure has dropped below threshold, STOP deleting\n- This prevents over-deletion when the first few deletions free enough space\n\n### Dry-Run Mode\nWhen dry_run=true:\n- Run entire pipeline including scoring and ranking\n- Log what WOULD be deleted with full details\n- Don't actually delete anything\n- Useful for testing and tuning scoring weights\n\n### Deletion Method\nFor directories (most common): rm_dir_all equivalent\nFor files: std::fs::remove_file\nBoth with error handling that continues on failure (log and move to next candidate)\n\n### Circuit Breaker (Alien Graveyard: resilience pattern)\nIf more than 3 consecutive deletions fail:\n- Pause deletion for 30 seconds\n- Log circuit breaker activation\n- After cooldown, retry with just 1 item\n- If that also fails, escalate to error reporting and stop\n\nThis prevents thrashing when, e.g., a filesystem goes read-only.\n\n### Post-Deletion Verification\nAfter deleting a batch:\n- Verify the paths actually disappeared (paranoia check)\n- Re-measure free space to confirm we gained what we expected\n- Log discrepancy if freed bytes don't match expectations (CoW filesystem?)\n\n## Acceptance Criteria\n- Candidates deleted in score-descending order (most obvious artifacts first)\n- Pre-flight checks prevent deletion of in-use or critical files\n- Batch strategy prevents over-deletion\n- Dry-run mode produces identical scoring without deleting\n- Circuit breaker activates on consecutive failures\n- All deletions logged with full context (path, size, score, reason)\n- Post-deletion verification confirms space was freed\n- Error recovery: single failed deletion doesn't stop the process\n- Unit tests with temp directory trees\n- Integration test: create artifacts → score → delete → verify space freed","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:48:46.218524146Z","created_by":"ubuntu","updated_at":"2026-02-14T20:03:33.648777443Z","closed_at":"2026-02-14T20:03:33.648754049Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["scanner"],"dependencies":[{"issue_id":"bd-1hh","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T16:55:49.754705322Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hh","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T16:55:49.671816719Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":11,"issue_id":"bd-1hh","author":"Dicklesworthstone","text":"REVIEW: The open-file check (checking if a file/directory is in use by a process) is one of the most important safety mechanisms. Implementation detail:\n\nLinux: Read /proc/*/fd symlinks. For each process, readlink() each fd. If any fd points to a path under the candidate directory, the candidate is \"open\" and gets a hard veto.\n\nPERFORMANCE CONCERN: Scanning /proc/*/fd for every deletion candidate is O(processes × fd_per_process). On a machine with 100+ agent processes, each with 100+ open fds, this is 10K+ readlink() calls per candidate. Mitigations:\n1. Cache the open-file set: collect ALL open paths once per scan cycle, store in a HashSet, then do O(1) lookups\n2. Use /proc/<pid>/maps as a secondary check (maps shows memory-mapped files)\n3. Rate-limit: only refresh the open-file cache every N seconds (default 5s)\n4. Under extreme pressure (Critical), SKIP the open-file check for non-.git, non-system paths (the daemon itself can't afford to scan /proc when the disk is about to fill up)\n\nThe is_open flag is computed by the CALLER (main loop or CLI clean command) and passed to the scoring engine as part of ScoringInput. The scoring engine itself never touches /proc.","created_at":"2026-02-14T17:14:24Z"},{"id":37,"issue_id":"bd-1hh","author":"Dicklesworthstone","text":"REVIEW: Clarification on /proc/*/fd open-file check vs scorer is_open flag: The scorer (bd-x9z) takes is_open: bool as input. The WALKER (bd-1w9) is responsible for checking /proc/*/fd during traversal and setting this flag on ScoringInput. The deletion executor (bd-1hh) does a SECOND /proc check as a pre-flight safety re-verification — this is NOT redundant because time may have passed between scoring and deletion (the candidate might have been opened since scoring). The re-check is a last-moment safety net. Keep it.","created_at":"2026-02-14T18:54:26Z"}]}
{"id":"bd-1k0","title":"Offline bundle preflight bypasses sigstore verification","description":"run_bundle_preflight and update verification pass SigstorePolicy::Disabled even when sigstore bundle metadata exists, leaving checksum-in-bundle as sole trust root.","status":"open","priority":1,"issue_type":"bug","created_at":"2026-02-15T16:39:57.229617848Z","created_by":"ubuntu","updated_at":"2026-02-15T16:39:57.229617848Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","security","supply-chain"]}
{"id":"bd-1kn","title":"Project scaffolding: Cargo.toml, workspace structure, rust-toolchain.toml","description":"Set up the complete Rust project structure for storage_ballast_helper (sbh).\n\n## Technical Approach\n- Create Cargo.toml with edition = \"2024\", binary name = \"sbh\"\n- Create rust-toolchain.toml targeting STABLE (edition 2024 is stable since Rust 1.85, Jan 2025)\n- Set up directory structure: src/{main.rs, lib.rs, cli/, core/, monitor/, ballast/, scanner/, logger/, daemon/, platform/}\n- Configure release profile for size optimization: opt-level=\"z\", lto=true, codegen-units=1, panic=\"abort\", strip=true\n- Add #![forbid(unsafe_code)] to lib.rs\n- Enable pedantic + nursery clippy lints\n\n## Key Dependencies (initial)\n- clap (derive) for CLI\n- serde + serde_json for serialization\n- rusqlite (bundled) for SQLite logging\n- toml for configuration parsing\n- thiserror for error derives\n- colored for terminal output\n- chrono for timestamps\n- nix (Linux) / core-foundation (macOS) for platform-specific ops\n- parking_lot for efficient locks\n- crossbeam-channel for thread communication\n- memchr for fast byte scanning\n- regex for artifact pattern matching (bd-1sw)\n- signal-hook for safe signal handling (bd-2s9)\n- rand for ballast file random data generation (bd-25g)\n\n## Design Rationale\nFollowing asupersync's pattern: NO tokio dependency. Use std threads + crossbeam channels for the daemon monitoring loop. This keeps the binary small, startup fast, and avoids async runtime complexity for what is fundamentally a polling-based system daemon.\n\n## Acceptance Criteria\n- `cargo check` passes with zero warnings\n- `cargo clippy --all-targets -- -D warnings` passes\n- Binary compiles to < 5MB release size\n- Project structure mirrors the modular architecture described in the design","acceptance_criteria":"1. Cargo workspace and module layout compile cleanly with Rust 2024 toolchain and strict lint gates. 2. Baseline test harness is scaffolded: unit-test module structure plus e2e script entrypoint with verbose logging hooks. 3. Build profiles and forbid-unsafe policy are enforced in root configuration. 4. Core dependency set supports scanner/daemon/CLI architecture without placeholder shims. 5. Foundation docs describe how new modules register tests and structured logs.","notes":"Progress update: implemented crate-level unsafe forbids, baseline integration harness (tests/common + tests/integration_tests), executable scripts/e2e_test.sh with verbose case logs, and docs/testing-and-logging.md. Verified: cargo fmt --check, cargo check --all-targets, cargo test --test integration_tests, ./scripts/e2e_test.sh. Status remains in_progress until repository-wide clippy -D warnings debt (pre-existing across multiple modules) is resolved.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:43:36.253025401Z","created_by":"ubuntu","updated_at":"2026-02-14T19:46:06.022861156Z","closed_at":"2026-02-14T19:46:06.022841960Z","close_reason":"Complete","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"],"comments":[{"id":3,"issue_id":"bd-1kn","author":"Dicklesworthstone","text":"REVIEW: Fixed two issues: (1) Added missing dependencies: regex (for bd-1sw pattern matching), signal-hook (for bd-2s9 signal handling), rand (for bd-25g ballast random data). These were referenced by downstream beads but not listed in the scaffolding bead's dep list. (2) Changed 'targeting nightly' to 'targeting STABLE' — edition 2024 has been stable since Rust 1.85 (Jan 2025). No reason to use nightly.","created_at":"2026-02-14T17:12:35Z"},{"id":18,"issue_id":"bd-1kn","author":"Dicklesworthstone","text":"=== BEAD REVIEW SESSION COMPLETE (2026-02-14) ===\n\nSummary of all revisions made across the 36-bead graph:\n\nSTRUCTURAL FIX:\n- Removed bd-x9z → bd-2pj dependency (scoring engine is pure computation, shouldn't depend on fs stats collector)\n\nBEAD ENHANCEMENTS (via comments with implementation guidance):\n- bd-1kn: Added missing deps (regex, signal-hook, rand). Fixed edition 2024 → stable channel\n- bd-x9z: Full description rewrite. Established pure-computation contract (zero I/O). Added ScoringInput struct with explicit now timestamp and is_open flag for determinism\n- bd-1w9: Walker must collect structural_markers during traversal for scoring engine\n- bd-25g: Added fallocate() optimization for ext4/xfs (instant vs 25-50s). Added file locking for concurrent daemon/CLI access\n- bd-48o: Added bounded channel sizing, backpressure strategy, thread panic recovery (3 strikes in 5 min → stop respawning)\n- bd-7ls: Clarified role: cross-cutting test infrastructure + coverage verification + gap-filling (not duplicate of per-module tests)\n- bd-1sw: Added error handling for invalid custom regex patterns from config\n- bd-394: Strengthened 4-level fallback chain (file → fallback_path → stderr → discard). write_all() for atomic lines\n- bd-2q9: Added 8 more E2E test cases (signals, concurrency, determinism, perf)\n- bd-2s9: Noted signal-hook crate dependency in Cargo.toml\n- bd-1hh: Added /proc/*/fd open-file check performance analysis and caching strategy\n- bd-3qw: Added PID controller config section and special locations config section\n- bd-2np: Defined state.json format, atomic writes, staleness detection\n- bd-224: Added cross-mount ballast edge case (ballast must be on same FS as pressure)\n- bd-1gm: Added try_send() non-blocking logging design, dedicated logger thread owns SQLite\n- bd-26g: Fixed ReadWritePaths conflict with ProtectHome=read-only\n- bd-2f8: Added startup check that SQLite db is NOT on monitored filesystem\n\nGRAPH HEALTH (post-review):\n- 36 nodes, 64 edges, 0 cycles\n- Critical path length: 11 (unchanged)\n- 1 ready bead (bd-1kn)\n- 4 articulation points\n- bd-48o remains the key bottleneck (unavoidable — it's the integration point)","created_at":"2026-02-14T17:16:49Z"},{"id":32,"issue_id":"bd-1kn","author":"Dicklesworthstone","text":"REVIEW: Additional dependency needed for TUI dashboard (bd-3s5): crossterm crate for terminal manipulation. This is NOT needed in initial scaffolding — add crossterm as an optional dependency gated behind a 'tui' feature flag: [features] tui = ['dep:crossterm', 'dep:unicode-width']. This way the core binary stays small and the dashboard is opt-in at compile time.","created_at":"2026-02-14T18:53:56Z"},{"id":38,"issue_id":"bd-1kn","author":"Dicklesworthstone","text":"MINOR: Time handling convention — use std::time::SystemTime and std::time::Duration throughout the codebase. chrono is in the dependency list but should ONLY be used for user-facing formatting (display, log timestamps, JSON serialization). All internal calculations, comparisons, and storage should use SystemTime/Duration. This avoids chrono's heavier API surface for simple elapsed-time checks.","created_at":"2026-02-14T18:54:41Z"},{"id":54,"issue_id":"bd-1kn","author":"Dicklesworthstone","text":"REVIEW-2: Additional dependency: toml_edit crate for config file modification preserving formatting/comments. Required by bd-7vl (sbh tune --apply). Add as optional dependency behind a feature flag if desired.","created_at":"2026-02-14T19:03:22Z"}]}
{"id":"bd-1q3","title":"Alien decision uplift: uncertainty-aware expected-loss gating in scoring","description":"Use entropy+calibration uncertainty to improve Delete/Review/Keep action quality while preserving deterministic behavior and explainability. Scope limited to src/scanner/scoring.rs with unit tests and rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T04:29:06.297135152Z","created_by":"ubuntu","updated_at":"2026-02-15T06:50:26.223913921Z","closed_at":"2026-02-15T06:42:28.044955217Z","close_reason":"Completed","source_repo":".","compaction_level":0,"original_size":0,"labels":["decision-engine","math","scanner"],"comments":[{"id":99,"issue_id":"bd-1q3","author":"Dicklesworthstone","text":"Completed scoped decision-engine uplift in src/scanner/scoring.rs: uncertainty-aware expected-loss action gating (entropy + calibration), pressure multiplier added to evidence ledger terms, richer ledger summary with loss margin + uncertainty, and focused boundary/uncertainty unit tests. Validation: rch exec \"cargo test --lib scoring\" PASS; rch exec \"cargo check --all-targets\" PASS; cargo fmt --check PASS. rch exec \"cargo clippy --all-targets -- -D warnings\" still fails with broad pre-existing lint debt outside this slice.","created_at":"2026-02-15T06:42:25Z"},{"id":100,"issue_id":"bd-1q3","author":"WildBeacon","text":"Follow-up refinement in src/scanner/scoring.rs: added uncertainty_adjusted_losses(), routed decision expected losses through uncertainty-aware adjustment, widened review band and posterior thresholding under uncertainty, expanded evidence ledger with base-vs-adjusted losses + calibration/uncertainty terms, and added unit test uncertainty_adjustment_penalizes_delete_loss_more_than_keep_loss. Validation: cargo fmt --check PASS; rch exec 'cargo test --lib scoring' PASS (12); rch exec 'cargo check --all-targets' PASS; rch exec 'cargo clippy --lib -- -D warnings' blocked by pre-existing unrelated lint in src/monitor/fs_stats.rs.","created_at":"2026-02-15T06:50:26Z"}]}
{"id":"bd-1sw","title":"Build artifact pattern registry with heuristic classification rules","description":"## Deliverable\nA comprehensive, extensible registry of filename/path patterns that identify build artifacts, temporary files, and cache directories across all major build systems.\n\n## Technical Approach\n### Pattern Categories (from real-world agent swarm data)\n\n#### Rust/Cargo Patterns (highest priority - this is the primary use case)\nMatching patterns observed across real agent sessions:\n- `target/` - standard Cargo target directory\n- `.target*` - hidden target variants (.target_boldmeadow_auth3, .target-topaz, .target-opus)\n- `_target_*` - underscore-prefixed (_target_opus_main, _target_codex_deepdive)\n- `.tmp_target*` - temp targets (.tmp_target_codex_or, .tmp_target_pearlgorge)\n- `cargo-target-*` - named cargo targets (cargo-target-quietwillow-mvcc, cargo-target-pearl)\n- `cargo_*` - alternate cargo patterns (cargo_opus_prime)\n- `*-target` - suffix targets (jadefinch-target, scarletwaterfall-target)\n- `target-*` - prefix targets (target-ubuntu-am, target-maroon)\n- `.tmp_cargo_home_*` - temp cargo homes\n- `.tmp-codex-*` - codex temp dirs\n- `.tmp-pijs-*` - pi_agent temp dirs\n- `.tmp-ext-*` - extension test dirs\n- `pi_agent_*` - pi_agent variants (pi_agent_cyanrobin, pi_agent_rust_opus)\n- `pi_target_*` - pi_target variants\n- `pi_opus_*` - pi_opus variants\n- `.tmp-opt-target*` - optimization test targets\n- `br-build*` - beads_rust build dirs\n- `cass-target*` - cass build dirs\n\n**Structural indicators within directories:**\n- Contains `incremental/` → almost certainly a Rust target dir\n- Contains `deps/` + `build/` → very likely Rust target\n- Contains `.fingerprint/` → definitely Rust target\n- Contains `release/` or `debug/` with .d files → Rust target\n\n#### Node.js Patterns\n- `node_modules/`\n- `.next/` (Next.js build)\n- `dist/`\n- `.cache/` (various build tools)\n- `.parcel-cache/`\n\n#### Python Patterns\n- `__pycache__/`\n- `.venv/` / `venv/`\n- `*.egg-info/`\n- `.mypy_cache/`\n- `.pytest_cache/`\n\n#### General Build Patterns\n- `build/` (many languages)\n- `.build/`\n- `out/`\n- `.gradle/`\n- `.tox/`\n\n#### Cache Patterns\n- `~/.cache/go-build`\n- `~/.cache/pip`\n- `~/.cache/huggingface`\n- `~/.cache/actcache`\n\n### Pattern Registry Implementation\n```rust\npub struct ArtifactPattern {\n    pub name: &'static str,\n    pub description: &'static str,\n    pub name_regex: Regex,\n    pub structural_markers: Vec<&'static str>,  // subdirs that confirm classification\n    pub confidence: f64,         // 0.0-1.0 base confidence\n    pub category: ArtifactCategory,\n    pub typical_size_range: (u64, u64),  // expected size range for plausibility\n}\n\npub enum ArtifactCategory {\n    RustTarget,     // highest confidence\n    NodeModules,\n    PythonCache,\n    BuildOutput,\n    CacheDir,\n    TempDir,\n    AgentWorkspace, // agent-specific temp builds\n    Unknown,\n}\n```\n\n### Matching Algorithm\nFor each discovered directory:\n1. Check name against all patterns (regex match)\n2. If name matches, check structural markers (does it contain incremental/, deps/, etc.)\n3. Combine name confidence with structural confidence\n4. Return ArtifactClassification with overall confidence\n\n```rust\npub struct ArtifactClassification {\n    pub pattern_name: &'static str,\n    pub category: ArtifactCategory,\n    pub name_confidence: f64,\n    pub structural_confidence: f64,\n    pub combined_confidence: f64,\n}\n```\n\n### Extensibility\nUsers can add custom patterns via config:\n```toml\n[[scanner.custom_patterns]]\nname = \"my-tool-cache\"\nregex = \"^\\\\.my-tool-cache$\"\nconfidence = 0.8\ncategory = \"CacheDir\"\n```\n\n## Design Rationale\nThe pattern list is derived from REAL data collected from weeks of agent swarm operation. The naming conventions are diverse and creative (agents name targets after themselves: boldmeadow, quietwillow, etc.), so we need comprehensive regex patterns. Structural markers add a second layer of verification: even if a directory is named \"target\", we check if it LOOKS like a Rust target dir inside. This prevents false positives.\n\n## Acceptance Criteria\n- All observed agent naming patterns from the specification are covered\n- Structural markers correctly identify Rust target directories\n- No false positives on common directory names (e.g., a user's \"target\" document folder)\n- Custom patterns loadable from config\n- Confidence scores reflect actual classification quality\n- Unit tests for every pattern category\n- Unit test: directory named \"target\" but containing only documents → low confidence\n- Unit test: directory named \".target_opus_main\" with incremental/ inside → high confidence","acceptance_criteria":"1. Deterministic pattern classification for identical inputs across all built-in and custom rules. 2. Unit tests cover positive/negative fixtures, precedence, and confidence calibration boundaries. 3. Integration tests validate handoff into walker/scorer without false category drift. 4. E2E scenarios exercise real artifact trees and verify expected matched categories. 5. Detailed match-trace logging includes pattern id, confidence, and suppression/veto reasons.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:47:37.220914613Z","created_by":"ubuntu","updated_at":"2026-02-14T19:46:14.652852740Z","closed_at":"2026-02-14T19:46:14.652831130Z","close_reason":"Implemented in scanner/patterns.rs","source_repo":".","compaction_level":0,"original_size":0,"labels":["scanner"],"dependencies":[{"issue_id":"bd-1sw","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T16:55:49.348276443Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1sw","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T19:02:40.033459397Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":7,"issue_id":"bd-1sw","author":"Dicklesworthstone","text":"REVIEW: Missing error handling for user-supplied custom regex patterns:\n\nWhen a user adds a custom pattern in config:\n  [[scanner.custom_patterns]]\n  name = \"my-cache\"\n  regex = \"[invalid(regex\"  # <-- broken regex\n\nThe pattern registry must:\n1. Validate each custom regex at config load time using Regex::new()\n2. If invalid: emit SBH-4001 error with the regex string, position of syntax error, and suggestion\n3. NEVER crash — skip the invalid pattern and log a warning\n4. In strict mode (sbh config validate): report all invalid patterns as errors\n5. In normal operation: silently skip invalid patterns but log them to JSONL\n\nThis follows the \"don't let user config mistakes crash the daemon\" principle. A typo in one custom pattern should not prevent sbh from monitoring with all its built-in patterns.","created_at":"2026-02-14T17:13:31Z"}]}
{"id":"bd-1td","title":"Special location registry for RAM-backed and critical filesystems","description":"## Deliverable\nA registry of special filesystem locations (tmpfs, /dev/shm, RAM-backed mounts) that require more aggressive monitoring and lower free-space thresholds.\n\n## Technical Approach\n### SpecialLocationRegistry\n```rust\npub struct SpecialLocationRegistry {\n    locations: Vec<SpecialLocation>,\n}\n\npub struct SpecialLocation {\n    pub path: PathBuf,\n    pub kind: SpecialKind,\n    pub buffer_pct: f64,          // minimum free % to maintain (default 15%)\n    pub scan_interval: Duration,   // how often to check (default 5s)\n    pub priority: u8,              // higher = more important to keep free\n}\n\npub enum SpecialKind {\n    Tmpfs,          // /tmp on most Linux systems\n    DevShm,         // /dev/shm - POSIX shared memory\n    RunShm,         // /run/shm - some distros\n    Ramfs,          // true RAM filesystem (no swap)\n    UserTmp,        // /data/tmp or similar user-managed temp\n    Custom(String), // user-configured special locations\n}\n```\n\n### Auto-Discovery\nOn startup, scan mount points (via PAL) to find:\n- All tmpfs mounts\n- /dev/shm (always special on Linux)\n- Any mount with fs_type == \"ramfs\"\n- User-configured locations from config\n\n### Why Special Locations Matter\nRAM-backed filesystems are bounded by physical memory. When /tmp (tmpfs) fills up:\n- Build processes fail with \"No space left on device\"\n- System services that use /tmp start failing\n- The machine becomes progressively more unstable\n- Unlike disk, there's no extra buffer - it's literally eating RAM\n\nThis is WORSE than disk filling because RAM pressure causes OOM kills and swap thrashing, which is exactly the scenario sbh exists to prevent.\n\n### Monitoring Strategy\nSpecial locations get:\n- 3-6x higher scan frequency than disk-backed locations\n- Lower buffer thresholds (15% free minimum vs 5% for disk)\n- Priority ballast release when they fill up\n- Immediate logging when buffer threshold is breached\n- RAM-backed locations report both their own free% AND system memory impact\n\n## Acceptance Criteria\n- Auto-discovers all tmpfs, devshm, ramfs mounts on Linux\n- Correctly classifies each special location\n- Configurable buffer_pct per location\n- Scan intervals are shorter for special locations\n- Reports special location pressure separately from disk pressure\n- Unit tests with mock mount point data","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:45:04.924331671Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:22.289462864Z","closed_at":"2026-02-14T19:45:22.289430143Z","close_reason":"Special location registry implemented in monitor/special_locations.rs (210 lines): SpecialLocationRegistry with auto-discovery from mount_points(), SpecialKind enum (Tmpfs/DevShm/Ramfs/UserTmp/Custom), per-location buffer_pct/scan_interval/priority, /tmp fallback, unit tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["monitoring"],"dependencies":[{"issue_id":"bd-1td","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T16:55:39.257225006Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1td","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T16:55:39.173782426Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1w9","title":"Parallel directory walker with cross-device and symlink safety","description":"## Deliverable\nA high-performance parallel directory walker that discovers candidate files/directories for cleanup, with safety guards against crossing filesystem boundaries, following symlinks, or entering protected paths.\n\n## Technical Approach\n### Walker Design\n```rust\npub struct DirectoryWalker {\n    config: WalkerConfig,\n    excluded_paths: HashSet<PathBuf>,\n    seen_inodes: HashSet<u64>,     // detect hardlink loops\n    results_tx: crossbeam::Sender<WalkEntry>,\n}\n\npub struct WalkerConfig {\n    pub root_paths: Vec<PathBuf>,\n    pub max_depth: usize,          // default 10\n    pub follow_symlinks: bool,     // default false (NEVER follow by default)\n    pub cross_devices: bool,       // default false (stay on same filesystem)\n    pub parallelism: usize,        // default num_cpus / 2\n    pub excluded_patterns: Vec<Regex>,\n}\n\npub struct WalkEntry {\n    pub path: PathBuf,\n    pub metadata: EntryMetadata,\n    pub depth: usize,\n}\n\npub struct EntryMetadata {\n    pub size_bytes: u64,           // for dirs: recursive size estimate\n    pub file_type: FileType,\n    pub modified: SystemTime,\n    pub created: Option<SystemTime>,\n    pub is_dir: bool,\n    pub inode: u64,\n    pub device_id: u64,\n    pub permissions: u32,\n}\n```\n\n### Safety Guards\n1. **No symlink following**: Symlinks could lead anywhere, including outside the scan scope. Always use lstat, never stat.\n2. **No cross-device**: Stay on the same filesystem. A target/ dir on /data shouldn't cause us to scan /boot.\n3. **Excluded paths**: Never enter /, /boot, /etc, /usr, /bin, /sbin, /var/log, /proc, /sys, /dev (except /dev/shm)\n4. **Inode dedup**: Detect hardlink cycles via inode tracking\n5. **Permission checking**: Skip directories we can't read (log warning, don't fail)\n6. **Rate limiting**: Under extreme system load, throttle walking to avoid making things worse\n\n### Parallelism Strategy\nUse a work-stealing pattern (inspired by rayon) but with explicit control:\n- Main thread discovers top-level directories\n- Worker threads process subdirectories via crossbeam channel\n- Each worker sends results back via channel\n- Bounded channel prevents unbounded memory growth\n\n### Performance Under Load\nWhen the system is already overloaded (the exact scenario where sbh is needed), the walker must be gentle:\n- Use ionice equivalent (Linux: ioprio_set with IOPRIO_CLASS_IDLE)\n- Yield between directory reads under high load average\n- Limit parallelism based on current load average\n- Skip recursive size calculation when time is critical (use estimate)\n\n### Directory Size Estimation\nFor directories, we need size but du -s is expensive. Two modes:\n- **Estimate mode** (default under pressure): sum of direct children only, extrapolate\n- **Accurate mode** (low pressure): full recursive stat\n\n## Design Rationale\nThe walker is the \"eyes\" of the scanner. It must be thorough but safe. The biggest risk is accidentally scanning a mounted USB drive, NFS share, or critical system directory. The safety guards prevent this. Parallelism speeds up scanning on large project directories but is throttled under load to avoid contributing to the very problem we're solving.\n\n## Acceptance Criteria\n- Discovers all directories/files under root_paths within configured depth\n- Never follows symlinks by default\n- Never crosses filesystem boundaries\n- Never enters excluded paths\n- Correctly handles permission denied (skip, warn, continue)\n- Parallel scanning is faster than sequential for large trees\n- Throttles under high load (verifiable by checking iteration delay)\n- Unit tests with synthetic directory trees\n- Unit test: directory with symlink loop → correctly handled\n- Unit test: cross-device mount → not followed","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:47:02.781065858Z","created_by":"ubuntu","updated_at":"2026-02-14T21:40:35.463883890Z","closed_at":"2026-02-14T21:40:35.463864774Z","close_reason":"Implemented parallel directory walker with crossbeam work-stealing, protection integration, structural signal collection, open-file detection, 14 tests passing","source_repo":".","compaction_level":0,"original_size":0,"labels":["scanner"],"dependencies":[{"issue_id":"bd-1w9","depends_on_id":"bd-3qm","type":"blocks","created_at":"2026-02-14T18:34:24.909933040Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1w9","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T16:55:49.264380785Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":2,"issue_id":"bd-1w9","author":"Dicklesworthstone","text":"REVIEW: The walker must collect structural markers during traversal. When walking a candidate directory, check for the presence of key subdirectories (incremental/, deps/, .fingerprint/, build/, .git/, node_modules/.package-lock.json, etc.) and attach them as Vec<String> to WalkEntry.structural_markers. This data is consumed by the scoring engine (bd-x9z) which is a pure computation and cannot do filesystem I/O. The walker is the only component that touches the filesystem during scanning.","created_at":"2026-02-14T17:12:14Z"},{"id":53,"issue_id":"bd-1w9","author":"Dicklesworthstone","text":"REVIEW-2: Walker must also perform open-file detection for the scorer. During each scan cycle, collect the set of all open file paths from /proc/*/fd into a HashSet<PathBuf> (refreshed every 5s, cached between refreshes). For each WalkEntry, set is_open: bool if any fd in the open-file set points to a path under the candidate. Add is_open: bool to WalkEntry struct. Under Critical pressure, skip this check entirely (too slow when disk is thrashing). On macOS, use lsof equivalent via PAL (bd-sth).","created_at":"2026-02-14T19:03:18Z"}]}
{"id":"bd-21z","title":"Integration test suite: full-pipeline tests with realistic scenarios","description":"## Deliverable\nIntegration tests that exercise the full sbh pipeline from pressure detection through scanning, scoring, cleanup, fallback, and recovery.\n\n## Scope Expansion (Plan-Space Revision)\nThis integration bead now explicitly validates all major feature clusters, including:\n- predictive actions and early-warning pipeline\n- emergency zero-write mode\n- multi-volume ballast coordination\n- project protection markers\n- process attribution and explainability paths\n- decision-plane adaptive/fallback behavior\n\n## Technical Approach\n### Scenario Set\n1. Normal operation with green pressure and no deletion\n2. Gradual pressure buildup with controller escalation\n3. Critical pressure with ballast-first recovery\n4. Special location pressure handling (/tmp/devshm style)\n5. Dry-run pipeline parity (no side effects)\n6. Recovery and ballast replenishment\n7. Concurrent scanner/deletion/logger interactions\n8. Error recovery with circuit-breaker behavior\n9. Predictive action preemption before threshold crossing\n10. Emergency zero-write policy path and rollback to normal mode\n11. Protected project markers preventing destructive actions\n12. Multi-volume pressure balancing and action allocation\n\n### Detailed Logging Contract\nEach scenario must assert both behavior and logs:\n- pressure transitions with timestamps\n- selected actions and reasons\n- fallback activations and recovery criteria\n- reclaimed bytes, skipped candidates, and veto reasons\n- trace IDs linking to decision/evidence records\n\n## Acceptance Criteria\n- All scenarios pass deterministically\n- No temp artifacts leaked after test run\n- Tests complete within bounded runtime budget\n- Behavior assertions include log assertions for each scenario\n- Coverage includes adaptive and conservative paths\n- Integration logs are machine-readable and human-auditable","acceptance_criteria":"1. Integration suite exercises full dataflow from scan to action/logging under realistic conditions. 2. Cross-module contracts (CLI/daemon/scanner/controller/logger) are verified in end-to-end pipeline fixtures. 3. Failure-mode tests validate fallback-safe behavior and recovery transitions. 4. Detailed integration logs include trace IDs, stage timings, and decision rationale snapshots. 5. Machine-readable reports summarize pass/fail per scenario and contract.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:54:08.740511207Z","created_by":"ubuntu","updated_at":"2026-02-15T01:01:01.214958496Z","closed_at":"2026-02-15T01:01:01.214881933Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["testing"],"dependencies":[{"issue_id":"bd-21z","depends_on_id":"bd-112","type":"blocks","created_at":"2026-02-14T18:58:59.435259149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T19:02:45.671411385Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T19:02:45.415721333Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-224","type":"blocks","created_at":"2026-02-14T19:02:45.501260292Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T19:02:45.587232321Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2f8","type":"blocks","created_at":"2026-02-14T19:02:45.759501970Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2m9","type":"blocks","created_at":"2026-02-14T18:58:59.787207115Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2np","type":"blocks","created_at":"2026-02-14T19:02:46.014644738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2qa","type":"blocks","created_at":"2026-02-14T18:58:59.176076788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:22.778812157Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2s9","type":"blocks","created_at":"2026-02-14T19:02:46.103205433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:58:59.085528584Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-394","type":"blocks","created_at":"2026-02-14T19:02:45.843017261Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-395","type":"blocks","created_at":"2026-02-14T18:58:59.613170925Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T19:02:45.929798235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-3qm","type":"blocks","created_at":"2026-02-14T18:58:59.349590650Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T19:02:46.188331980Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-3s5","type":"blocks","created_at":"2026-02-14T18:58:59.700908186Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T16:56:22.697203491Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-7vl","type":"blocks","created_at":"2026-02-14T18:58:59.522564071Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-p2u","type":"blocks","created_at":"2026-02-14T18:58:59.878020646Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21z","depends_on_id":"bd-u92","type":"blocks","created_at":"2026-02-14T18:58:59.264028141Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":25,"issue_id":"bd-21z","author":"Dicklesworthstone","text":"ADDITIONAL INTEGRATION TEST (idea-wizard):\n\n**Scenario 9: Predictive Pre-emptive Cleanup**\n- Start at 80% free (Green)\n- Simulate rapid fill at 500 MB/s using MockPlatform\n- EWMA should detect acceleration and predict exhaustion in ~20 min\n- Predictive action pipeline should trigger PreemptiveCleanup BEFORE threshold is crossed\n- Verify cleanup starts while disk is still at 75% free (well above threshold)\n- Verify cleanup is gentle (high score threshold, not aggressive)\n- Verify disk never reaches Orange pressure level\n\n**Scenario 10: Multi-Volume Ballast**\n- Set up 2 mock volumes (/data at 5% free, /var at 60% free)\n- Provision ballast pools on both\n- Trigger pressure on /data\n- Verify ballast released from /data's pool (not /var's)\n- Verify space freed on /data\n- Recovery: verify replenishment on /data when pressure subsides","created_at":"2026-02-14T18:37:00Z"}]}
{"id":"bd-224","title":"Pressure-responsive ballast release with automatic replenishment","description":"## Deliverable\nIntegration between the PID pressure controller and ballast manager: automatically release ballast files when pressure rises, and replenish when pressure subsides.\n\n## Technical Approach\n### Release Logic\nThe PID controller outputs a PressureResponse. When it includes ReleaseBallast(N):\n1. Log the decision with full context (pressure level, free%, rate estimate)\n2. Release N ballast files via BallastManager\n3. Wait 2 seconds for filesystem to update\n4. Re-measure pressure\n5. If still critical, release more (up to all ballast)\n6. Log total released and resulting free%\n\n### Replenishment Logic\nWhen pressure returns to Green and stays there for replenish_cooldown (default 30 minutes):\n1. Check how many ballast files were released\n2. Start replenishing one file at a time\n3. After each file, re-check pressure - abort if no longer Green\n4. Log replenishment progress\n\n### Rate-Limited Replenishment\nReplenishing (writing 1GB files) itself causes disk I/O. Rate-limit to:\n- One file every 5 minutes during replenishment\n- Only during Green pressure\n- Pause if any other disk activity is detected (via I/O wait)\n\n### Graduated Response\nThe PID controller determines HOW MANY ballast files to release based on urgency:\n- urgency 0.0-0.3: release 1 file (buy ~1GB breathing room)\n- urgency 0.3-0.6: release 3 files (buy ~3GB)\n- urgency 0.6-0.9: release half of remaining ballast\n- urgency 0.9-1.0: release ALL ballast (emergency)\n\n## Acceptance Criteria\n- Ballast released proportionally to pressure urgency\n- No oscillation: released ballast stays released until cooldown\n- Replenishment is gradual and pressure-aware\n- Replenishment pauses if pressure increases\n- All releases logged with full context\n- Integration test: simulate pressure → verify release → simulate recovery → verify replenish","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:46:36.789122074Z","created_by":"ubuntu","updated_at":"2026-02-14T20:13:31.590808505Z","closed_at":"2026-02-14T20:13:31.590742110Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ballast"],"dependencies":[{"issue_id":"bd-224","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T16:55:43.958597339Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-224","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T16:55:43.877980079Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":14,"issue_id":"bd-224","author":"Dicklesworthstone","text":"REVIEW: Important edge case — ballast files must be on the SAME filesystem that's under pressure. If /data is filling up but ballast is on /var (different mount), deleting ballast doesn't help.\n\nImplementation must:\n1. During provisioning, detect which mount point the ballast_dir is on\n2. During release, verify the ballast files are on the same mount as the filesystem under pressure\n3. If they're on different mounts, skip ballast release and go straight to artifact scanning\n4. Log a warning: \"Ballast on /var/lib/sbh/ballast cannot relieve pressure on /data — consider setting ballast.location to /data/.sbh/ballast\"\n5. Config should suggest putting ballast on the primary data volume: ballast.location = \"/data/.sbh/ballast\"","created_at":"2026-02-14T17:14:58Z"},{"id":30,"issue_id":"bd-224","author":"Dicklesworthstone","text":"REVIEW: Integration with multi-volume ballast pool coordinator (bd-u92): When bd-u92 is implemented, this bead's BallastManager interface changes from single-pool to multi-pool. Key changes: (1) Release logic must select ballast pool on the SAME filesystem as the pressure source (per existing review comment). (2) BallastPoolCoordinator.release_for_mount(mount_point, count) replaces BallastManager.release(count). (3) Replenishment targets pools proportionally based on config. (4) Per-pool lock files prevent concurrent access from CLI and daemon. (5) State tracking: each pool has its own available/released count.","created_at":"2026-02-14T18:53:44Z"},{"id":49,"issue_id":"bd-224","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-u92 hard dependency. Initial implementation uses single-pool BallastManager from bd-25g directly. When bd-u92 (multi-volume coordinator) is available later, the interface upgrades to BallastPoolCoordinator.release_for_mount(). The same-filesystem check from comment 14 still applies. Multi-volume is layered on top, not load-bearing for v1.","created_at":"2026-02-14T19:02:59Z"}]}
{"id":"bd-25g","title":"Ballast file manager: creation, verification, and inventory","description":"## Deliverable\nThe core ballast file management system: creating, tracking, and verifying the \"sacrificial anode\" files that can be deleted under pressure.\n\n## Technical Approach\n### Ballast File Design\nFiles named: SBH_BALLAST_FILE_00001.dat through SBH_BALLAST_FILE_NNNNN.dat\nLocated in: /var/lib/sbh/ballast/ (Linux), ~/Library/Application Support/sbh/ballast/ (macOS)\nDefault: 10 files × 1GB each = 10GB of reclaimable space\n\n### File Format\nEach ballast file contains:\n- First 4096 bytes: Header with metadata (JSON)\n  - magic: \"SBH_BALLAST_v1\"\n  - file_index: sequential number\n  - created_at: ISO 8601 timestamp\n  - file_size: total size in bytes\n  - xxhash64: checksum of data portion\n  - purpose: \"Storage ballast for emergency space recovery\"\n- Remaining bytes: random data generated by getrandom (or /dev/urandom fallback)\n\nUsing random data (not zeros) prevents filesystem-level deduplication or compression from defeating the purpose. The data must actually occupy disk blocks.\n\n### BallastManager\n```rust\npub struct BallastManager {\n    ballast_dir: PathBuf,\n    config: BallastConfig,\n    inventory: Vec<BallastFile>,\n}\n\npub struct BallastFile {\n    pub path: PathBuf,\n    pub index: u32,\n    pub size: u64,\n    pub created_at: DateTime<Utc>,\n    pub integrity_ok: bool,\n    pub released: bool,\n}\n\nimpl BallastManager {\n    /// Create all ballast files (idempotent, skips existing valid files)\n    pub fn provision(&mut self) -> Result<ProvisionReport, SbhError>;\n    \n    /// Check inventory: how many files exist, their status\n    pub fn inventory(&self) -> &[BallastFile];\n    \n    /// How many bytes can be released\n    pub fn releasable_bytes(&self) -> u64;\n    \n    /// Release N ballast files (delete highest-index first)\n    pub fn release(&mut self, count: usize) -> Result<ReleaseReport, SbhError>;\n    \n    /// Verify integrity of all ballast files (check headers, optional checksum)\n    pub fn verify(&self) -> Result<VerifyReport, SbhError>;\n    \n    /// Replenish: recreate released ballast files when pressure subsides\n    pub fn replenish(&mut self) -> Result<ProvisionReport, SbhError>;\n}\n```\n\n### Creation Strategy\nCreating 10GB of random data takes time. Strategy:\n1. Create files sequentially (not in parallel - we don't want to cause the very disk pressure we're trying to prevent)\n2. Write in 4MB chunks with fsync every 64MB\n3. Check free space before each file creation - abort if we'd go below 20% free\n4. Log progress to stderr for visibility\n\n### Release Strategy\nWhen pressure rises, delete ballast files in reverse order (highest index first):\n- Each deletion frees exactly file_size_mb (e.g., 1GB)\n- Log each release to the activity logger\n- After release, immediately re-check pressure to see if more releases needed\n\n## Design Rationale\nThe \"ballast\" concept is inspired by ships using ballast water for stability. By pre-allocating space, we create a guaranteed buffer that can be instantly reclaimed. This is critical because under extreme disk pressure, even creating a log file might fail. The ballast provides breathing room for the scanner and cleaner to do their work.\n\nRandom data prevents CoW/dedup/compress from silently reclaiming the space. The header enables verification that files haven't been tampered with.\n\n## Acceptance Criteria\n- Ballast files created with correct format and random content\n- Files are genuinely occupying disk blocks (verified via du vs stat)\n- Release correctly deletes files and reports freed space\n- Replenish recreates files when pressure subsides\n- Integrity verification catches corrupted/truncated files\n- Idempotent provision: re-running doesn't duplicate files\n- Unit tests for all operations\n- Test: provision → verify → release 3 → verify → replenish → verify","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:46:20.561757848Z","created_by":"ubuntu","updated_at":"2026-02-14T20:07:40.569064718Z","closed_at":"2026-02-14T20:07:40.569028791Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ballast"],"dependencies":[{"issue_id":"bd-25g","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:55:43.725422335Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-25g","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T19:02:39.036744847Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-25g","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T16:55:43.800734091Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":4,"issue_id":"bd-25g","author":"Dicklesworthstone","text":"REVIEW: Two important additions needed:\n\n1. FALLOCATE OPTIMIZATION: Writing 10GB of random data takes 25-50s (getrandom/urandom maxes out ~200-400 MB/s). On ext4/xfs, use posix_fallocate() or fallocate() which is INSTANT — it reserves blocks on disk without writing data. This is the correct approach for the primary target (Linux ext4). Only fall back to random data writing on CoW filesystems (btrfs, zfs) where fallocate might not prevent deduplication. Detection: check fs_type from PAL. Implementation:\n  - ext4/xfs: fallocate() → instant provisioning\n  - btrfs/zfs: write random data in 4MB chunks → 25-50s for 10GB\n  - tmpfs: NEVER put ballast on tmpfs (that defeats the purpose)\n\n2. FILE LOCKING: When the daemon is running and a user runs 'sbh ballast release 1', both could try to manipulate ballast files simultaneously. Use flock() on a lockfile (/var/lib/sbh/.ballast.lock) to serialize access. The daemon holds the lock during ballast operations, CLI commands wait for the lock with a timeout (5s).","created_at":"2026-02-14T17:12:51Z"}]}
{"id":"bd-26g","title":"systemd unit file generation and service installation (Linux)","description":"## Deliverable\nGenerate and install a systemd service unit file for the sbh daemon on Linux systems.\n\n## Technical Approach\n### Generated Unit File\n```ini\n[Unit]\nDescription=Storage Ballast Helper - Disk Space Guardian\nDocumentation=man:sbh(1)\nAfter=local-fs.target\nWants=local-fs.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/sbh daemon\nExecReload=/bin/kill -HUP $MAINPID\nRestart=on-failure\nRestartSec=10\nWatchdogSec=60\nTimeoutStopSec=30\nNice=19\nIOSchedulingClass=idle\nIOSchedulingPriority=7\n\n# Security hardening\nNoNewPrivileges=true\nProtectSystem=strict\nReadWritePaths=/var/lib/sbh /tmp /data/tmp\nProtectHome=read-only\nPrivateTmp=false\nProtectKernelTunables=true\nProtectControlGroups=true\nRestrictSUIDSGID=true\nLimitNOFILE=4096\n\n# Resource limits\nMemoryMax=256M\nCPUQuota=10%\n\n# Logging\nStandardOutput=journal\nStandardError=journal\nSyslogIdentifier=sbh\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### Key Design Choices\n- **Type=notify**: sbh will notify systemd when it's ready (sd_notify)\n- **Nice=19 + IOSchedulingClass=idle**: sbh runs at lowest priority - it must NEVER compete with the build processes it's trying to protect\n- **ProtectSystem=strict**: security hardening, only write to /var/lib/sbh\n- **ReadWritePaths**: explicitly list all paths sbh needs to write to\n- **ProtectHome=read-only**: can read home dirs (to scan) but not write\n- **MemoryMax=256M**: prevent sbh itself from consuming too much RAM\n- **CPUQuota=10%**: limit CPU usage\n- **WatchdogSec=60**: systemd restarts sbh if it stops responding\n- **Restart=on-failure**: auto-restart on crash\n\n### Installation Flow\n```bash\nsbh install --systemd\n```\n1. Generate unit file from template (with config-specific paths)\n2. Copy to /etc/systemd/system/sbh.service (or ~/.config/systemd/user/)\n3. systemctl daemon-reload\n4. systemctl enable sbh\n5. systemctl start sbh\n6. Verify service is running\n\n### User vs System Service\n- `sbh install`: system service (requires root)\n- `sbh install --user`: user service (no root needed)\n\n### sd_notify Integration\nThe daemon calls sd_notify at key points:\n- READY=1 after startup is complete\n- WATCHDOG=1 every 30 seconds during normal operation\n- STATUS=Monitoring N paths, M GB free on /data\n\n## Acceptance Criteria\n- Generated unit file is valid (systemd-analyze verify)\n- Service starts, stops, and restarts correctly\n- Watchdog keeps service alive\n- Resource limits enforced (memory, CPU)\n- Security sandboxing works (can't write to system dirs)\n- User-mode service works without root\n- sd_notify integration functional\n- Unit tests for unit file generation\n- Integration test: install → start → verify running → stop → uninstall","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:51:07.334963803Z","created_by":"ubuntu","updated_at":"2026-02-14T23:08:37.935328755Z","closed_at":"2026-02-14T23:08:37.935309319Z","close_reason":"Implemented systemd unit file generation and service installation. 15 unit tests passing. Wired up CLI install/uninstall handlers.","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon","platform"],"dependencies":[{"issue_id":"bd-26g","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:04:03.281385667Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26g","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:07.606703356Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26g","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:56:03.477895301Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":16,"issue_id":"bd-26g","author":"Dicklesworthstone","text":"REVIEW: ReadWritePaths conflict with artifact deletion in home directories:\n\nThe unit file specifies ProtectHome=read-only, but if scanner.watched_paths includes /home/*/projects/*, sbh CANNOT delete artifacts there because it only has read access. This creates a silent failure.\n\nFix: When generating the unit file, dynamically set ReadWritePaths to include all paths from scanner.watched_paths that are under /home:\n  ReadWritePaths=/var/lib/sbh /tmp /data/tmp /home\n  ProtectHome=false  (or use read-only + explicit ReadWritePaths)\n\nAlternative: Separate the systemd sandboxing for system-mode vs user-mode:\n- System mode (root): ProtectHome=read-only, ReadWritePaths includes /home\n- User mode (no root): no ProtectHome needed (already running as the user)\n\nThe template must be generated from config, not hardcoded.","created_at":"2026-02-14T17:16:01Z"}]}
{"id":"bd-284","title":"Open-file detection boundary false positives in scanner deletion","description":"Root cause: Linux open-file check compared /proc fd symlink targets using raw string prefix, so sibling paths like /tmp/foo-2 matched /tmp/foo and '(deleted)' suffixes were not normalized. Fix: canonical path-component matching with deleted-suffix normalization and regression tests.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-15T16:21:10.188311318Z","created_by":"ubuntu","updated_at":"2026-02-15T16:21:15.113532100Z","closed_at":"2026-02-15T16:21:15.113510009Z","close_reason":"Completed","source_repo":".","compaction_level":0,"original_size":0,"labels":["linux","reliability","scanner"],"comments":[{"id":102,"issue_id":"bd-284","author":"WildBeacon","text":"Fixed in src/scanner/deletion.rs + src/monitor/fs_stats.rs (+ src/cli/assets.rs compile unblock). Added regression tests: fd_link_matching_requires_component_boundary and fd_link_matching_accepts_deleted_suffix. Validation: cargo fmt --check PASS; rch exec 'cargo check --all-targets' PASS; rch exec 'cargo test --lib deletion' PASS; rch exec 'cargo test --lib fs_stats' PASS; rch exec 'cargo test --test integration_tests' PASS; rch exec 'cargo clippy --lib -- -D warnings' PASS.","created_at":"2026-02-15T16:21:15Z"}]}
{"id":"bd-2e2","title":"Clippy debt burn-down slice: cli_app setup helper lint cluster","description":"Fix a narrow cluster of clippy -D warnings in src/cli_app.rs setup helper region (collapsible_if, option_if_let_else/map_or_else, manual_let_else, match_same_arms, map_unwrap_or, related small cleanups) while preserving behavior and validating via rch.","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-02-15T16:37:46.875433214Z","created_by":"ubuntu","updated_at":"2026-02-15T16:37:50.606678681Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"]}
{"id":"bd-2e5","title":"Clippy debt burn-down slice: core errors/config lint blockers","description":"Fix immediate clippy -D warnings blockers in src/core/errors.rs and src/core/config.rs (io_other_error, redundant_closure_for_method_calls, redundant_clone in tests) while preserving behavior with focused rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:33:25.966594071Z","created_by":"ubuntu","updated_at":"2026-02-15T16:35:15.449576738Z","closed_at":"2026-02-15T16:35:15.449558123Z","close_reason":"Completed: core errors/config clippy blockers resolved and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":110,"issue_id":"bd-2e5","author":"Dicklesworthstone","text":"Completed non-overlap clippy slice for core errors/config tests.\\n\\nChanges:\\n- src/core/errors.rs: replaced std::io::Error::new(ErrorKind::Other, ..) with std::io::Error::other(..) in tests; replaced closure map(|e| e.code()) with method reference map(SbhError::code).\\n- src/core/config.rs: removed redundant clone in stable_hash_changes_when_config_changes test by constructing a fresh Config::default() for the modified config.\\n\\nValidation:\\n- rch exec \"cargo test --lib errors\" ✅\\n- rch exec \"cargo test --lib config\" ✅\\n- rch exec \"cargo check --all-targets\" ✅\\n- cargo fmt --check ✅\\n- rch exec \"cargo clippy --all-targets -- -D warnings\" ❌ (remaining repo-wide backlog outside this slice; e.g., tests/stress_tests.rs, src/decision_plane_tests.rs, src/cli_app.rs, etc.)","created_at":"2026-02-15T16:35:11Z"}]}
{"id":"bd-2f8","title":"SQLite activity database with WAL mode and schema","description":"## Deliverable\nSQLite database for persistent activity logging, using WAL mode for concurrent read/write and optimized for append-heavy workloads.\n\n## Technical Approach\n### Schema\n```sql\n-- Core activity log\nCREATE TABLE IF NOT EXISTS activity_log (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    timestamp TEXT NOT NULL,             -- ISO 8601\n    event_type TEXT NOT NULL,            -- 'ballast_release', 'artifact_delete', 'pressure_change', etc.\n    severity TEXT NOT NULL,              -- 'info', 'warning', 'critical'\n    path TEXT,                           -- affected path (nullable for non-path events)\n    size_bytes INTEGER,                  -- size of deleted item\n    score REAL,                          -- candidacy score at time of action\n    score_factors TEXT,                  -- JSON blob of individual factor values\n    pressure_level TEXT,                 -- pressure level at time of action\n    free_pct REAL,                       -- free% at time of action\n    duration_ms INTEGER,                 -- how long the action took\n    success INTEGER NOT NULL DEFAULT 1,  -- 1=success, 0=failure\n    error_code TEXT,                     -- SBH-XXXX error code if failed\n    error_message TEXT,                  -- human-readable error\n    details TEXT                         -- JSON blob for additional context\n);\n\n-- Pressure history (for trend analysis)\nCREATE TABLE IF NOT EXISTS pressure_history (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    timestamp TEXT NOT NULL,\n    mount_point TEXT NOT NULL,\n    total_bytes INTEGER NOT NULL,\n    free_bytes INTEGER NOT NULL,\n    free_pct REAL NOT NULL,\n    rate_bytes_per_sec REAL,\n    pressure_level TEXT NOT NULL,\n    ewma_rate REAL,\n    pid_output REAL\n);\n\n-- Ballast inventory\nCREATE TABLE IF NOT EXISTS ballast_inventory (\n    file_index INTEGER PRIMARY KEY,\n    path TEXT NOT NULL,\n    size_bytes INTEGER NOT NULL,\n    created_at TEXT NOT NULL,\n    released_at TEXT,\n    replenished_at TEXT,\n    integrity_hash TEXT\n);\n\n-- Indexes for common queries\nCREATE INDEX IF NOT EXISTS idx_activity_timestamp ON activity_log(timestamp);\nCREATE INDEX IF NOT EXISTS idx_activity_event_type ON activity_log(event_type);\nCREATE INDEX IF NOT EXISTS idx_pressure_timestamp ON pressure_history(timestamp);\nCREATE INDEX IF NOT EXISTS idx_pressure_mount ON pressure_history(mount_point);\n```\n\n### Connection Management\n```rust\npub struct SqliteLogger {\n    conn: rusqlite::Connection,\n    insert_activity: rusqlite::Statement,    // prepared statement\n    insert_pressure: rusqlite::Statement,\n}\n```\n\n### PRAGMAs\n```sql\nPRAGMA journal_mode = WAL;           -- concurrent reads during writes\nPRAGMA synchronous = NORMAL;         -- good durability, better perf than FULL\nPRAGMA cache_size = -8000;           -- 8MB page cache\nPRAGMA mmap_size = 67108864;         -- 64MB mmap for reads\nPRAGMA temp_store = MEMORY;          -- temp tables in memory\nPRAGMA busy_timeout = 5000;          -- 5s retry on lock contention\n```\n\n### Graceful Degradation\nIf SQLite operations fail (disk full!), sbh must NOT stop working:\n- Log error to stderr\n- Continue operating using JSONL-only mode\n- Attempt to reopen SQLite periodically\n\n## Design Rationale\nSQLite with WAL mode provides excellent concurrent read performance (agents can query stats while daemon writes). Prepared statements minimize parse overhead for high-frequency inserts. The schema captures enough detail for comprehensive post-incident analysis while keeping writes fast.\n\nThe graceful degradation is critical: if the disk is so full that even SQLite can't write, sbh must still function (using JSONL on a different filesystem, or just stderr).\n\n## Acceptance Criteria\n- Database created with correct schema and PRAGMAs\n- Prepared statements for all common operations\n- WAL mode confirmed active\n- Insert latency < 1ms for typical log entries\n- Concurrent read/write without lock contention\n- Graceful handling of disk-full during write\n- Automatic schema migration for version upgrades\n- Unit tests for all CRUD operations\n- Test: 10,000 rapid inserts → no data loss\n- Test: concurrent reader + writer → no errors","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:49:11.278026287Z","created_by":"ubuntu","updated_at":"2026-02-14T19:51:08.972653915Z","closed_at":"2026-02-14T19:51:08.972633296Z","close_reason":"SQLite logger implemented in src/logger/sqlite.rs: WAL mode, NORMAL sync, 8MB cache, schema with activity_log/pressure_history/ballast_inventory tables, indexes, CRUD operations, aggregate queries (count_events_since/bytes_freed_since), upsert ballast, 7 passing tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["logger"],"dependencies":[{"issue_id":"bd-2f8","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:55:54.835554402Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2f8","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T16:55:54.753659740Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":17,"issue_id":"bd-2f8","author":"Dicklesworthstone","text":"REVIEW: Database filesystem location consideration:\n\nThe SQLite database MUST NOT be on the same filesystem that's under pressure. If /data is filling up and the database is on /data, SQLite writes will fail precisely when logging is most critical.\n\nDefault location should be /var/lib/sbh/sbh.db (typically on / root partition, separate from /data). The config default already handles this: sqlite_path = /var/lib/sbh/sbh.db.\n\nHowever, the bead should explicitly validate during startup that the SQLite database path is NOT on a monitored filesystem (or at least warn if it is). Add a startup check:\n1. Resolve the mount point for sqlite_path\n2. Compare against scanner.watched_paths mount points\n3. If same mount: log WARNING \"[SBH-5001] Activity database is on monitored filesystem /data — database writes may fail under extreme pressure. Consider moving to a separate partition.\"","created_at":"2026-02-14T17:16:07Z"}]}
{"id":"bd-2g8","title":"Clippy debt burn-down slice: daemon self-monitor literals/closures","description":"Address top blocking clippy -D warnings findings in src/daemon/self_monitor.rs with minimal behavioral change, validate with rch targeted checks, and report remaining blockers.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:15:24.681176080Z","created_by":"ubuntu","updated_at":"2026-02-15T16:16:53.809551932Z","closed_at":"2026-02-15T16:16:53.809528378Z","close_reason":"Completed","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":101,"issue_id":"bd-2g8","author":"Dicklesworthstone","text":"Completed narrow clippy debt slice in src/daemon/self_monitor.rs: fixed unreadable literal (-12_400_000.0) and replaced redundant closure with method reference (iter().all(ThreadStatus::is_healthy)). Validation: rch exec \"cargo test --lib self_monitor\" PASS; rch exec \"cargo check --all-targets\" PASS; cargo fmt --check PASS; rch exec \"cargo clippy --all-targets -- -D warnings\" still fails due broad pre-existing lint backlog, now at 79 errors.","created_at":"2026-02-15T16:16:51Z"}]}
{"id":"bd-2it","title":"sbh status command: real-time system health dashboard","description":"## Deliverable\nA status command that shows current system health, pressure levels, ballast inventory, and recent activity in a clear, informative format.\n\n## Technical Approach\n### Human Output\n```\nsbh status\n\nStorage Ballast Helper v0.1.0\n  Uptime: 3 days, 14 hours, 22 minutes\n  Config: /etc/sbh/config.toml\n\nPressure Status:\n  ┌──────────────┬───────────┬───────────┬─────────┬───────────┐\n  │ Mount Point  │ Total     │ Free      │ Free %  │ Level     │\n  ├──────────────┼───────────┼───────────┼─────────┼───────────┤\n  │ /data        │   1.8 TB  │   342 GB  │  18.5%  │ 🟢 GREEN  │\n  │ /            │   100 GB  │    23 GB  │  23.0%  │ 🟢 GREEN  │\n  │ /tmp (tmpfs) │    32 GB  │   4.8 GB  │  15.0%  │ 🟡 YELLOW │\n  │ /dev/shm     │   256 GB  │   198 GB  │  77.3%  │ 🟢 GREEN  │\n  └──────────────┴───────────┴───────────┴─────────┴───────────┘\n\nRate Estimate:\n  /data:     -12.4 MB/s (recovering)  ~∞ until threshold\n  /tmp:     +245.6 MB/s (filling)     ~18 seconds to threshold!\n\nBallast:\n  Files: 8/10 available (2 released under pressure)\n  Reclaimable: 8 GB\n\nRecent Activity (last hour):\n  Deletions: 12 items, 18.7 GB freed\n  Most common: cargo-target-* (8), .target_* (3), target/ (1)\n  Failures: 0\n```\n\n### JSON Output (--json)\n```json\n{\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 307342,\n  \"pressure\": {\n    \"mounts\": [\n      {\"path\": \"/data\", \"total\": 1800000000000, \"free\": 342000000000, \"free_pct\": 18.5, \"level\": \"green\"},\n      ...\n    ],\n    \"overall\": \"yellow\"\n  },\n  \"rate\": {\n    \"/data\": {\"bytes_per_sec\": -12400000, \"trend\": \"recovering\", \"seconds_to_threshold\": null},\n    \"/tmp\": {\"bytes_per_sec\": 245600000, \"trend\": \"filling\", \"seconds_to_threshold\": 18}\n  },\n  \"ballast\": {\"available\": 8, \"total\": 10, \"released\": 2, \"reclaimable_bytes\": 8000000000},\n  \"recent_hour\": {\"deletions\": 12, \"bytes_freed\": 18700000000, \"failures\": 0}\n}\n```\n\n### Implementation\nReads from:\n- Live filesystem stats (PAL)\n- EWMA rate estimator (if daemon running, via shared state or status file)\n- SQLite database (recent activity)\n- Ballast inventory\n\nIf daemon is not running, still shows filesystem stats and database history but notes that live monitoring is inactive.\n\n## Acceptance Criteria\n- Shows all monitored mount points with pressure levels\n- Color-coded pressure levels (green/yellow/orange/red)\n- Rate estimates with time-to-threshold\n- Ballast inventory status\n- Recent activity summary\n- Works even when daemon is not running (degraded mode)\n- JSON mode produces complete, parseable output\n- Unit tests for formatting functions","acceptance_criteria":"1. Unit tests cover status rendering, JSON schema, and degraded-mode state handling. 2. Integration tests validate daemon state file ingestion plus live monitor metrics merging. 3. E2E scenarios verify behavior when daemon is running, stopped, stale, or partially unavailable. 4. Command performance stays bounded under high activity history. 5. Detailed status logs include source freshness, missing-signal reasons, and fallback mode transitions.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:52:25.394142213Z","created_by":"ubuntu","updated_at":"2026-02-14T21:56:31.896019766Z","closed_at":"2026-02-14T21:56:31.895948503Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-2it","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:13.094091004Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2it","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T16:56:13.002879889Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":48,"issue_id":"bd-2it","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-2np dependency. sbh status works in degraded mode without state.json: shows direct fs stats from statvfs() + database query for recent events. When state.json is available (daemon running), status ADDITIONALLY shows live EWMA predictions, PID state, scan activity. The state.json read is opportunistic, not required.","created_at":"2026-02-14T19:02:55Z"}]}
{"id":"bd-2j5","title":"Installer DX Parity Program (dcg-grade automation for curl|bash + update + bootstrap)","description":"## Background\nWe want `sbh`/`fsfs` install UX to be as user-friendly and automated as `/dp/destructive_command_guard` (`dcg`): one-command setup, robust verification, auto-configuration, guided defaults, and safe update+rollback.\n\n## Why this epic exists\nExisting install-related beads cover service setup and baseline CLI work, but they do not yet encode the full parity set that makes dcg's distribution path smooth for both humans and agents.\n\n## dcg parity baseline (concrete features to mirror)\n1. Unix installer with rich flags, platform detection, artifact targeting, preflight checks, checksum + optional sigstore verification, source-build fallback, PATH/completions automation, and polished summary UX.\n2. Windows PowerShell installer with equivalent integrity and path automation.\n3. Uninstall flow with safe cleanup + optional purge behavior.\n4. First-class updater with check/cache/force/rollback/version-list/system-user modes.\n5. Auto-configuration integrations for AI tooling with backup/merge semantics.\n6. Release artifact contract (target archives + `.sha256` + optional `.sigstore.json`) that installers/updaters can rely on deterministically.\n\n## Program goals\n- Batteries-included install and update flow with minimal manual steps.\n- Default-safe integrity checks and explicit remediation on failures.\n- Idempotent bootstrap and reversible rollback.\n- Self-contained bead graph (all rationale and intent in issue bodies/comments).\n\n## Success criteria\n1. Fresh machine: install -> verify -> run -> update succeeds via documented happy path.\n2. Installer automates PATH, completions, integration bootstrap, and first-run setup.\n3. Updater has backup+rollback with bounded retention and clear UX.\n4. E2E tests cover happy paths and failure injection scenarios.\n5. Docs are sufficient for future maintainers without external planning docs.","acceptance_criteria":"1. Epic scope covers Unix installer, Windows installer, uninstall parity, update/rollback, model/assets bootstrap, offline bundle support, and migration/self-healing without dropping functionality.\n2. Every feature bead in this epic links to both unit-test coverage (bd-2j5.19) and E2E verification (bd-2j5.14) directly or through dependencies.\n3. Structured observability (bd-2j5.18) is integrated so install/update failures are diagnosable from logs alone.\n4. Release artifact contract (bd-2j5.13) and docs handbook (bd-2j5.15) are complete before epic closure.\n5. Dependency graph remains acyclic and supports parallel work where safe.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-14T21:02:47.540739742Z","created_by":"ubuntu","updated_at":"2026-02-15T04:38:52.418707349Z","closed_at":"2026-02-15T04:38:52.418681741Z","close_reason":"Epic acceptance met: all child beads are closed including unit matrix (bd-2j5.19), E2E matrix (bd-2j5.14), offline bundle mode (bd-2j5.20), update cache/notice policy (bd-2j5.9), release artifact contract (bd-2j5.13), structured observability (bd-2j5.18), and operator handbook (bd-2j5.15). Dependency graph remains acyclic with no blocked issues; key validation gates were re-run via rch during closure pass.","source_repo":".","compaction_level":0,"original_size":0,"labels":["automation","distribution","dx","installer"],"comments":[{"id":59,"issue_id":"bd-2j5","author":"Dicklesworthstone","text":"Implementation note: This epic intentionally mirrors concrete dcg behaviors that drive real-world usability: (1) deterministic artifact resolution, (2) default integrity checks, (3) idempotent auto-configuration, (4) updater rollback semantics, and (5) strong installer summaries with remediation hints. Keep parity decisions explicit: if we diverge, record why in child issue comments.","created_at":"2026-02-14T21:04:20Z"},{"id":66,"issue_id":"bd-2j5","author":"Dicklesworthstone","text":"Revision note (plan-space optimization): Added explicit Windows parity, uninstall parity, observability, unit-matrix, offline bundle, and migration/self-healing tracks. This closes major UX/completeness gaps while preserving all previously scoped functionality. Also promoted explicit acceptance criteria fields and test/logging gates across all children so completion standards are enforceable by tooling, not just narrative descriptions.","created_at":"2026-02-14T21:14:12Z"}]}
{"id":"bd-2j5.1","title":"Parity Matrix and Acceptance Spec for dcg-grade installer/update UX","description":"## Deliverable\nA parity matrix that translates dcg installer/update automation into explicit sbh/fsfs behavior requirements and CLI surface definitions.\n\n## Background and rationale\nWe want implementation to be deterministic, not vibe-driven. This issue codifies exactly what we are mirroring from dcg and what we intentionally diverge from.\n\n## Scope\n- Enumerate required installer flags (version pin, dest path, easy mode, verify/no-verify, from-source, offline/no-network, quiet/no-color, no-configure).\n- Enumerate updater controls (check, refresh cache, force, rollback, list versions).\n- Define required idempotency and rollback guarantees.\n- Define output/UX contract (human mode + machine mode).\n\n## Acceptance criteria\n1. Matrix maps each dcg automation capability to sbh behavior (implemented/planned/not-applicable + why).\n2. Gaps become linked child dependencies in this epic.\n3. Future maintainers can make scope decisions from this issue alone.","acceptance_criteria":"1. Parity matrix maps each dcg automation capability to sbh/fsfs behavior: implemented/planned/deferred with rationale.\n2. Matrix includes explicit CLI flag contract for install/update/bootstrap/uninstall flows.\n3. Matrix includes security policy for verification defaults, bypass behavior, and failure handling.\n4. Matrix references required unit/e2e/logging gates for every capability.\n5. Output is detailed enough to drive implementation without external planning docs.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:38.854226306Z","created_by":"ubuntu","updated_at":"2026-02-14T21:27:25.612362498Z","closed_at":"2026-02-14T21:27:25.612343672Z","close_reason":"Completed: parity matrix and acceptance spec documented in docs/installer-dx-parity-matrix.md with capability mapping, CLI contracts, security policy, and gate/bead linkage.","source_repo":".","compaction_level":0,"original_size":0,"labels":["dx","installer","planning"],"dependencies":[{"issue_id":"bd-2j5.1","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:38.854226306Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":67,"issue_id":"bd-2j5.1","author":"Dicklesworthstone","text":"Implemented parity matrix artifact at docs/installer-dx-parity-matrix.md. Includes dcg->sbh capability mapping with implemented/planned/deferred statuses, explicit CLI flag contracts (install/update/bootstrap/uninstall), security defaults/bypass policy, and required unit/e2e/observability gates mapped to beads.","created_at":"2026-02-14T21:27:22Z"}]}
{"id":"bd-2j5.10","title":"Updater backups, rollback flow, and retention pruning","description":"## Deliverable\nUpdater backup snapshots, rollback command support, and bounded retention pruning.\n\n## Background and rationale\nAutomatic updates without rollback are operationally unsafe.\n\n## Scope\n- Store pre-update binary/config snapshots before replacement.\n- Add rollback flow (latest or selected prior version).\n- Prune old backups by policy while preserving recent recovery points.\n\n## Acceptance criteria\n1. Failed update can be rolled back in one command.\n2. Backup inventory is inspectable (list timestamps/versions).\n3. Retention policy prevents unbounded disk growth.","acceptance_criteria":"1. Update process always captures restore points before replacement.\n2. Rollback supports latest and selected prior versions deterministically.\n3. Retention pruning enforces bounds without deleting required recovery points.\n4. Unit tests cover backup inventory and rollback selection logic.\n5. E2E validates failed-update recovery and rollback logging.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.734964821Z","created_by":"ubuntu","updated_at":"2026-02-15T02:33:14.002388112Z","closed_at":"2026-02-15T02:33:14.002322940Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["recovery","reliability","update"],"dependencies":[{"issue_id":"bd-2j5.10","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.734964821Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.10","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:24.595024952Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.10","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:04:04.880839883Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":69,"issue_id":"bd-2j5.10","author":"Dicklesworthstone","text":"Coordination note from CopperAnchor: rch validation surfaced compile blocker in src/cli/update.rs (E0433 undeclared BackupStore at line 306; downstream E0282). Reported to owning agent on Agent Mail thread bd-2j5.10.","created_at":"2026-02-15T02:25:33Z"}]}
{"id":"bd-2j5.11","title":"Guided first-run install wizard + non-interactive --auto mode","description":"## Deliverable\nGuided first-run installer wizard plus non-interactive `--auto` mode.\n\n## Background and rationale\nNew users should get smart defaults, while automation users need deterministic non-interactive behavior.\n\n## Scope\n- Interactive flow for selecting install mode, watched paths, and ballast defaults.\n- Clear summary/confirmation step before writing config and provisioning resources.\n- `--auto` path for CI/agent usage that applies documented defaults without prompts.\n\n## Acceptance criteria\n1. Interactive mode reduces manual post-install editing for first-time users.\n2. `--auto` mode is fully non-interactive and script-safe.\n3. Wizard choices map cleanly to generated config with reproducible output.","acceptance_criteria":"1. Interactive wizard provides guided defaults and explicit confirmation before mutation.\n2. `--auto` mode is fully non-interactive and deterministic for automation.\n3. Wizard and auto outputs include config decisions and safety-impacting choices.\n4. Unit tests cover wizard decision mapping and non-interactive defaults.\n5. E2E validates first-run onboarding and repeat-run idempotency with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:03:39.825228176Z","created_by":"ubuntu","updated_at":"2026-02-15T00:11:41.817560945Z","closed_at":"2026-02-15T00:11:41.817497566Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","onboarding","ux"],"dependencies":[{"issue_id":"bd-2j5.11","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.825228176Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.11","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:04.968069673Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.11","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:24.045643446Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.11","depends_on_id":"bd-2j5.6","type":"blocks","created_at":"2026-02-14T21:11:23.850607348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.11","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:11:23.950623268Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.12","title":"Automatic model/asset bootstrap: download, verify, cache, prefetch","description":"## Deliverable\nAutomatic model/asset bootstrap pipeline (download, verify, cache, optional prefetch) for components that require external model/data bundles.\n\n## Background and rationale\nUser request explicitly calls for automatic model download and batteries-included setup. This must be explicit, policy-driven, and safe.\n\n## Scope\n- Define artifact manifest format (name, version, checksum, source URL, size, signature info).\n- Implement resumable download + integrity verification + local cache layout.\n- Add CLI controls for prefetch, offline mode, and cache inspection/cleanup.\n- Integrate bootstrap into install/update where relevant.\n\n## Acceptance criteria\n1. Required model/assets can be fetched automatically during install/update.\n2. Corrupt or partial downloads are detected and recovered safely.\n3. Offline mode fails fast with clear instructions when required assets are missing.","acceptance_criteria":"1. Required model/assets can be auto-fetched, verified, cached, and reused safely.\n2. Partial/corrupt downloads are detected and recovered without inconsistent state.\n3. Offline behavior is explicit: missing required assets fail fast with remediation.\n4. Unit tests cover manifest parsing, checksum validation, and cache policy logic.\n5. E2E validates online and offline asset bootstrap paths with detailed logs.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.917102900Z","created_by":"ubuntu","updated_at":"2026-02-15T00:03:22.811104591Z","closed_at":"2026-02-15T00:03:22.811010685Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["assets","automation","installer"],"dependencies":[{"issue_id":"bd-2j5.12","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.917102900Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.12","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:05.140306197Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.12","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:05.229692257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.12","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:04:05.323278819Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.12","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:11:24.229766414Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":63,"issue_id":"bd-2j5.12","author":"Dicklesworthstone","text":"Model/bootstrap note: this issue exists because user explicitly requested automatic model download and batteries-included setup. Treat model/assets as first-class artifacts with manifest-driven integrity, resumable fetch, and offline diagnostics.","created_at":"2026-02-14T21:04:20Z"}]}
{"id":"bd-2j5.13","title":"Release artifact publishing contract for installer/update compatibility","description":"## Deliverable\nRelease pipeline alignment so installers/updaters can rely on consistent artifact naming, checksums, signatures, and metadata.\n\n## Background and rationale\nInstaller quality is inseparable from release artifact discipline.\n\n## Scope\n- Define required release outputs per platform.\n- Ensure CI produces checksums/signature bundles and publishes predictable URLs.\n- Validate artifact contract continuously in CI to catch regressions before release.\n\n## Acceptance criteria\n1. Every supported target publishes installer-consumable artifacts with metadata.\n2. CI fails when artifact naming/metadata contract is broken.\n3. Install/update logic no longer depends on ad-hoc release layout assumptions.","acceptance_criteria":"1. CI publishes deterministic installer/updater artifacts for all supported targets.\n2. Release outputs include integrity metadata required by verification policy.\n3. Contract checks fail CI on naming/metadata drift.\n4. Unit/CI tests validate artifact contract manifests.\n5. Contract supports both online and bundle-based install/update flows.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:40.010974145Z","created_by":"ubuntu","updated_at":"2026-02-15T02:01:19.659149518Z","closed_at":"2026-02-15T02:01:19.659067213Z","close_reason":"Release artifact publishing contract fully implemented: ReleaseArtifactContract with deterministic naming for 6 targets, validate_release_assets() for CI enforcement, supply chain verification (SHA256+sigstore), install scripts aligned with contract. 11 unit tests cover all acceptance criteria. CI validates via cargo test --lib.","source_repo":".","compaction_level":0,"original_size":0,"labels":["ci","installer","release"],"dependencies":[{"issue_id":"bd-2j5.13","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:40.010974145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.13","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:11:23.661538843Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.13","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:11:23.757189753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.13","depends_on_id":"bd-5vm","type":"blocks","created_at":"2026-02-14T21:04:05.414962645Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.14","title":"Installer/update/integration E2E matrix with failure injection","description":"## Deliverable\nComprehensive installer/update/integration test matrix, including golden UX snapshots and failure-injection scenarios.\n\n## Background and rationale\nThese workflows are safety-critical and user-facing; regressions here have outsized impact.\n\n## Scope\n- E2E tests for fresh install, reinstall, uninstall, update, rollback, and bootstrap integrations.\n- Failure injection for network loss, checksum mismatch, unsupported targets, missing prerequisites.\n- Golden output snapshots for key user-visible installer/update screens.\n\n## Acceptance criteria\n1. Core workflows are test-covered on Linux and macOS paths, with Windows parity where feasible.\n2. Failure paths produce deterministic remediation messages.\n3. CI gate prevents shipping installer/update regressions.","acceptance_criteria":"1. E2E suite covers install, reinstall, update, rollback, uninstall, bootstrap, and bundle/offline scenarios.\n2. E2E suite includes failure injection: network loss, checksum mismatch, permission failure, unsupported target, partial install.\n3. Every E2E run captures detailed logs and structured event artifacts for diagnosis.\n4. E2E scripts are CI-runnable and deterministic across repeated runs.\n5. E2E gate blocks release when critical workflows regress.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:40.104061010Z","created_by":"ubuntu","updated_at":"2026-02-15T04:35:33.192316298Z","closed_at":"2026-02-15T04:33:55.367570189Z","close_reason":"Acceptance met: installer/update integration E2E matrix passes with failure injection and lifecycle coverage (rch cargo test --test installer_e2e: 43 passed), offline/update integration scenarios pass (rch cargo test --test integration_tests -- update_check_), and CI compile gate remains green (rch cargo check --all-targets). e2e shell suite script is syntactically valid (bash -n scripts/e2e_test.sh).","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","installer","testing"],"dependencies":[{"issue_id":"bd-2j5.14","depends_on_id":"bd-26g","type":"blocks","created_at":"2026-02-14T21:04:06.660503333Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:40.104061010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.10","type":"blocks","created_at":"2026-02-14T21:04:06.043872248Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.11","type":"blocks","created_at":"2026-02-14T21:04:06.136443678Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.12","type":"blocks","created_at":"2026-02-14T21:04:06.236560146Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.13","type":"blocks","created_at":"2026-02-14T21:04:06.433827175Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.16","type":"blocks","created_at":"2026-02-14T21:11:26.560527415Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.17","type":"blocks","created_at":"2026-02-14T21:11:26.655490236Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:26.375976766Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.19","type":"blocks","created_at":"2026-02-14T21:11:26.467339801Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:05.502538223Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.20","type":"blocks","created_at":"2026-02-14T21:11:26.747337047Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:26.841469172Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:04:05.592759859Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.6","type":"blocks","created_at":"2026-02-14T21:04:05.682276585Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:04:05.767895265Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:04:05.858732094Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2j5.9","type":"blocks","created_at":"2026-02-14T21:04:05.953232980Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2pi","type":"blocks","created_at":"2026-02-14T21:04:06.541785768Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-2q9","type":"blocks","created_at":"2026-02-14T21:11:26.931912784Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-3i3","type":"blocks","created_at":"2026-02-14T21:04:06.755896219Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.14","depends_on_id":"bd-7ls","type":"blocks","created_at":"2026-02-14T21:11:27.020357981Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":64,"issue_id":"bd-2j5.14","author":"Dicklesworthstone","text":"Testing bar: include failure injection for checksum mismatch, truncated download, permission errors, unsupported target, and rollback failure. We want deterministic reproductions rather than ad-hoc manual QA.","created_at":"2026-02-14T21:04:20Z"},{"id":96,"issue_id":"bd-2j5.14","author":"Dicklesworthstone","text":"Added blocker-clearing E2E coverage in tests/installer_e2e.rs for offline updater failure injection: (1) e2e_update_offline_bundle_bad_checksum_fails_without_network_download and (2) e2e_update_offline_bundle_missing_manifest_fails_without_network_download, plus helper create_update_bundle(). Validation: cargo fmt --check -- tests/installer_e2e.rs ✅; rch exec \"cargo test --test installer_e2e -- e2e_update_offline_bundle\" ✅ (2/2); rch exec \"cargo test --test installer_e2e -- e2e_update_\" ✅ (4/4); rch exec \"cargo check --test installer_e2e\" ✅. Note: rch exec \"cargo check --all-targets\" currently fails on unrelated examples/* API drift outside this slice.","created_at":"2026-02-15T04:25:32Z"},{"id":97,"issue_id":"bd-2j5.14","author":"LavenderOak","text":"Added installer_e2e failure-injection slice in tests/installer_e2e.rs: new tests e2e_update_offline_bundle_unsupported_target_fails_without_network_download and e2e_update_offline_bundle_blocked_install_path_fails_deterministically. Also added update_sequence_test_lock guard and applied it to full offline-update (check_only=false) tests to eliminate tempdir millisecond collision flake during parallel runs. Validation: rch exec 'cargo test --test installer_e2e -- e2e_update_offline_bundle_unsupported_target_fails_without_network_download' PASS; rch exec 'cargo test --test installer_e2e -- e2e_update_offline_bundle_blocked_install_path_fails_deterministically' PASS; rch exec 'cargo test --test installer_e2e -- e2e_update_offline_bundle' PASS (4/4); rch exec 'cargo check --test installer_e2e' PASS; cargo fmt --check PASS; rch exec 'cargo check --all-targets' PASS. rch exec 'cargo clippy --all-targets -- -D warnings' currently fails in many pre-existing files outside this slice (e.g., src/daemon/self_monitor.rs, src/logger/stats.rs, src/decision_plane_tests.rs, src/cli/bootstrap.rs).","created_at":"2026-02-15T04:35:33Z"}]}
{"id":"bd-2j5.15","title":"Installer/Updater operator handbook with troubleshooting and security model","description":"## Deliverable\nOperator-facing installer/update handbook that is self-sufficient for humans and agents.\n\n## Background and rationale\nThe easiest system to use is the one whose expected behavior is clearly documented with examples and troubleshooting.\n\n## Scope\n- Document happy path + advanced flags + rollback + recovery procedures.\n- Include security model (what is verified, what is optional, what is bypassed by `--no-verify`).\n- Provide copy-paste recipes for common deployment scenarios (desktop dev, headless server, CI agent host).\n\n## Acceptance criteria\n1. README/docs cover complete installer/update lifecycle.\n2. Troubleshooting includes concrete commands and expected outputs.\n3. Documentation remains aligned with actual CLI flags and test assertions.","acceptance_criteria":"1. Documentation covers full lifecycle: install, update, rollback, uninstall, bootstrap, offline bundle mode.\n2. Security model and bypass implications are explicit and example-driven.\n3. Troubleshooting includes concrete command sequences and expected diagnostics.\n4. Docs align with implemented flags and validated test behavior.\n5. Handbook is sufficient for both human operators and agent workflows.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:03:40.197974956Z","created_by":"ubuntu","updated_at":"2026-02-15T04:38:38.904245544Z","closed_at":"2026-02-15T04:38:38.904222581Z","close_reason":"Acceptance met: operator handbook coverage is present across README, docs/installer-dx-parity-matrix.md, and docs/testing-and-logging.md for install, update, rollback, uninstall, bootstrap, offline mode, security model, and troubleshooting workflows. Parity matrix command/flag contract now aligns with CLI semantics including update --version, update --list-backups, and bootstrap via install/setup integration paths.","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","installer","update"],"dependencies":[{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:40.197974956Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.10","type":"blocks","created_at":"2026-02-14T21:04:07.042320079Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.11","type":"blocks","created_at":"2026-02-14T21:04:07.126958633Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.12","type":"blocks","created_at":"2026-02-14T21:04:07.218156649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.13","type":"blocks","created_at":"2026-02-14T21:04:07.304559439Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.14","type":"blocks","created_at":"2026-02-14T21:11:27.113623171Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.16","type":"blocks","created_at":"2026-02-14T21:11:27.383813370Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.17","type":"blocks","created_at":"2026-02-14T21:11:27.510037501Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:27.208893468Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.19","type":"blocks","created_at":"2026-02-14T21:11:27.297123512Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:06.854148995Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.20","type":"blocks","created_at":"2026-02-14T21:11:27.623764554Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:27.726480574Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.15","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:04:06.948297821Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":70,"issue_id":"bd-2j5.15","author":"Dicklesworthstone","text":"Support docs slice started by CopperAnchor while dependency blockers remain: added operator-runbook/security/troubleshooting content to README and docs parity/testing guides to reduce handoff risk once implementation beads close.","created_at":"2026-02-15T02:25:33Z"},{"id":93,"issue_id":"bd-2j5.15","author":"Dicklesworthstone","text":"Taking a non-overlapping docs slice: add operator-facing guidance for update metadata cache, refresh semantics, and opt-out controls in README.md + docs/testing-and-logging.md; will keep aligned with implemented CLI/config behavior and validate formatting.","created_at":"2026-02-15T04:24:26Z"},{"id":98,"issue_id":"bd-2j5.15","author":"ubuntu","text":"Reopened: Correcting close reason text after shell escaping stripped flag names.","created_at":"2026-02-15T04:38:35Z"}]}
{"id":"bd-2j5.16","title":"Windows PowerShell installer parity with deterministic verification and rollback support","description":"## Deliverable\nFull Windows installer parity (PowerShell) matching Unix installer convenience and safety guarantees.\n\n## Background and rationale\n`dcg` has a first-class Windows path. For true batteries-included UX, we cannot leave Windows as second-class. Feature parity reduces support burden and avoids fragmented docs.\n\n## Scope\n- Ship `install.ps1` equivalent with version pinning, destination selection, easy-mode PATH update, and verification controls.\n- Implement deterministic Windows artifact resolution and integrity checks.\n- Ensure non-interactive CI/agent mode and human-friendly interactive mode.\n- Produce consistent summary output and machine-readable status.\n\n## Acceptance criteria\n1. Windows install supports latest + pinned version flows with deterministic artifact selection.\n2. Integrity checks (checksum and optional signature path) behave consistently with Unix policy.\n3. PATH/profile updates are idempotent and reversible.\n4. Unit tests cover argument parsing, artifact resolution, and PATH mutation logic.\n5. E2E test validates install -> verify -> update -> rollback -> uninstall with detailed step logs.","acceptance_criteria":"1. Windows installer feature parity is achieved for version pinning, verification, easy-mode PATH updates, and diagnostics.\n2. Behavior is deterministic and script-safe in non-interactive mode.\n3. Structured logs are emitted with parity to Unix operation phases.\n4. Unit tests cover Windows-specific parser/path/integrity logic.\n5. E2E validates complete Windows lifecycle with detailed logging.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:10:54.875579461Z","created_by":"ubuntu","updated_at":"2026-02-14T23:42:28.621266873Z","closed_at":"2026-02-14T23:42:28.621240193Z","close_reason":"Created install.ps1 PowerShell installer with full parity: version pinning, SHA-256 verification, user/system modes, PATH automation, rollback support, dry-run, JSON output, structured event logging, tar/7z extraction","source_repo":".","compaction_level":0,"original_size":0,"labels":["dx","installer","windows"],"dependencies":[{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:54.875579461Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5.1","type":"blocks","created_at":"2026-02-14T21:13:10.806115835Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:24.870564535Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:13:10.895968951Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:11:24.688017100Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.16","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:11:24.780741626Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.17","title":"Uninstall parity: safe cleanup modes, dry-run plans, and reversible teardown","description":"## Deliverable\nComprehensive uninstall parity with safe cleanup controls and reversible integration teardown.\n\n## Background and rationale\nGreat installers require equally robust uninstall paths. Users need confident cleanup without accidental data loss.\n\n## Scope\n- Implement uninstall modes: conservative default, keep-data/keep-config/keep-assets, and explicit purge.\n- Remove auto-configured integrations/hooks with backup-first semantics.\n- Add dry-run/plan output showing exactly what will be removed or kept.\n- Support partial-failure recovery so uninstall can be retried safely.\n\n## Acceptance criteria\n1. Uninstall behavior is explicit, idempotent, and never silently destructive.\n2. Integration teardown restores/retains user config according to selected mode.\n3. Dry-run output is complete enough for operator review before execution.\n4. Unit tests cover cleanup decision matrix and backup/restore paths.\n5. E2E tests cover keep-* variants, purge flow, and partial-failure recovery with detailed logs.","acceptance_criteria":"1. Uninstall exposes safe default, keep-* modes, and explicit purge with clear previews.\n2. Teardown reverses installer-managed integrations safely with backup awareness.\n3. Partial-failure paths are recoverable and idempotent.\n4. Unit tests cover deletion policy matrix and teardown sequencing.\n5. E2E validates uninstall variants with detailed logs and outcome checks.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:10:54.965794615Z","created_by":"ubuntu","updated_at":"2026-02-15T00:07:02.187555Z","closed_at":"2026-02-15T00:07:02.187472856Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","safety","uninstall"],"dependencies":[{"issue_id":"bd-2j5.17","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:54.965794615Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.17","depends_on_id":"bd-2j5.12","type":"blocks","created_at":"2026-02-14T21:11:25.057983620Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.17","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:25.151934123Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.17","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:11:24.964037694Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.18","title":"Structured installer/update observability with trace IDs and phase-level diagnostics","description":"## Deliverable\nStructured installer/update observability with trace IDs, phase timings, and dual human+machine output.\n\n## Background and rationale\nThe user explicitly wants confidence that everything works perfectly. That requires rich logs designed for debugging and automation, not ad-hoc println output.\n\n## Scope\n- Define event schema for installer/update/bootstrap operations.\n- Emit per-phase start/end/failure events with trace IDs and duration metrics.\n- Support human-readable and JSONL outputs without loss of diagnostic detail.\n- Capture remediation hints and decision reasons in failure events.\n\n## Acceptance criteria\n1. Every major installer/update phase emits structured start/success/failure records.\n2. Trace IDs allow correlating multi-step operations end-to-end.\n3. Logs are stable enough for automated assertions in CI.\n4. Unit tests validate event schema and serialization contracts.\n5. E2E scripts archive structured logs as artifacts for failed runs.","acceptance_criteria":"1. Installer/update flows emit structured start/success/failure events with trace IDs.\n2. Event schema is stable and machine-assertable in tests.\n3. Human summaries and JSON logs are consistent in meaning.\n4. Unit tests validate schema serialization and reason-code contracts.\n5. E2E archives logs and traces for failed scenarios automatically.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:10:55.058137386Z","created_by":"ubuntu","updated_at":"2026-02-14T22:56:22.288299837Z","closed_at":"2026-02-14T22:56:22.288281944Z","close_reason":"Implemented structured installer observability in scripts/install.sh with trace IDs, phase-level start/success/failure JSONL events (--event-log), and human/json output parity; added E2E assertions for trace/event schema stability","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","logging","observability"],"dependencies":[{"issue_id":"bd-2j5.18","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:55.058137386Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.19","title":"Comprehensive unit-test matrix for installer/update/bootstrap/rollback internals","description":"## Deliverable\nComprehensive unit test matrix for installer, updater, bootstrap, and rollback internals.\n\n## Background and rationale\nReliability in install/update flows depends on deterministic behavior under edge cases. Unit tests must lock those contracts before implementation scales.\n\n## Scope\n- Table-driven tests for target resolution, checksums, signature policy, rollback selection, cache TTL, and retention pruning.\n- Tests for shell/profile mutation idempotency and path normalization.\n- Snapshot/golden tests for key machine-readable outputs.\n- Negative tests for malformed manifests, missing assets, and corrupt metadata.\n\n## Acceptance criteria\n1. Critical decision logic has deterministic unit coverage with explicit edge-case cases.\n2. Golden outputs protect CLI/JSON contract stability.\n3. Failure cases produce precise reason codes/messages validated by tests.\n4. Coverage includes both Unix and Windows-specific logic branches.\n5. Test logs are detailed enough to diagnose failures without rerunning interactively.","acceptance_criteria":"1. Unit matrix covers all core installer/update/bootstrap/rollback decision modules.\n2. Edge cases and malformed input paths have explicit tests.\n3. Golden outputs protect machine-readable contract stability.\n4. Tests include platform-specific branches (Unix and Windows logic).\n5. Unit test logs are detailed enough for non-interactive diagnosis.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:10:55.147239544Z","created_by":"ubuntu","updated_at":"2026-02-15T04:30:44.706337755Z","closed_at":"2026-02-15T04:30:44.706317336Z","close_reason":"Acceptance met: comprehensive installer/update/bootstrap/rollback unit matrix is now green across modules (full rch cargo test --lib passed 765 tests) with explicit edge-case coverage; machine-readable contracts and failure paths validated; all-target compile gate revalidated via rch cargo check --all-targets.","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","testing","unit"],"dependencies":[{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:55.147239544Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.10","type":"blocks","created_at":"2026-02-14T21:11:26.000958960Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.12","type":"blocks","created_at":"2026-02-14T21:11:26.106670252Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.16","type":"blocks","created_at":"2026-02-14T21:11:25.721035211Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.17","type":"blocks","created_at":"2026-02-14T21:11:25.812144972Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:25.625454753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:13:11.038422685Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.20","type":"blocks","created_at":"2026-02-14T21:11:26.196883172Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:26.284128802Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:13:11.127155882Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.5","type":"blocks","created_at":"2026-02-14T21:13:11.215310084Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.6","type":"blocks","created_at":"2026-02-14T21:13:11.304619841Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:13:11.393177369Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:11:25.905502464Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.19","depends_on_id":"bd-2j5.9","type":"blocks","created_at":"2026-02-14T21:13:11.487734910Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":80,"issue_id":"bd-2j5.19","author":"AmberFalcon","text":"Taking non-overlap unit-test matrix slice on src/cli/from_source.rs (exclusive reservation). Focus: deterministic branch/edge-case coverage for from-source installer logic; avoiding currently reserved integration/E2E/update surfaces.","created_at":"2026-02-15T03:27:53Z"},{"id":83,"issue_id":"bd-2j5.19","author":"AmberFalcon","text":"Completed from_source unit-test matrix slice in src/cli/from_source.rs: added deterministic probe_version edge-case tests for failing command exit and fallback-first-line behavior without semver token (unix script harness), and tightened existing tests to avoid clippy issues (Path comparison, extension assertion via Path::extension, redundant closure removal). Validation: cargo fmt --check OK; rch exec cargo test --lib from_source::tests::probe_version_returns_none_when_command_fails OK; rch exec cargo test --lib from_source::tests::probe_version_falls_back_to_first_line_without_semver OK; rch exec cargo check --lib OK. rch exec cargo check --all-targets and rch exec cargo clippy --all-targets -- -D warnings still fail on unrelated pre-existing installer_e2e and other files outside this slice.","created_at":"2026-02-15T03:32:02Z"},{"id":84,"issue_id":"bd-2j5.19","author":"Dicklesworthstone","text":"Validation note from CopperAnchor: rch exec \"cargo check --all-targets\" currently blocked by API drift in reserved tests/installer_e2e.rs against current cli::update BackupStore/UpdateOptions signatures (create args, inventory field rename, rollback signature/order, removed UpdateOptions fields).","created_at":"2026-02-15T03:32:10Z"}]}
{"id":"bd-2j5.2","title":"Unix curl|bash Installer with polished UX and idempotent behavior","description":"## Deliverable\nA polished Unix `curl|bash` installer entrypoint with explicit flags and sensible defaults for new users.\n\n## Background and rationale\ndcg's installer succeeds because it is practical: one command works, advanced users still get control, and output is readable under stress.\n\n## Scope\n- Implement shell installer command interface and argument parsing.\n- Support user/system destination modes and dry-run mode.\n- Provide consistent UX framing (headers, progress sections, summary block, clear failure remediations).\n- Keep behavior idempotent on repeated runs.\n\n## Acceptance criteria\n1. Fresh install and repeat install both succeed without destructive side effects.\n2. `--help` clearly documents all flags and examples.\n3. Installer output remains clear in both TTY and non-TTY environments.","acceptance_criteria":"1. Unix curl|bash installer supports idempotent fresh/repeat installs with clear summary output.\n2. Installer supports documented flags and deterministic non-interactive behavior for CI/agent usage.\n3. Installer emits structured observability events per major phase.\n4. Unit tests validate parser and control-flow decisions for installer modes.\n5. E2E tests validate install path with failure injection and detailed logs.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:38.949794051Z","created_by":"ubuntu","updated_at":"2026-02-14T22:53:53.357239668Z","closed_at":"2026-02-14T22:53:53.357220422Z","close_reason":"Implemented scripts/install.sh Unix installer with explicit flags, dry-run, user/system destination handling, idempotent re-run logic, local-fixture test coverage in scripts/e2e_test.sh, and README installer docs","source_repo":".","compaction_level":0,"original_size":0,"labels":["dx","installer","unix"],"dependencies":[{"issue_id":"bd-2j5.2","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:38.949794051Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.2","depends_on_id":"bd-2j5.1","type":"blocks","created_at":"2026-02-14T21:04:03.545170674Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.2","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T21:04:03.633244375Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":60,"issue_id":"bd-2j5.2","author":"Dicklesworthstone","text":"UX contract detail: treat installer output as an operator interface. Every failure path must include: cause, what was attempted, and exact copy-paste recovery command. This mirrors dcg's practical diagnostics and is critical for agent-run sessions where logs are often the only artifact.","created_at":"2026-02-14T21:04:20Z"}]}
{"id":"bd-2j5.20","title":"Offline/airgapped bundle mode for installer/update/model assets","description":"## Deliverable\nOffline/airgapped bundle mode for installer + updater + model/assets bootstrap.\n\n## Background and rationale\nUsers in restricted environments still need full batteries-included setup. Bundle mode prevents installer UX collapse when network access is unavailable.\n\n## Scope\n- Define bundle artifact format containing binaries, checksums, signatures, and required model/assets manifests.\n- Add commands to create and consume bundles deterministically.\n- Validate bundle integrity before install/update.\n- Ensure update/rollback logic works with bundle-backed sources.\n\n## Acceptance criteria\n1. Offline install from bundle supports full bootstrap with no network dependency.\n2. Bundle verification enforces integrity exactly as online flow does.\n3. Missing/invalid bundle components fail with actionable diagnostics.\n4. Unit tests validate bundle manifest parsing and integrity checks.\n5. E2E tests simulate airgapped install/update with verbose logs and artifact capture.","acceptance_criteria":"1. Bundle mode supports fully offline install/update/bootstrap for required assets.\n2. Bundle integrity checks enforce same trust model as online flow.\n3. Bundle build/consume commands are deterministic and documented.\n4. Unit tests validate bundle manifests and error handling.\n5. E2E validates airgapped workflows with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:10:55.247556839Z","created_by":"ubuntu","updated_at":"2026-02-15T04:30:28.465541169Z","closed_at":"2026-02-15T04:30:28.465514589Z","close_reason":"Acceptance met: offline bundle install/update/bootstrap path, deterministic bundle manifest/consume, integrity checks, unit+integration+E2E coverage, and fresh rch validation (cargo test --lib; cargo test --test integration_tests -- update_check_; cargo check --all-targets).","source_repo":".","compaction_level":0,"original_size":0,"labels":["assets","installer","offline"],"dependencies":[{"issue_id":"bd-2j5.20","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:55.247556839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.20","depends_on_id":"bd-2j5.12","type":"blocks","created_at":"2026-02-14T21:11:25.343317366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.20","depends_on_id":"bd-2j5.13","type":"blocks","created_at":"2026-02-14T21:11:25.435559338Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.20","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:11:25.529611112Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.20","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:11:25.250510195Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":68,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Implemented core offline bundle primitives: (1) added typed bundle manifest + host-target bundle resolution in src/cli/mod.rs with strict artifact-name validation and local file existence checks, and (2) added bundle-aware asset hydration/readiness in src/cli/assets.rs (FetchOptions.bundle_root, restore_from_bundle flow, offline_readiness_with_bundle). Added unit tests for success/failure paths. Full rch cargo check is currently blocked by pre-existing compile errors in src/cli/update.rs (BackupStore unresolved).","created_at":"2026-02-15T02:25:09Z"},{"id":72,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Progress (JadeBass): tightened offline bundle contract in src/cli/mod.rs to allow nested paths by basename validation while rejecting parent-dir traversal (..). Added tests for nested-path success and traversal rejection. Expanded src/cli/assets.rs tests for flat bundle layout restore/readiness. Validation: cargo fmt OK; rch exec \"cargo check --lib\" OK. Full all-target check remains blocked by unrelated unsafe blocks in src/cli/update.rs test code under forbid(unsafe_code).","created_at":"2026-02-15T02:30:25Z"},{"id":75,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"CobaltOtter progress slice: hardened offline bundle asset restore/readiness path safety in src/cli/assets.rs by rejecting non-normal/absolute/parent components before bundle lookup (prevents ../ escape). Added unit coverage: fetch_offline_rejects_bundle_parent_dir_escape and offline_readiness_rejects_bundle_parent_dir_escape. Updated docs/installer-dx-parity-matrix.md with explicit offline bundle safety contract (nested->flat lookup, path safety, sha256 verify, fail-fast offline diagnostics). Validation: rch exec \"cargo test --lib assets\" PASS; rch exec \"cargo check --all-targets\" PASS. Global rch clippy all-targets currently fails in unrelated pre-existing files outside this slice.","created_at":"2026-02-15T03:12:23Z"},{"id":76,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Progress (RedWolf): added installer-side offline bundle preflight API in src/cli/install.rs via run_install_sequence_with_bundle(opts, bundle_manifest_path), including host-aware bundle contract validation + checksum integrity verification using verify_artifact_supply_chain. Added two unit tests for success and checksum-failure paths. Hardened src/cli/mod.rs bundle parser with manifest schema-version gate (expects version=1) and added rejection test for unsupported manifest versions. Validation: cargo fmt --check OK; rch exec \"cargo test --lib install_sequence_with_bundle_preflight\" OK; rch exec \"cargo test --lib bundle_contract_rejects_unsupported_manifest_version\" OK; rch exec \"cargo check --all-targets\" OK. Repo-wide clippy -D warnings remains failing on many pre-existing files outside this slice.","created_at":"2026-02-15T03:14:40Z"},{"id":77,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Follow-up (RedWolf): expanded installer bundle-preflight edge-case tests in src/cli/install.rs with (1) dry-run plan-only behavior when bundle manifest path is provided and (2) non-dry-run failure path for missing bundle manifest file. Validation: rch exec \"cargo test --lib install_sequence_with_bundle_preflight\" OK; rch exec \"cargo check --all-targets\" OK. Note: cargo fmt --check currently fails due unrelated formatting deltas in src/cli/from_source.rs and src/cli/integrations.rs owned by other agents; touched files are formatted.","created_at":"2026-02-15T03:17:11Z"},{"id":82,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Landed additional offline-bundle core APIs in src/cli/assets.rs: build_offline_bundle (deterministic sorted manifest snapshot + per-asset copied/missing/corrupt status), bundle_manifest_path/load_offline_bundle_manifest helpers, and offline_bundle_readiness (bundle-only checksum/readiness checks). Added targeted tests for deterministic export ordering, missing/corrupt diagnostics, bundle-only readiness, and invalid manifest handling. Validation: rch cargo test --lib assets ✅, rch cargo check --lib ✅, rch clippy --lib with known unrelated suppressions ✅. all-targets check still blocked by unrelated tests/installer_e2e drift.","created_at":"2026-02-15T03:30:44Z"},{"id":86,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Completed updater offline-bundle follow-up wiring slice in src/cli/update.rs + src/cli_app.rs. Added/preserved focused tests for check-only + pinned tag mismatch and validated with rch targeted runs (lib tests + update CLI parse bin test). Applied clippy cleanup in touched updater block (map_or_else). cargo fmt --check passes. rch cargo check --all-targets currently blocked by unrelated tests/installer_e2e.rs API drift (backup/update test signatures/fields). Releasing updater/cli_app reservations.","created_at":"2026-02-15T03:35:42Z"},{"id":87,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Added integration offline-bundle coverage in tests/integration_tests.rs: update_check_with_offline_bundle_manifest_reports_target_json and update_check_with_offline_bundle_and_pinned_tag_mismatch_fails_json, plus helper create_offline_update_bundle for deterministic local manifest/archive/checksum setup. Validation: cargo fmt --check ✅, rch exec \"cargo test --test integration_tests -- update_check_with_offline_bundle\" ✅ (2/2), rch exec \"cargo check --all-targets\" ✅. Targeted clippy for integration test is blocked by unrelated pre-existing src/monitor/fs_stats.rs lint (significant_drop_tightening).","created_at":"2026-02-15T03:40:31Z"},{"id":88,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"CopperAnchor start: implementing E2E offline-bundle update scenarios in scripts/e2e_test.sh (deterministic local fixture + success/mismatch cases) to raise acceptance-criteria #5 coverage.","created_at":"2026-02-15T04:20:40Z"},{"id":89,"issue_id":"bd-2j5.20","author":"AmberFalcon","text":"Starting focused bd-2j5.20 updater hardening in src/cli/update.rs: add offline-bundle edge-case tests for dry-run+refresh-cache behavior, notice suppression, and same-version check-only semantics; will validate with rch check/tests and report.","created_at":"2026-02-15T04:22:50Z"},{"id":91,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"TealDuck progress: added E2E offline-bundle update coverage in scripts/e2e_test.sh via deterministic local fixture helper + success/mismatch JSON cases for sbh update --check --offline. Validation: bash -n scripts/e2e_test.sh PASS; rch exec \"cargo test --test integration_tests -- update_check_with_offline_bundle\" PASS (2 tests).","created_at":"2026-02-15T04:23:42Z"},{"id":92,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Taking non-overlap follow-up slice: tighten bundle artifact path normalization in src/cli/mod.rs to reject non-normal components (absolute/root/prefix/dot traversal variants) and add focused unit tests; validate with rch.","created_at":"2026-02-15T04:23:52Z"},{"id":94,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"TealDuck validation follow-up: e2e script edits remain clean (git diff only scripts/e2e_test.sh). Gates: bash -n scripts/e2e_test.sh PASS; rch exec \"cargo test --test integration_tests -- update_check_with_offline_bundle\" PASS (2/2). Full rch exec \"cargo check --all-targets\" currently fails in pre-existing examples API drift (examples/pressure_monitor.rs + examples/scan_artifacts.rs); cargo fmt --check currently reports unrelated formatting deltas in examples/src/cli/update/tests files not touched in this slice.","created_at":"2026-02-15T04:24:29Z"},{"id":95,"issue_id":"bd-2j5.20","author":"Dicklesworthstone","text":"Contributed additional offline-path failure-injection E2E coverage via tests/installer_e2e.rs update tests: bad checksum and missing manifest now explicitly assert no network-download fallback in offline mode. See bd-2j5.14 comment for detailed validation commands/results.","created_at":"2026-02-15T04:25:32Z"}]}
{"id":"bd-2j5.21","title":"Installer bootstrap migration + self-healing for legacy/partial environments","description":"## Deliverable\nBootstrap migration and self-healing for existing configs/integrations across installer and updater runs.\n\n## Background and rationale\nReal users have drifted environments. A friendly installer must detect and repair outdated or partially-broken states automatically and safely.\n\n## Scope\n- Detect prior installer footprints and stale integration entries.\n- Apply deterministic migrations with timestamped backups.\n- Repair common broken states (partial installs, stale PATH entries, mismatched integration snippets).\n- Expose migration reports in both human summary and JSON output.\n\n## Acceptance criteria\n1. Legacy/partial states are migrated or repaired without manual editing in common cases.\n2. Every mutation is backup-first and reversible.\n3. Migration decisions are logged with clear reason codes.\n4. Unit tests cover migration matrix and rollback-on-failure behavior.\n5. E2E tests include upgrade-from-older-install scenarios with detailed logs.","acceptance_criteria":"1. Legacy and partial installations are detected and migrated/repaired safely.\n2. Every migration mutation is backup-first and reversible.\n3. Migration reason codes and actions are logged in structured output.\n4. Unit tests cover migration matrix and rollback-on-error behavior.\n5. E2E validates upgrade-from-older-environment scenarios with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:10:55.352543453Z","created_by":"ubuntu","updated_at":"2026-02-14T23:55:23.151868746Z","closed_at":"2026-02-14T23:55:23.151697846Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["automation","installer","migration"],"dependencies":[{"issue_id":"bd-2j5.21","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:10:55.352543453Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.3","title":"Platform/Arch Artifact Resolution Contract for installer/updater","description":"## Deliverable\nDeterministic platform/architecture detection and release artifact resolution for installer/update flows.\n\n## Background and rationale\nInstaller reliability depends on downloading the right artifact every time. dcg resolves target triples and artifact names deterministically.\n\n## Scope\n- Detect OS + arch + ABI variants and map to release targets.\n- Define canonical artifact naming contract (`tar.xz` on Unix, zip on Windows where applicable).\n- Support explicit version pin and channel selection logic.\n- Define failure strategy when target is unsupported.\n\n## Acceptance criteria\n1. All supported host triples map to exactly one artifact URL.\n2. Unsupported hosts fail with actionable guidance.\n3. Resolution behavior is shared between installer and updater (no drift).","acceptance_criteria":"1. Supported OS/arch combinations map deterministically to release artifacts with no ambiguity.\n2. Unsupported targets fail fast with actionable remediation text.\n3. Mapping contract is shared by installer and updater.\n4. Unit tests cover all supported mappings and edge/unknown targets.\n5. Mapping assumptions are reflected in release contract validation.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.062961415Z","created_by":"ubuntu","updated_at":"2026-02-14T21:34:34.372495164Z","closed_at":"2026-02-14T21:34:34.372473052Z","close_reason":"Implemented deterministic installer/updater target resolver + release artifact contract validation with unit tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","platform","release"],"dependencies":[{"issue_id":"bd-2j5.3","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.062961415Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.3","depends_on_id":"bd-2j5.1","type":"blocks","created_at":"2026-02-14T21:04:03.724420450Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.4","title":"Installer Supply-Chain Verification (SHA256 + optional Sigstore)","description":"## Deliverable\nSupply-chain integrity pipeline with mandatory checksum verification and optional sigstore/cosign verification.\n\n## Background and rationale\nIntegrity validation is not optional in curl|bash workflows. It must be default-on and visible.\n\n## Scope\n- Download artifact checksum metadata and verify before extraction/install.\n- Implement optional signature verification path with graceful degradation messaging if cosign unavailable.\n- Add explicit `--no-verify` escape hatch with strong warning text.\n- Log/emit structured verification outcomes for diagnostics.\n\n## Acceptance criteria\n1. Tampered artifact is always rejected with clear reason.\n2. Missing checksum/signature metadata is handled predictably by policy.\n3. Verification status is visible in installer summary and machine-readable output.","acceptance_criteria":"1. Checksum verification is default and mandatory unless explicit bypass is requested.\n2. Optional signature verification path is implemented with clear degradation messaging.\n3. Bypass path is loud, explicit, and logged in structured output.\n4. Unit tests cover success/failure/tamper scenarios.\n5. E2E tests include integrity failure injection and remediation validation.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.154960402Z","created_by":"ubuntu","updated_at":"2026-02-14T21:38:43.595422377Z","closed_at":"2026-02-14T21:38:43.595403531Z","close_reason":"Implemented SHA256 enforcement + optional/required Sigstore policy contract with structured bypass semantics and tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","security","supply-chain"],"dependencies":[{"issue_id":"bd-2j5.4","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.154960402Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.4","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:03.812759218Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":61,"issue_id":"bd-2j5.4","author":"Dicklesworthstone","text":"Security decision note: checksum verification is baseline mandatory; signature verification is highly recommended but may be conditionally optional if cosign is absent. Any bypass () must be loud, explicit, and reflected in summary output/log metadata.","created_at":"2026-02-14T21:04:20Z"},{"id":65,"issue_id":"bd-2j5.4","author":"Dicklesworthstone","text":"Clarification: bypass flag name is --no-verify; using it must emit a high-visibility warning and appear in machine-readable result metadata.","created_at":"2026-02-14T21:04:28Z"}]}
{"id":"bd-2j5.5","title":"From-Source Fallback Install Mode with prerequisite checks","description":"## Deliverable\nFrom-source fallback install mode with prerequisite checks and deterministic build behavior.\n\n## Background and rationale\nWhen release assets are unavailable (airgapped, unsupported target, CI lag), users still need a path to success.\n\n## Scope\n- Implement `--from-source` path with toolchain checks and explicit prompts/remediations.\n- Support offline/limited-network behavior where feasible.\n- Ensure source-build mode respects destination, verification, and summary UX conventions.\n\n## Acceptance criteria\n1. Source mode can install from repository tag/commit with reproducible command flow.\n2. Missing prerequisites are reported with direct fix commands.\n3. Source mode is covered by installer tests.","acceptance_criteria":"1. From-source mode works deterministically with clear prerequisite validation.\n2. Source mode preserves destination, verification, and summary behavior parity.\n3. Missing prerequisites produce exact fix commands.\n4. Unit tests cover source-mode decision logic and prerequisite checks.\n5. E2E includes source-install scenario with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:03:39.263367275Z","created_by":"ubuntu","updated_at":"2026-02-15T00:05:57.610371400Z","closed_at":"2026-02-15T00:05:57.610306519Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["build","installer"],"dependencies":[{"issue_id":"bd-2j5.5","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.263367275Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.5","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:03.901294564Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.5","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:04:03.992642821Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.6","title":"Post-install automation: PATH, completions, and verification","description":"## Deliverable\nPost-install automation for PATH setup, shell completions, and immediate binary verification.\n\n## Background and rationale\nThe install is not complete until the command actually works in the user's shell with minimal manual edits.\n\n## Scope\n- Optional easy-mode PATH mutation for common shells with safe backups/idempotency.\n- Install completion scripts (bash/zsh/fish) automatically when requested.\n- Run post-install sanity command (`sbh --version`, optional health check) and report status.\n\n## Acceptance criteria\n1. New users can run `sbh` in a new shell without manual PATH surgery when easy mode is enabled.\n2. Completion install does not clobber unrelated shell config.\n3. Verification failures provide exact next-step commands.","acceptance_criteria":"1. PATH/completion automation is idempotent and backup-safe across supported shells.\n2. Post-install verification confirms binary usability and reports actionable remediation on failure.\n3. Automation outcomes are visible in both human summary and structured logs.\n4. Unit tests cover shell profile mutation edge cases.\n5. E2E validates end-to-end behavior in clean shell environments.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.364973615Z","created_by":"ubuntu","updated_at":"2026-02-14T23:49:31.015590483Z","closed_at":"2026-02-14T23:49:31.015511595Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["installer","shell","ux"],"dependencies":[{"issue_id":"bd-2j5.6","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.364973615Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.6","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:04.079556028Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.7","title":"Automatic AI tool integration bootstrap with backup-first safety","description":"## Deliverable\nAutomatic integration bootstrap for AI coding toolchains (Claude Code, Codex, Gemini, and related hook-capable CLIs) with backup-first semantics.\n\n## Background and rationale\ndcg succeeds partly because it wires itself into user workflows immediately. We need similar convenience while preserving safety and reversibility.\n\n## Scope\n- Detect known tool config files and hook registries.\n- Inject/update sbh integration entries idempotently.\n- Create timestamped backups before mutation and print restore guidance.\n- Support `--no-configure` mode for manual-only users.\n\n## Acceptance criteria\n1. Re-running bootstrap does not duplicate hooks or corrupt config files.\n2. Backups are always created and discoverable.\n3. Installer summary reports per-tool status (configured/skipped/failed + reason).","acceptance_criteria":"1. Integration bootstrap updates known tool configs idempotently with backup-first behavior.\n2. Re-runs never duplicate entries or corrupt configuration files.\n3. Per-tool status is surfaced in summary and JSON diagnostics.\n4. Unit tests cover merge/update and rollback-on-failure logic.\n5. E2E validates multi-tool bootstrap with detailed logs and artifacted backups.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.460926551Z","created_by":"ubuntu","updated_at":"2026-02-14T23:59:08.919914083Z","closed_at":"2026-02-14T23:59:08.919841177Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["automation","installer","integrations"],"dependencies":[{"issue_id":"bd-2j5.7","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.460926551Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.7","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:04.169771793Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.7","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:24.135011533Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.7","depends_on_id":"bd-2j5.6","type":"blocks","created_at":"2026-02-14T21:04:04.259944297Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j5.8","title":"sbh update command orchestration (check/apply/pin/system-user)","description":"## Deliverable\nA first-class `sbh update` command that orchestrates install/update flows with explicit policy controls.\n\n## Background and rationale\nUsers need an in-product update command, not just docs telling them to re-run installer scripts.\n\n## Scope\n- Implement `sbh update` command surface: check-only, install target version, force refresh, system/user destination controls.\n- Reuse installer logic for download/verify/install to avoid split behavior.\n- Support dry-run and machine-readable output.\n\n## Acceptance criteria\n1. `sbh update --check` is fast and safe for cron/agent usage.\n2. Updating to latest and pinned versions works across supported platforms.\n3. Update output clearly states old/new version and any required follow-up actions.","acceptance_criteria":"1. `sbh update` supports check/apply/pin/system-user controls with deterministic behavior.\n2. Update flow reuses installer integrity and artifact resolution logic (no drift).\n3. Update outcomes include old/new version and required follow-up actions.\n4. Unit tests cover command options, policy decisions, and error mapping.\n5. E2E covers update success/failure and rollback handoff with detailed logs.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T21:03:39.553479917Z","created_by":"ubuntu","updated_at":"2026-02-15T02:19:22.876874004Z","closed_at":"2026-02-15T02:19:22.876774046Z","close_reason":"Implemented sbh update command: UpdateArgs with --check/--version/--force/--system/--user/--no-verify/--dry-run, module-based orchestration in cli/update.rs with 11 unit tests (version resolution, report formatting, step tracking), 3 bin tests (CLI parsing, defaults, conflict validation). Shares artifact resolution and verification with installer via resolve_updater_artifact_contract/verify_artifact_supply_chain. Full pipeline: detect platform → resolve contract → check version → download via curl → SHA-256 verify → extract tar.xz → backup+install with rollback. Human and JSON output modes.","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","installer","update"],"dependencies":[{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.553479917Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.13","type":"blocks","created_at":"2026-02-14T21:04:04.614375957Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.16","type":"blocks","created_at":"2026-02-14T21:11:24.410580041Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.2","type":"blocks","created_at":"2026-02-14T21:04:04.349160069Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.21","type":"blocks","created_at":"2026-02-14T21:11:24.500008190Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:04.437743856Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.4","type":"blocks","created_at":"2026-02-14T21:04:04.524070744Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2j5.5","type":"blocks","created_at":"2026-02-14T21:11:24.319523991Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.8","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T21:04:04.705556369Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":62,"issue_id":"bd-2j5.8","author":"Dicklesworthstone","text":"Update orchestration note: updater must not fork behavior from installer. Shared code paths are required so install/update verification and artifact resolution semantics cannot drift over time.","created_at":"2026-02-14T21:04:20Z"}]}
{"id":"bd-2j5.9","title":"Update cache + background notice policy + opt-out controls","description":"## Deliverable\nUpdater version-intelligence cache and background notification policy with clear opt-out controls.\n\n## Background and rationale\nFast startup matters; repeated network calls for every command do not scale. dcg uses cached checks plus optional background refresh.\n\n## Scope\n- Add local cache for update metadata with TTL.\n- Implement lightweight background refresh policy where appropriate.\n- Support config/env opt-out for update checks and notices.\n\n## Acceptance criteria\n1. Update check latency stays low with warm cache.\n2. Cache expiry/refresh semantics are deterministic and test-covered.\n3. Users/agents can disable notices globally in config or env.","acceptance_criteria":"1. Update metadata caching reduces check latency while preserving correctness.\n2. TTL and refresh behavior are deterministic and configurable.\n3. Opt-out controls work via config/env without side effects.\n4. Unit tests cover cache lifecycle and opt-out logic.\n5. E2E includes stale-cache and offline-check scenarios with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T21:03:39.644217741Z","created_by":"ubuntu","updated_at":"2026-02-15T04:30:31.519562857Z","closed_at":"2026-02-15T04:30:31.519545735Z","close_reason":"Acceptance met: update metadata cache/TTL + refresh policy + opt-out controls are wired and test-covered (unit + integration stale-cache/offline); revalidated with rch cargo test --lib, rch cargo test --test integration_tests -- update_check_, and rch cargo check --all-targets.","source_repo":".","compaction_level":0,"original_size":0,"labels":["observability","update"],"dependencies":[{"issue_id":"bd-2j5.9","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:03:39.644217741Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j5.9","depends_on_id":"bd-2j5.8","type":"blocks","created_at":"2026-02-14T21:04:04.792737789Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":71,"issue_id":"bd-2j5.9","author":"Dicklesworthstone","text":"Progress (JadeBass): added update policy config + deterministic env override tests in src/core/config.rs and added file-backed cache module with expanded lifecycle tests in src/core/update_cache.rs. Validation: cargo fmt OK, rch exec \"cargo check --lib\" OK. Remaining blocker: src/cli/update.rs reserved by DustyDove and currently failing clippy/check-all-targets due unclosed delimiters + unsafe std::env::set_var in tests; awaiting fix/release to wire cache+notice behavior into updater command path.","created_at":"2026-02-15T02:27:45Z"},{"id":73,"issue_id":"bd-2j5.9","author":"Dicklesworthstone","text":"Update: retrying reservation for src/cli/update.rs + src/cli_app.rs still conflicts with DustyDove (reservation 386/387 expiring ~03:02Z). all-target rch checks currently fail in update.rs test code due unsafe std::env::set_var blocks under crate forbid(unsafe_code). Ready to wire cache/notices into updater command path immediately once reservation clears.","created_at":"2026-02-15T02:31:01Z"},{"id":74,"issue_id":"bd-2j5.9","author":"AmberFalcon","text":"Taking unblock slice: reserved src/cli/update.rs + src/cli_app.rs, fixing compile/lint blockers and wiring update-cache/notice behavior into updater path; will report rch check/clippy results.","created_at":"2026-02-15T03:04:48Z"},{"id":78,"issue_id":"bd-2j5.9","author":"AmberFalcon","text":"Completed updater cache/notice wiring slice: added --refresh-cache in cli_app UpdateArgs; mapped config.update.metadata_cache_file/ttl/notices_enabled into UpdateOptions via build_update_options(); wired cache-aware target metadata resolution in src/cli/update.rs (cache hit vs network refresh path), and gated notice follow-ups/Run","created_at":"2026-02-15T03:22:49Z"},{"id":79,"issue_id":"bd-2j5.9","author":"AmberFalcon","text":"Validation details for updater slice: cargo fmt --check OK; rch exec cargo check --all-targets OK; rch exec cargo test --lib update::tests::target_metadata_uses_cache_when_refresh_disabled OK; rch exec cargo test --lib update::tests::target_metadata_refresh_bypasses_cache_and_writes_new_entry OK; rch exec cargo test --lib update::tests::format_check_only_update_available_notice_suppressed OK; rch exec cargo test --bin sbh -- update_options_include_cache_and_notice_config OK. Full rch exec cargo clippy --all-targets -- -D warnings still fails on unrelated existing files outside src/cli/update.rs and src/cli_app.rs.","created_at":"2026-02-15T03:22:54Z"},{"id":81,"issue_id":"bd-2j5.9","author":"Dicklesworthstone","text":"CopperAnchor progress: added integration stale-cache/offline-check scenarios in tests/integration_tests.rs and env-aware CLI helper in tests/common/mod.rs. Validation currently blocked by compile issues in reserved files: src/cli/update.rs non-exhaustive TargetMetadataSource match (missing OfflineBundle arm) and src/cli/assets.rs missing test helper seed_cache_entry references.","created_at":"2026-02-15T03:28:41Z"},{"id":85,"issue_id":"bd-2j5.9","author":"Dicklesworthstone","text":"CopperAnchor update: added integration coverage for stale-cache/offline-check and isolated backup inventory paths in tests/integration_tests.rs (4 new tests) plus env-aware helper in tests/common/mod.rs. Validation: rch exec \"cargo test --test integration_tests -- update_\" PASS (8 update_* tests).","created_at":"2026-02-15T03:32:10Z"},{"id":90,"issue_id":"bd-2j5.9","author":"Dicklesworthstone","text":"Taking a non-overlapping bd-2j5.9 slice now: runtime enforcement of update.enabled + background_refresh behavior in src/cli_app.rs and src/cli/update.rs, with focused tests and rch validation. Avoiding reserved integration/e2e surfaces.","created_at":"2026-02-15T04:23:42Z"}]}
{"id":"bd-2ld","title":"Clippy debt burn-down slice: deletion/release lint blockers","description":"Reduce immediate clippy -D warnings blockers in unreserved files: remove unused self in src/scanner/deletion.rs and replace unchecked Instant subtraction patterns in src/ballast/release.rs with checked_sub-based setup in tests. Preserve behavior; validate with rch.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:23:11.582405963Z","created_by":"ubuntu","updated_at":"2026-02-15T16:26:29.274282766Z","closed_at":"2026-02-15T16:26:29.274264402Z","close_reason":"Completed targeted non-overlap clippy burn-down in deletion/release files with rch validation; touched lints resolved while global clippy remains blocked by unrelated files.","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":104,"issue_id":"bd-2ld","author":"LavenderOak","text":"Completed non-overlap clippy slice in reserved files. Changes: (1) src/scanner/deletion.rs removed unused self on dry-run logger helper by making log_dry_run associated and updating call site; (2) src/ballast/release.rs replaced unchecked Instant subtraction in tests with checked_sub helper one_hour_ago(). Validation: cargo fmt --check PASS; rch exec 'cargo test --lib release' PASS; rch exec 'cargo check --all-targets' PASS; rch exec 'cargo clippy --all-targets -- -D warnings' still FAIL due many unrelated pre-existing files (tests/stress_tests.rs, examples/*, src/decision_plane_tests.rs, etc.), with no remaining mentions of src/scanner/deletion.rs or src/ballast/release.rs in clippy error stream.","created_at":"2026-02-15T16:26:26Z"}]}
{"id":"bd-2m9","title":"sbh config and sbh ballast subcommands","description":"## Deliverable\nConfig management and ballast management subcommands.\n\n## Technical Approach\n### sbh config\n```bash\nsbh config show              # Show current effective config (merged layers)\nsbh config path              # Show config file path\nsbh config validate          # Validate config file\nsbh config set <key> <value> # Set a config value\nsbh config reset             # Reset to defaults\nsbh config diff              # Show diff between file and defaults\n```\n\n### sbh ballast\n```bash\nsbh ballast status           # Show ballast inventory\nsbh ballast provision        # Create ballast files (idempotent)\nsbh ballast release N        # Manually release N files\nsbh ballast replenish        # Manually replenish released files\nsbh ballast verify           # Check integrity of all files\n```\n\n### Implementation\nThese commands are relatively straightforward wrappers around the config system and ballast manager. The key value is providing CLI access to internals for debugging and manual intervention.\n\n## Acceptance Criteria\n- All config subcommands work correctly\n- Config validation produces helpful errors\n- Ballast status shows accurate inventory\n- Ballast verify checks file integrity\n- Manual release/replenish work correctly\n- JSON output available for all subcommands","acceptance_criteria":"1. Unit tests cover config key parsing, validation error messaging, and ballast command argument handling. 2. Integration tests validate effective-config layering and ballast manager command effects. 3. E2E scripts cover config show/set/validate/reset/diff and ballast status/provision/release/replenish/verify flows. 4. JSON and human outputs remain consistent and parseable under success and failure paths. 5. Detailed command logs include config source precedence, ballast inventory deltas, and safety guard decisions.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:53:28.335870700Z","created_by":"ubuntu","updated_at":"2026-02-15T00:22:16.453122972Z","closed_at":"2026-02-15T00:22:16.453057750Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-2m9","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T16:56:13.951667870Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2m9","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:14.034227557Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2m9","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:56:13.868829892Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2np","title":"Daemon self-monitoring and health reporting","description":"## Deliverable\nThe daemon monitors its own health: memory usage, thread status, scan latency, and reports this via sd_notify STATUS and the status command.\n\n## Technical Approach\n### Self-Monitoring Metrics\n```rust\npub struct DaemonHealth {\n    pub uptime: Duration,\n    pub memory_rss_bytes: u64,      // /proc/self/status on Linux\n    pub scan_count: u64,\n    pub avg_scan_duration: Duration,\n    pub last_scan_at: Instant,\n    pub deletions_total: u64,\n    pub bytes_freed_total: u64,\n    pub errors_total: u64,\n    pub thread_status: Vec<ThreadStatus>,\n    pub last_pressure_level: PressureLevel,\n}\n\npub enum ThreadStatus {\n    Running { name: String, last_heartbeat: Instant },\n    Stalled { name: String, stalled_since: Instant },\n    Dead { name: String, died_at: Instant, error: String },\n}\n```\n\n### Thread Heartbeats\nEach worker thread periodically writes to an AtomicU64 timestamp. The self-monitor checks these:\n- If heartbeat is > 60s stale → thread is stalled\n- Log warning and attempt restart\n\n### Health Report for sd_notify\n```\nSTATUS=Monitoring 4 mounts | GREEN 23.4% free on /data | 312 deletions (467 GB freed) | RSS 42 MB\n```\n\n### Stale State File\nWrite a JSON state file (/var/lib/sbh/state.json) periodically:\n- Allows `sbh status` to read daemon state even when not connecting directly\n- Includes last scan time, current pressure, recent activity summary\n- Written every 30 seconds\n\n## Acceptance Criteria\n- Memory usage tracked and bounded (< 256MB, alert if exceeded)\n- Thread stalls detected within 60 seconds\n- sd_notify STATUS updated every 30 seconds\n- State file written for status command\n- Self-monitoring overhead < 1% CPU","acceptance_criteria":"1. Unit tests cover heartbeat staleness detection, memory parsing, and state serialization correctness. 2. Integration tests validate self-monitor interaction with daemon threads and sd_notify updates. 3. E2E scenarios validate stalled-thread detection, alert emission, and recovery/restart handling. 4. Health state persists accurately across daemon restarts and degraded operation. 5. Detailed health logs include per-thread heartbeat age, memory trends, status payloads, and alert reason codes.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:55:20.178762071Z","created_by":"ubuntu","updated_at":"2026-02-15T00:29:26.706762811Z","closed_at":"2026-02-15T00:29:26.706696376Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon"],"dependencies":[{"issue_id":"bd-2np","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T16:56:03.726565338Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":13,"issue_id":"bd-2np","author":"Dicklesworthstone","text":"REVIEW: The state file (/var/lib/sbh/state.json) is the PRIMARY mechanism for CLI-to-daemon communication. Its format must be well-defined:\n\n```json\n{\n  \"version\": \"0.1.0\",\n  \"pid\": 12345,\n  \"started_at\": \"2026-02-14T10:00:00Z\",\n  \"uptime_seconds\": 307342,\n  \"last_updated\": \"2026-02-14T15:30:00Z\",\n  \"pressure\": {\n    \"overall\": \"green\",\n    \"mounts\": [\n      {\"path\": \"/data\", \"free_pct\": 23.4, \"level\": \"green\", \"rate_bps\": -12400000}\n    ]\n  },\n  \"ballast\": {\"available\": 8, \"total\": 10, \"released\": 2},\n  \"last_scan\": {\"at\": \"2026-02-14T15:29:55Z\", \"candidates\": 3, \"deleted\": 0},\n  \"counters\": {\"scans\": 1542, \"deletions\": 312, \"bytes_freed\": 467800000000, \"errors\": 2}\n}\n```\n\nATOMICITY: Write to state.json.tmp first, then rename() (atomic on POSIX). This prevents sbh status from reading a half-written file.\nSTALENESS: sbh status must check last_updated and warn if > 60s old (daemon might be stalled).\nPERMISSIONS: state.json readable by the user running sbh commands (0644).","created_at":"2026-02-14T17:14:49Z"}]}
{"id":"bd-2pi","title":"sbh install / sbh uninstall commands","description":"## Deliverable\nCommands to install and uninstall sbh as a system service, including ballast provisioning.\n\n## Technical Approach\n### sbh install\n```bash\nsbh install [OPTIONS]\n\nOptions:\n  --systemd          Install as systemd service (Linux, default on Linux)\n  --launchd          Install as launchd agent (macOS, default on macOS)\n  --user             Install as user-level service (no root required)\n  --ballast-count N  Number of ballast files to create (default: 10)\n  --ballast-size N   Size of each ballast file in MB (default: 1024)\n  --ballast-path P   Location for ballast files\n  --dry-run          Show what would be done without doing it\n```\n\n### Installation Steps\n1. Detect platform (Linux → systemd, macOS → launchd)\n2. Create data directory (/var/lib/sbh/ or equivalent)\n3. Write default config file\n4. Provision ballast files (with progress bar)\n5. Create SQLite database\n6. Generate and install service file\n7. Enable and start service\n8. Verify service is running\n9. Print summary\n\n### sbh uninstall\n```bash\nsbh uninstall [OPTIONS]\n\nOptions:\n  --keep-data        Keep data directory and logs\n  --keep-ballast     Keep ballast files (don't reclaim space)\n  --force            Don't ask for confirmation\n```\n\n### Uninstall Steps\n1. Stop service\n2. Disable service\n3. Remove service file\n4. Optionally remove data directory (with confirmation)\n5. Optionally remove ballast files (with confirmation)\n6. Print summary\n\n### Progress Reporting\nBallast provisioning (writing 10GB) takes time. Show progress:\n```\nInstalling sbh v0.1.0...\n  Creating data directory... done\n  Writing default config... done\n  Provisioning ballast files:\n    [████████████████░░░░] 8/10 (8.0 GB) - SBH_BALLAST_FILE_00008.dat\n  Creating activity database... done\n  Installing systemd service... done\n  Starting service... done\n\n✓ sbh installed successfully\n  Service: sbh.service (running)\n  Config:  /etc/sbh/config.toml\n  Data:    /var/lib/sbh/\n  Ballast: 10 files × 1 GB = 10 GB reclaimable\n```\n\n## Acceptance Criteria\n- Install works on Linux (systemd) and macOS (launchd)\n- Idempotent: running install twice doesn't break anything\n- Uninstall cleanly removes all components\n- Progress bar during ballast provisioning\n- --dry-run shows plan without executing\n- --user mode works without root\n- Verification step confirms service is actually running\n- Integration test: install → verify → uninstall → verify clean","acceptance_criteria":"1. `sbh install` and `sbh uninstall` remain idempotent and deterministic across Linux/macOS paths.\n2. Install command integrates wizard/auto flow, post-install automation, integration bootstrap, and structured diagnostics.\n3. Uninstall command integrates safe cleanup modes and reversible teardown behavior.\n4. Unit tests validate install/uninstall option handling and safety decisions.\n5. E2E validates install -> verify -> update -> rollback -> uninstall with detailed logs.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:52:04.488580920Z","created_by":"ubuntu","updated_at":"2026-02-15T00:20:36.178919253Z","closed_at":"2026-02-15T00:20:36.178840786Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-2pi","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T16:56:12.918868574Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-26g","type":"blocks","created_at":"2026-02-14T16:56:12.661746133Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:04:03.190515957Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.11","type":"blocks","created_at":"2026-02-14T21:11:27.829276743Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.16","type":"blocks","created_at":"2026-02-14T21:11:27.923847249Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.17","type":"blocks","created_at":"2026-02-14T21:11:28.013351431Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.18","type":"blocks","created_at":"2026-02-14T21:11:28.105068058Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.6","type":"blocks","created_at":"2026-02-14T21:04:07.389022003Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2j5.7","type":"blocks","created_at":"2026-02-14T21:04:07.475512027Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2pi","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:12.832915905Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":20,"issue_id":"bd-2pi","author":"Dicklesworthstone","text":"ENHANCEMENT (idea-wizard): Add interactive first-run wizard mode to sbh install. When run without flags, it should:\n  1. Detect all mounted filesystems and show them in a table\n  2. Auto-suggest scan paths based on discovered project directories\n  3. Show estimated reclaimable space (quick scan with built-in patterns)\n  4. Let user adjust scan paths, exclusions, and ballast sizing\n  5. Show config summary and ask for confirmation\n  6. Create config, provision ballast, install service\n  7. Run first scan and show results\nThis replaces the current 'install with defaults' approach with a guided experience. For headless: sbh install --auto uses all defaults without prompting.","created_at":"2026-02-14T18:35:03Z"},{"id":51,"issue_id":"bd-2pi","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-3i3 (launchd) dependency. macOS launchd support is optional at compile time. Install command detects platform: Linux -> systemd (bd-26g, required dep), macOS -> launchd (bd-3i3, wired in when available). If bd-3i3 is not yet implemented, 'sbh install' on macOS returns: 'macOS service installation not yet available. Run sbh daemon in foreground instead.'","created_at":"2026-02-14T19:03:09Z"}]}
{"id":"bd-2pj","title":"Cross-platform filesystem stats collector","description":"## Deliverable\nEfficient filesystem statistics collection that works across all target platforms, with caching and rate limiting.\n\n## Technical Approach\n### FsStatsCollector\nWraps the PAL's fs_stats() with intelligent caching and batching:\n\n```rust\npub struct FsStatsCollector {\n    platform: Arc<dyn Platform>,\n    cache: HashMap<PathBuf, CachedStats>,\n    cache_ttl: Duration,  // default: 1 second\n}\n\nstruct CachedStats {\n    stats: FsStats,\n    collected_at: Instant,\n}\n```\n\n### Batch Collection\nWhen monitoring multiple paths, group by mount point to avoid redundant statvfs calls:\n- /data/projects/foo and /data/projects/bar → single statvfs for /data mount\n- /tmp/build1 and /tmp/build2 → single statvfs for /tmp mount\n\n### Rate Limiting\nstatvfs is a syscall; avoid calling it too frequently:\n- Cache results for configurable TTL (default 1s)\n- Under high pressure, reduce TTL to 100ms\n- During normal operation, 5s TTL is fine\n\n### Zero-Allocation Hot Path\nThe stats check in the monitoring loop should not allocate:\n- Pre-allocate the stats buffer\n- Reuse PathBuf instances\n- Use stack-allocated arrays for mount point looking\n\n## Design Rationale\nstatvfs is fast (~1μs on Linux) but calling it hundreds of times per second across many mount points adds up. Caching by mount point deduplicates the work. The cache TTL adapts to pressure level: more frequent checks when storage is under pressure.\n\n## Acceptance Criteria\n- Correctly reports free/total/available for all filesystems\n- Caching prevents redundant syscalls (verifiable by counting calls)\n- Mount point deduplication works (paths on same mount = single call)\n- Thread-safe for concurrent access\n- Unit tests with MockPlatform","acceptance_criteria":"1. Unit tests validate cache TTL behavior, mount deduplication, and concurrency safety. 2. Integration tests verify PAL-backed stats correctness across multiple mount configurations. 3. E2E scenarios exercise mixed-path monitoring and confirm no redundant stats collection regressions. 4. Performance bounds are met under high-frequency polling workloads. 5. Detailed collector logs include mount key, cache hit/miss, syscall timing, and normalization decisions.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:44:47.357260610Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:22.025631582Z","closed_at":"2026-02-14T19:45:22.025610593Z","close_reason":"Filesystem stats collector implemented in monitor/fs_stats.rs (274 lines): FsStatsCollector with cache-aware collection, per-mount caching with TTL, mount point deduplication via collect_many(), cache expiry pruning, unit tests for dedup and cache hits.","source_repo":".","compaction_level":0,"original_size":0,"labels":["monitoring"],"dependencies":[{"issue_id":"bd-2pj","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T16:55:39.093006348Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2q9","title":"E2E test script with detailed logging","description":"## Deliverable\nShell-based end-to-end test script suite that exercises CLI, daemon, policy modes, and recovery paths with detailed structured logs.\n\n## Scope Expansion (Plan-Space Revision)\nE2E scripts must cover:\n- core CLI commands and JSON output validation\n- scan/clean/ballast lifecycles\n- predictive pipeline + fallback-safe transitions\n- emergency zero-write mode\n- project protection markers\n- process attribution output paths\n- status/dashboard/stats visibility checks\n- decision-plane shadow/canary trace assertions\n\n## Logging Requirements\nEach test case logs:\n- command executed\n- expected vs actual exit status\n- expected vs actual output constraints\n- timing and environment metadata\n- trace IDs and fallback reason codes when present\n\nOutput artifacts:\n- machine-readable JSON summary\n- per-test logs\n- compact human summary for CI console\n\n## Acceptance Criteria\n- comprehensive command and policy-mode coverage\n- deterministic pass/fail behavior\n- explicit cleanup on success/failure paths\n- integrated into CI e2e stage\n- runtime budget and flaky-test guardrails documented","acceptance_criteria":"1. E2E script suite covers normal, degraded, and emergency user workflows without feature gaps. 2. Scenarios validate CLI UX plus daemon behavior and decision-plane transitions where applicable. 3. Every scenario emits verbose structured logs with timestamps, trace IDs, and assertion context. 4. Scripts produce deterministic exit codes and machine-readable summary artifacts for CI. 5. Flake diagnostics are captured with retry metadata and first-failure preservation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:54:25.464012324Z","created_by":"ubuntu","updated_at":"2026-02-15T01:40:35.299786122Z","closed_at":"2026-02-15T01:40:35.299714828Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["testing"],"dependencies":[{"issue_id":"bd-2q9","depends_on_id":"bd-112","type":"blocks","created_at":"2026-02-14T18:59:00.842388131Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-1fr","type":"blocks","created_at":"2026-02-14T18:58:59.964818117Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-14T16:56:22.863704391Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-224","type":"blocks","created_at":"2026-02-14T19:02:46.273892560Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T19:02:46.357380229Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-26g","type":"blocks","created_at":"2026-02-14T19:02:46.614830375Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2it","type":"blocks","created_at":"2026-02-14T18:59:00.144512471Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2m9","type":"blocks","created_at":"2026-02-14T18:59:00.308803746Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2np","type":"blocks","created_at":"2026-02-14T19:02:46.443207597Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2pi","type":"blocks","created_at":"2026-02-14T16:56:22.945003658Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2qa","type":"blocks","created_at":"2026-02-14T18:59:00.665961857Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-2s9","type":"blocks","created_at":"2026-02-14T19:02:46.529483807Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-395","type":"blocks","created_at":"2026-02-14T18:59:00.753517468Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-3i3","type":"blocks","created_at":"2026-02-14T19:02:46.700967394Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-3qm","type":"blocks","created_at":"2026-02-14T18:59:00.576601567Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T19:02:46.787091228Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-3s5","type":"blocks","created_at":"2026-02-14T18:59:00.928514707Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-7ls","type":"blocks","created_at":"2026-02-14T19:02:46.875140797Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-g0c","type":"blocks","created_at":"2026-02-14T18:59:00.490213671Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-nhm","type":"blocks","created_at":"2026-02-14T18:59:00.053700112Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q9","depends_on_id":"bd-p2u","type":"blocks","created_at":"2026-02-14T18:59:00.398730637Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":9,"issue_id":"bd-2q9","author":"Dicklesworthstone","text":"REVIEW: Additional E2E test cases needed:\n\n23. **Signal handling SIGHUP**: Start daemon → send SIGHUP → verify config reload logged → verify daemon still running\n24. **Signal handling SIGUSR1**: Start daemon → send SIGUSR1 → verify immediate scan triggered\n25. **Concurrent CLI access**: Start daemon → run 'sbh status' while daemon is running → verify no deadlock or corruption\n26. **Ballast concurrent access**: Start daemon → run 'sbh ballast release 1' while daemon monitors → verify file locking works (no race condition)\n27. **Scoring determinism**: Run 'sbh scan --json' twice on identical directory → verify JSON output is byte-identical\n28. **JSONL tailing**: Start daemon → tail -f JSONL file → trigger scan → verify events appear in real-time with complete JSON lines (no partial writes)\n29. **Large directory tree**: Create 10,000+ files in nested dirs → 'sbh scan' completes in < 10s\n30. **Exit codes**: Verify all documented exit codes (0, 1, 2, 3, 4) match their descriptions\n\nAlso: Each test should capture TIMING information (start/end timestamps) and log it, so we can detect performance regressions across runs.","created_at":"2026-02-14T17:14:00Z"},{"id":24,"issue_id":"bd-2q9","author":"Dicklesworthstone","text":"ADDITIONAL E2E TESTS for new features (idea-wizard beads):\n31. **Emergency mode**: sbh emergency --dry-run /tmp/test_dir -> finds test artifacts, scores them, no deletions\n32. **Emergency mode auto**: Create test artifacts -> sbh emergency --yes --target-free 20 -> verify artifacts deleted, space freed\n33. **Emergency mode zero-write**: Run sbh emergency --dry-run under strace -> verify ZERO write syscalls\n34. **Project protection**: touch /tmp/test_dir/.sbh-protect -> sbh scan -> verify test_dir NOT in candidates\n35. **Protection CLI**: sbh protect /tmp/test -> verify file created -> sbh unprotect /tmp/test -> verify removed\n36. **Pre-build check**: Fill test partition to 95% -> sbh check -> verify exit code 2 (CRITICAL)\n37. **Pre-build check OK**: Fresh test partition at 50% -> sbh check -> verify exit code 0\n38. **Blame command**: Create test artifacts with known process CWDs -> sbh blame --json -> verify attribution","created_at":"2026-02-14T18:37:00Z"}]}
{"id":"bd-2qa","title":"Emergency zero-write recovery mode (sbh emergency)","description":"## Deliverable\nA zero-write emergency mode that works when the disk is already critically full — the exact moment users need sbh most.\n\n## The Problem\nUsers install sbh PRECISELY when their disk is full. But the normal startup sequence requires creating directories, writing config, creating SQLite database, provisioning ballast files. ALL of these fail on a full disk. sbh can't help in the exact scenario it exists for.\n\n## The Solution: sbh emergency [paths...]\n\nA completely self-contained mode that:\n- Performs ZERO disk writes (no database, no JSONL, no config file)\n- Scans specified paths immediately using built-in patterns\n- Scores and ranks all candidates using default scoring weights\n- Shows interactive deletion list (like sbh clean but stateless)\n- All output to stdout/stderr only\n- Works with zero configuration — pure binary execution\n- Supports automation: sbh emergency --yes --target-free 10 /data\n\n### CLI Interface\n  sbh emergency                        # Scan common paths (/data, /tmp, /home)\n  sbh emergency /data/projects         # Scan specific path\n  sbh emergency --yes --target-free 10 # Auto-delete until 10% free\n  sbh emergency --top 20               # Show top 20 candidates only\n  sbh emergency --min-score 0.7        # Only high-confidence candidates\n  sbh emergency --dry-run              # Just show what would be deleted\n\n### Implementation\nEmergency mode creates no objects requiring persistence:\n- ScoringEngine with default weights (hardcoded, no config file read)\n- DirectoryWalker configured for the specified paths\n- PatternRegistry with built-in patterns only (no custom patterns from config)\n- No ActivityLogger — all output goes to stderr for logging, stdout for JSON\n- No BallastManager, No EWMA, No PID controller — crisis mode, delete by score alone\n- No /proc/*/fd open-file checks in emergency (too slow when disk is thrashing)\n\n### Output Format\n  sbh emergency — ZERO-WRITE RECOVERY MODE\n  Warning: /data is 99.2% used (8.1 GB free / 1.8 TB total)\n  Scanning /data/projects... found 47 candidates (182.3 GB reclaimable)\n  [interactive deletion list with scores, sizes, ages]\n  Delete items? [y/N/a(ll)/s(kip)/q(uit)]\n\n### Post-Recovery Guidance\nAfter freeing space, print: \"Emergency cleanup complete: freed 87.3 GB. Next: run sbh install for ongoing protection.\"\n\n### Safety\n- Hard vetoes still enforced: .git directories, system paths, files < 30 min old\n- Requires user confirmation unless --yes flag\n- Clear EMERGENCY MODE banner\n\n## Design Rationale\nThis is the single most impactful UX improvement. Every user's first interaction with sbh will likely be \"my disk is full, help!\" The implementation reuses scoring engine + walker + pattern registry WITHOUT the persistence layer. It is the pure-computation core of sbh exposed directly.\n\n## Acceptance Criteria\n- Works on a 99%+ full disk (zero disk writes verified via strace)\n- Correctly finds and scores build artifacts using built-in patterns\n- Interactive confirmation before deletion\n- --yes flag enables automated operation\n- --target-free stops cleanup when target free% is reached\n- --dry-run shows candidates without deleting\n- Post-recovery message suggests sbh install\n- All hard vetoes still enforced\n- Works with zero configuration (no config file needed)\n- Exit codes: 0=success, 1=no candidates, 2=user cancelled\n- Unit test: scoring without persistence layer\n- Integration test: create artifacts -> emergency scan -> delete -> verify\n- E2E test: verify zero disk writes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:30:40.968645441Z","created_by":"ubuntu","updated_at":"2026-02-14T21:53:05.778243349Z","closed_at":"2026-02-14T21:53:05.778176153Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","scanner"],"dependencies":[{"issue_id":"bd-2qa","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T18:53:34.927473553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2qa","depends_on_id":"bd-1sw","type":"blocks","created_at":"2026-02-14T18:34:11.505930727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2qa","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T18:34:11.601272755Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2qa","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T18:34:11.803745003Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2qa","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T18:34:11.696849182Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":27,"issue_id":"bd-2qa","author":"Dicklesworthstone","text":"REVIEW: Emergency mode interaction with protection system (bd-3qm): Emergency mode depends on walker (bd-1w9) which depends on protection (bd-3qm). But emergency mode has NO config file. The protection system handles this via marker-only mode (see bd-3qm comment). Emergency mode WILL honor .sbh-protect marker files (they are on-disk, zero-config). Emergency mode WILL NOT honor config-level glob patterns (scanner.protected_paths). This is documented and acceptable — marker files are the user's explicit intent, while config patterns may be stale or from a different context. Emergency mode output should note: 'Config-level protections are not active in emergency mode. Only .sbh-protect marker files are honored.'","created_at":"2026-02-14T18:53:26Z"},{"id":29,"issue_id":"bd-2qa","author":"Dicklesworthstone","text":"REVIEW: Emergency mode needs deletion capability. Added bd-1hh (deletion executor) as dependency. Emergency mode creates a DeletionExecutor with dry_run=false (unless --dry-run flag) but skips the circuit breaker (emergency = delete everything above score threshold). The executor's pre-flight safety checks (re-verify exists, check .git, verify permissions) still apply. Emergency mode uses a simplified deletion flow: score -> rank -> present to user -> delete confirmed items. No batch-then-remeasure loop needed since there is no PID target — just delete what the user confirms.","created_at":"2026-02-14T18:53:39Z"}]}
{"id":"bd-2rq","title":"CLI framework with clap derive and subcommands","description":"## Deliverable\nThe main CLI argument parser using clap derive, defining all subcommands and their options.\n\n## Technical Approach\n### CLI Structure\n```\nsbh [OPTIONS] <COMMAND>\n\nCommands:\n  daemon     Run the monitoring daemon (foreground or background)\n  install    Install sbh as a system service\n  uninstall  Remove sbh system service\n  status     Show current system status and pressure levels\n  stats      Show activity statistics and metrics\n  scan       Run a manual scan for build artifacts\n  clean      Manually clean build artifacts (with confirmation)\n  ballast    Manage ballast files\n  config     View/edit configuration\n  version    Show version and build info\n  help       Show help\n\nOptions:\n  --config <PATH>   Override config file path\n  --json            Output as JSON (for agent consumption)\n  --no-color        Disable colored output\n  --verbose         Increase log verbosity\n  --quiet           Quiet mode (errors only)\n```\n\n### Clap Derive Setup\n```rust\n#[derive(Parser)]\n#[command(name = \"sbh\", about = \"Storage Ballast Helper - Disk Space Guardian\")]\n#[command(version, long_about = None)]\npub struct Cli {\n    #[command(subcommand)]\n    pub command: Command,\n    \n    #[arg(long, global = true)]\n    pub config: Option<PathBuf>,\n    \n    #[arg(long, global = true)]\n    pub json: bool,\n    \n    #[arg(long, global = true)]\n    pub no_color: bool,\n    \n    #[arg(short, long, global = true)]\n    pub verbose: bool,\n    \n    #[arg(short, long, global = true)]\n    pub quiet: bool,\n}\n\n#[derive(Subcommand)]\npub enum Command {\n    Daemon(DaemonArgs),\n    Install(InstallArgs),\n    Uninstall(UninstallArgs),\n    Status(StatusArgs),\n    Stats(StatsArgs),\n    Scan(ScanArgs),\n    Clean(CleanArgs),\n    Ballast(BallastArgs),\n    Config(ConfigArgs),\n}\n```\n\n### Dual Output Mode\nFollowing dcg's pattern:\n- Human mode: colored terminal output with tables and progress indicators\n- JSON mode: structured JSON to stdout for agent consumption\n- Auto-detect based on TTY + --json flag + SBH_OUTPUT_FORMAT env var\n\n### Exit Codes\n```\n0 → Success\n1 → User error (bad arguments, invalid config)\n2 → Runtime error (permission denied, disk error)\n3 → Internal error (bug)\n4 → Partial success (some items cleaned, some failed)\n```\n\n## Acceptance Criteria\n- All subcommands parse correctly\n- --help produces clear, helpful output\n- --version shows version + build metadata\n- --json flag works globally\n- --no-color respected\n- Exit codes match specification\n- Shell completions generatable (bash, zsh, fish)\n- Unit tests for argument parsing edge cases","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:51:48.582281754Z","created_by":"ubuntu","updated_at":"2026-02-14T21:25:35.045331351Z","closed_at":"2026-02-14T21:25:35.045310492Z","close_reason":"Completed: clap-derive CLI framework with global flags, expanded subcommands, JSON/human output mode, completions generation, parser tests, and updated integration/e2e scaffolding.","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-2rq","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:56:12.581084079Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":34,"issue_id":"bd-2rq","author":"Dicklesworthstone","text":"REVIEW: New subcommands that must be wired into the CLI framework: (1) sbh emergency [paths...] (bd-2qa) — zero-write recovery mode. (2) sbh protect <path> / sbh unprotect <path> / sbh protect --list (bd-3qm) — project protection. (3) sbh tune / sbh tune --apply (bd-7vl) — auto-tuning recommendations. (4) sbh check [path] (bd-g0c) — pre-build disk check. (5) sbh blame (bd-395) — agent/process attribution. (6) sbh dashboard (bd-3s5) — live TUI dashboard. All are gated behind their respective beads as dependencies, but bd-2rq must define the clap subcommand enum variants for all of them upfront so the CLI structure is complete.","created_at":"2026-02-14T18:54:09Z"}]}
{"id":"bd-2s9","title":"Signal handling, graceful shutdown, and watchdog heartbeat","description":"## Deliverable\nRobust signal handling for the daemon: SIGTERM/SIGINT for graceful shutdown, SIGHUP for config reload, plus watchdog heartbeat for systemd integration.\n\n## Technical Approach\n### Signal Handler\n```rust\npub struct SignalHandler {\n    shutdown_flag: Arc<AtomicBool>,\n    reload_flag: Arc<AtomicBool>,\n}\n```\n\nUsing signal-hook crate for safe signal handling:\n- SIGTERM/SIGINT → set shutdown_flag, wake monitoring loop\n- SIGHUP → set reload_flag (config reload)\n- SIGUSR1 → trigger immediate scan and report\n\n### Graceful Shutdown Sequence\n1. Set shutdown_flag (all threads check this)\n2. Stop accepting new scan requests\n3. Wait up to 30 seconds for in-progress operations to complete\n4. Flush all logger buffers\n5. Close database connections\n6. Log shutdown event with stats summary\n7. Exit 0\n\n### Config Reload (SIGHUP)\n1. Re-read config file\n2. Validate new config\n3. If valid: apply changes, log what changed\n4. If invalid: log error, keep running with old config\n\n### Watchdog Heartbeat (systemd)\nEvery 30 seconds (half of WatchdogSec=60), notify systemd:\n- sd_notify(WATCHDOG=1)\n- Include STATUS=... with current state\n\nIf the monitoring loop is stuck (deadlock, infinite loop), the watchdog timeout fires and systemd restarts sbh.\n\n### Platform Considerations\n- Linux: signal-hook + sd_notify\n- macOS: signal-hook only (no sd_notify equivalent)\n- Windows: SetConsoleCtrlHandler for Ctrl-C\n\n## Acceptance Criteria\n- SIGTERM/SIGINT triggers graceful shutdown within 30s\n- SIGHUP reloads config without restart\n- SIGUSR1 triggers immediate scan\n- Watchdog heartbeat keeps systemd happy\n- Invalid config on reload doesn't crash the daemon\n- All pending operations complete before shutdown\n- Unit tests for signal flag handling\n- Integration test: send signals → verify correct behavior","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:51:31.888092520Z","created_by":"ubuntu","updated_at":"2026-02-14T20:09:03.058863186Z","closed_at":"2026-02-14T20:09:03.058840854Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon"],"dependencies":[{"issue_id":"bd-2s9","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T16:56:03.641499899Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":10,"issue_id":"bd-2s9","author":"Dicklesworthstone","text":"REVIEW: This bead requires the signal-hook crate (already added to bd-1kn's dependency list during review). The nix crate's signal module may also be useful for POSIX signal manipulation (sigprocmask, etc.), which is already a dependency. signal-hook provides safe, higher-level abstractions over raw signal handling that prevent common footguns (e.g., calling non-signal-safe functions in signal handlers).","created_at":"2026-02-14T17:14:09Z"},{"id":42,"issue_id":"bd-2s9","author":"Dicklesworthstone","text":"MINOR: SIGHUP reload clarification — on SIGHUP, the daemon should: (1) Re-read config file from disk. (2) Update in-memory config (pressure thresholds, PID gains, scan intervals, scoring weights). (3) Re-discover special locations. (4) Log 'Configuration reloaded' event. It should NOT: restart threads, re-initialize SQLite, re-provision ballast (those require full restart). Implementation: signal_hook::iterator::Signals in the main loop checks for SIGHUP each cycle.","created_at":"2026-02-14T18:54:57Z"}]}
{"id":"bd-2up","title":"Predictive action pipeline with early warning alerts","description":"## Deliverable\nTransform sbh from reactive (acts after thresholds are crossed) to predictive (acts before thresholds are crossed based on EWMA forecasts). Includes early warning alerts.\n\n## The Problem\nThe EWMA estimator (bd-x1k) already computes time_to_exhaustion and acceleration. The PID controller (bd-3po) already determines pressure levels. But the main loop (bd-48o) only acts on CURRENT pressure levels — it waits for thresholds to actually be crossed before doing anything.\n\nThis means: if 10 agents simultaneously start cargo builds and disk consumption jumps from 0 MB/s to 500 MB/s, sbh won't start cleaning until the disk actually hits the threshold (say 15% free). By then, with 500 MB/s consumption, the disk may be at 5% within 3 minutes. The reactive approach leaves very little time for cleanup.\n\n## The Solution: Predictive Pre-emptive Action\n\n### Predictive Action Policy\nNew module that maps EWMA predictions to pre-emptive actions:\n\n```rust\npub struct PredictiveActionPolicy {\n    config: PredictiveConfig,\n}\n\npub struct PredictiveConfig {\n    pub enabled: bool,                    // default: true\n    pub action_horizon_minutes: f64,      // default: 30.0\n    pub warning_horizon_minutes: f64,     // default: 60.0  \n    pub min_confidence: f64,              // default: 0.7\n    pub min_samples: u64,                 // default: 5\n}\n\npub enum PredictiveAction {\n    /// No predicted issue within horizon\n    Clear,\n    /// Disk will be full within warning horizon — log and increase monitoring\n    EarlyWarning {\n        mount: PathBuf,\n        minutes_remaining: f64,\n        confidence: f64,\n    },\n    /// Disk will be full within action horizon — start pre-emptive cleanup\n    PreemptiveCleanup {\n        mount: PathBuf,\n        minutes_remaining: f64,\n        confidence: f64,\n        recommended_free_target_pct: f64,\n    },\n    /// Imminent disk full (< 5 min) — emergency measures\n    ImminentDanger {\n        mount: PathBuf,\n        minutes_remaining: f64,\n    },\n}\n\nimpl PredictiveActionPolicy {\n    pub fn evaluate(&self, estimate: &RateEstimate, current_free_pct: f64) -> PredictiveAction;\n}\n```\n\n### Timeline-Based Response\n```\ntime_to_exhaustion  │  Action\n> 60 min            │  Clear — no action\n30-60 min           │  EarlyWarning — log WARNING, increase scan frequency to 10s\n10-30 min           │  PreemptiveCleanup — start scanning and deleting high-score artifacts\n5-10 min            │  PreemptiveCleanup (aggressive) — lower score threshold, delete more\n< 5 min             │  ImminentDanger — release ballast + aggressive cleanup\n< 2 min             │  ImminentDanger (critical) — release ALL ballast + emergency cleanup\n```\n\n### Key Innovation: Acting BEFORE Thresholds\nThe current system: \"disk is 92% full -> pressure is Orange -> start cleanup\"\nThe predictive system: \"disk is 78% full but filling at 500 MB/s -> predicted 95% in 28 min -> start gentle cleanup NOW while disk is still healthy\"\n\nThis means:\n1. Cleanup has MORE time (starting 30 min early vs 3 min before critical)\n2. Cleanup can be GENTLER (lower urgency, higher score threshold)\n3. Less chance of user-impacting aggressive deletion\n4. Disk never actually enters critical state\n\n### Early Warning Alerts\nWhen predictions cross thresholds, emit structured events:\n\n```json\n{\"ts\":\"2026-02-14T16:30:00Z\",\"event\":\"predictive_warning\",\"mount\":\"/data\",\"minutes_remaining\":42.3,\"confidence\":0.85,\"rate_mbps\":487.2,\"trend\":\"accelerating\",\"message\":\"At current rate, /data will be full in ~42 minutes\"}\n```\n\nThese events go to:\n1. Activity log (SQLite + JSONL)\n2. systemd journal (sd_notify STATUS update)\n3. Optional: notification channel (if notification system bead is implemented)\n\n### Confidence Gating\nPre-emptive action is ONLY taken when:\n- EWMA has >= min_samples (default 5) — need enough data\n- Prediction confidence >= min_confidence (default 0.7) — high-variance predictions are unreliable\n- Trend is Accelerating or Stable (not Recovering or Decelerating)\n- Current free% is actually declining (not just noisy measurement)\n\nThis prevents false alarms from brief spikes (e.g., one large file creation that immediately finishes).\n\n### Configuration\n```toml\n[pressure.prediction]\nenabled = true\naction_horizon_minutes = 30\nwarning_horizon_minutes = 60\nmin_confidence = 0.7\nmin_samples = 5\n```\n\n## Design Rationale\nThis transforms sbh from reactive to proactive. The EWMA data already exists — this connects it to the action pipeline earlier. A 30-minute prediction window means sbh starts gentle cleanup well before an emergency. Users never see a critical alert because the situation was handled proactively. This is THE key differentiator from a dumb cron job running find -delete.\n\n## Acceptance Criteria\n- Pre-emptive cleanup starts when predicted exhaustion < action_horizon\n- Cleanup intensity proportional to time remaining (not just current pressure)\n- Confidence gating prevents false positives\n- Early warning events logged with structured data\n- Does NOT act on low-confidence predictions\n- Does NOT act when trend is Recovering/Decelerating\n- Integrates cleanly with PID controller (advisory, not replacement)\n- Configuration allows tuning horizons and confidence thresholds\n- Unit tests: prediction to action mapping with synthetic scenarios\n- Test: 500 MB/s consumption rate at 80% full -> pre-emptive cleanup starts\n- Test: brief spike (1 min) -> no pre-emptive action (insufficient samples)\n- Test: steady recovery after cleanup -> pre-emptive actions stop\n- Integration test: full pipeline with predictive + reactive working together","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:32:51.922586640Z","created_by":"ubuntu","updated_at":"2026-02-14T22:55:27.840003403Z","closed_at":"2026-02-14T22:55:27.839980961Z","close_reason":"Fully implemented by another agent: PredictiveActionPolicy, PredictiveConfig, PredictiveAction enum, 22+ tests in src/monitor/predictive.rs. Config integrated into PressureConfig.","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon","monitoring"],"dependencies":[{"issue_id":"bd-2up","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T18:34:25.376869088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2up","depends_on_id":"bd-x1k","type":"blocks","created_at":"2026-02-14T18:34:25.300776653Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":44,"issue_id":"bd-2up","author":"Dicklesworthstone","text":"MINOR: Notification wiring clarification — when predictive action triggers an EarlyWarning or PreemptiveCleanup, it should emit a structured event that the notification system (bd-112) can pick up IF notifications are configured. The predictive module itself does NOT depend on the notification system. Instead, it emits events to the same ActivityEvent channel that all threads use. The notification system subscribes to that channel and filters by event type and min_level.","created_at":"2026-02-14T18:55:09Z"}]}
{"id":"bd-2xk","title":"Clippy debt burn-down slice: walker + decision_record test lint blockers","description":"Fix clippy -D warnings blockers in src/scanner/walker.rs and src/scanner/decision_record.rs (needless_collect/significant_drop_tightening and float_cmp in tests) while preserving behavior, with focused rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:29:12.804265531Z","created_by":"ubuntu","updated_at":"2026-02-15T16:33:24.734756936Z","closed_at":"2026-02-15T16:33:24.734731558Z","close_reason":"Completed non-overlap walker+decision_record clippy slice with rch check/test validation; global clippy remains blocked by unrelated files.","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":108,"issue_id":"bd-2xk","author":"Dicklesworthstone","text":"Implemented non-overlap clippy cleanup in src/scanner/walker.rs + src/scanner/decision_record.rs. Changes: replaced needless collect+contains pattern with iterator any in handles_empty_directory test; tightened lock guard lifetime in protection_registry_updated_during_walk via explicit drop(prot); replaced float assert_eq comparisons with to_bits equality for deterministic float comparisons in decision_record tests. Validation: rch exec \"cargo check --all-targets\" PASS; rch exec \"cargo test --lib walker\" PASS (14 tests); rch exec \"cargo test --lib decision_record\" PASS (27 tests); rch exec \"cargo clippy --all-targets -- -D warnings\" still fails globally on unrelated files, but filtered stderr scan shows no hits for src/scanner/walker.rs or src/scanner/decision_record.rs. cargo fmt --check currently fails due unrelated staged ordering changes in tests/common/mod.rs (held by another agent).","created_at":"2026-02-15T16:33:22Z"}]}
{"id":"bd-2yw","title":"Walker correctness: honor follow_symlinks and normalize open-file path checks","description":"Fix two walker correctness issues in src/scanner/walker.rs: (1) follow_symlinks config is effectively inert because traversal metadata handling always uses symlink_metadata-style behavior, and (2) open-file detection may miss when candidate paths are relative while /proc fd targets are absolute. Implement behavior-preserving normalization and add/adjust tests.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-15T16:33:51.666438881Z","created_by":"ubuntu","updated_at":"2026-02-15T16:36:58.678973351Z","closed_at":"2026-02-15T16:36:58.678955097Z","close_reason":"Completed walker follow_symlinks + relative open-file path normalization fixes with focused test/check validation.","source_repo":".","compaction_level":0,"original_size":0,"labels":["safety","scanner","walker"],"comments":[{"id":111,"issue_id":"bd-2yw","author":"Dicklesworthstone","text":"Implemented walker correctness fixes in src/scanner/walker.rs: (1) traversal now honors WalkerConfig.follow_symlinks via metadata_for_path helper used for root seeding, child metadata, and emitted directory metadata; (2) is_path_open now normalizes candidate paths (absolute/current_dir+canonicalize fallback) so relative paths can match absolute /proc fd targets. Added tests: follows_symlinks_when_enabled (unix) and is_path_open_handles_relative_candidate_paths. Validation: cargo fmt --check PASS; rch exec \"cargo test --lib walker\" PASS (16 tests); rch exec \"cargo check --all-targets\" PASS; rch exec \"cargo clippy --all-targets -- -D warnings\" still fails globally on unrelated files, and filtered clippy stderr contains no src/scanner/walker.rs hits.","created_at":"2026-02-15T16:36:56Z"}]}
{"id":"bd-2ze","title":"Clippy debt burn-down slice: update + merkle lint blockers","description":"Fix immediate clippy -D warnings blockers in src/cli/bootstrap.rs and src/cli/uninstall.rs (needless_collect and unnecessary_mut_passed style test-lint issues) with behavior preserved and rch validation; created after conflicts on update/merkle reservations.","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-02-15T16:34:23.437276505Z","created_by":"ubuntu","updated_at":"2026-02-15T16:39:16.780311111Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":112,"issue_id":"bd-2ze","author":"Dicklesworthstone","text":"Completed scoped clippy cleanup in reserved files. Changes: src/cli/bootstrap.rs removed needless collect+len, unnecessary mutable-reference passing to apply_init_state_file/apply_fix_permissions, redundant clone in backup footprint, and single-char pattern string; src/cli/uninstall.rs replaced needless collect patterns with iterator any/next checks and tightened KeepData assertions. Validation: cargo fmt --check initially showed only local formatting adjustments; cargo fmt --check on touched files PASS. rch exec 'cargo check --lib' PASS. rch exec 'cargo test --lib bootstrap' and rch exec 'cargo test --lib uninstall' BLOCKED by unrelated compile error in src/cli/assets.rs (borrow of moved entry) under another agent reservation. rch exec 'cargo check --all-targets' likewise blocked by same unrelated assets error. rch exec 'cargo clippy --all-targets -- -D warnings' fails on global backlog; targeted grep shows no remaining clippy mentions for src/cli/bootstrap.rs or src/cli/uninstall.rs.","created_at":"2026-02-15T16:38:06Z"},{"id":114,"issue_id":"bd-2ze","author":"ubuntu","text":"Reopened: Follow-up fix: keep_data_mode test assertion regression introduced during clippy cleanup","created_at":"2026-02-15T16:39:12Z"}]}
{"id":"bd-329","title":"Clippy debt burn-down slice: example binaries lint blockers","description":"Fix clippy -D warnings blockers in examples/pressure_monitor.rs and examples/scan_artifacts.rs (map_or_else usage, clone_on_copy, numeric cast cleanup) with behavior preserved and rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:24:55.372080802Z","created_by":"ubuntu","updated_at":"2026-02-15T16:28:20.265569751Z","closed_at":"2026-02-15T16:28:20.265548401Z","close_reason":"Completed: example binaries clippy blockers resolved and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":105,"issue_id":"bd-329","author":"Dicklesworthstone","text":"Completed non-overlap clippy slice for example binaries.\\n\\nChanges:\\n- examples/pressure_monitor.rs: replaced Option map+unwrap_or_else with map_or_else; removed clone/cast lint path in threshold bytes calc using bounded basis-points integer math.\\n- examples/scan_artifacts.rs: replaced Option map+unwrap_or_else with map_or_else; removed clone_on_copy on structural_signals.\\n\\nValidation:\\n- rch exec \"cargo clippy --examples -- -D warnings\" ✅\\n- rch exec \"cargo check --all-targets\" ✅\\n- cargo fmt --check ✅\\n- rch exec \"cargo clippy --all-targets -- -D warnings\" ❌ (pre-existing lint backlog outside this slice)","created_at":"2026-02-15T16:28:17Z"}]}
{"id":"bd-394","title":"JSONL append-only structured logger","description":"## Deliverable\nAppend-only JSONL (JSON Lines) writer that provides a second, independent log of all sbh activity, optimized for agent consumption and grep-ability.\n\n## Technical Approach\n### JSONL Format\nEach line is a self-contained JSON object:\n```json\n{\"ts\":\"2026-02-14T16:30:00Z\",\"event\":\"artifact_delete\",\"path\":\"/data/projects/foo/.target_opus\",\"size\":3456789012,\"score\":0.87,\"factors\":{\"location\":0.85,\"name\":0.90,\"age\":0.95,\"size\":0.80,\"structure\":0.85},\"pressure\":\"orange\",\"free_pct\":8.3,\"duration_ms\":145,\"ok\":true}\n{\"ts\":\"2026-02-14T16:30:01Z\",\"event\":\"ballast_release\",\"path\":\"/var/lib/sbh/ballast/SBH_BALLAST_FILE_00010.dat\",\"size\":1073741824,\"pressure\":\"red\",\"free_pct\":4.2,\"ok\":true}\n{\"ts\":\"2026-02-14T16:30:02Z\",\"event\":\"pressure_change\",\"level\":\"red->orange\",\"free_pct\":12.1,\"rate_bps\":-50000000}\n```\n\n### Writer Implementation\n```rust\npub struct JsonlWriter {\n    file: BufWriter<File>,\n    path: PathBuf,\n    bytes_written: u64,\n    max_size: u64,           // rotation threshold\n    rotation_count: u32,\n}\n```\n\n### Write Strategy\n- Buffer writes in memory (BufWriter with 64KB buffer)\n- Flush after every logical event (not every line - batch related events)\n- fsync every 10 seconds or on pressure change (not every write - too expensive)\n- Each line is atomic: write the full line then newline in one write() call\n\n### Rotation\nWhen file exceeds max_size_mb:\n- Rename current file to sbh.jsonl.1 (shift existing rotations)\n- Open new sbh.jsonl\n- Keep up to 5 rotated files (configurable)\n\n### Agent-Friendly Design\nAgents can:\n- `tail -f /var/lib/sbh/sbh.jsonl` for live monitoring\n- `grep \"artifact_delete\" sbh.jsonl | jq .` for filtering\n- `jq -s '[.[] | select(.event == \"artifact_delete\")] | length' sbh.jsonl` for counts\n- Parse each line independently (no multi-line records)\n\n### Why Both SQLite AND JSONL?\n1. SQLite: efficient queries, aggregation, statistics, joins\n2. JSONL: portable, grep-able, agent-friendly, survives SQLite corruption\n3. JSONL works even when disk is critically full (can be on a different filesystem)\n4. JSONL is the \"black box recorder\" - if everything else fails, this survives\n\n## Acceptance Criteria\n- Each line is valid JSON\n- Lines are atomic (no partial writes visible to readers)\n- Rotation works correctly without data loss\n- Agent can tail -f and get real-time events\n- All event types have consistent field names\n- Timestamps in ISO 8601 UTC\n- File rotation handles edge cases (rename failures, etc.)\n- Unit tests for writing, rotation, and parsing","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:49:33.020087320Z","created_by":"ubuntu","updated_at":"2026-02-14T19:51:08.884116886Z","closed_at":"2026-02-14T19:51:08.884093723Z","close_reason":"JSONL logger implemented in src/logger/jsonl.rs: LogEntry/EventType/Severity types, JsonlWriter with BufWriter (64KB), atomic write_all lines, file rotation with configurable max_size/rotated_files, 4-level fallback chain (file→fallback→stderr→discard), fsync batching, try_recover, 6 passing tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["logger"],"dependencies":[{"issue_id":"bd-394","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:55:54.999718177Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-394","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T16:55:54.916430116Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":8,"issue_id":"bd-394","author":"Dicklesworthstone","text":"REVIEW: Strengthened fallback chain specification:\n\nThe JSONL writer must implement this explicit degradation ladder:\n\n1. NORMAL: BufWriter → file (64KB buffer, fsync every 10s)\n2. DEGRADED-1: If file write fails (disk full), try reopening file on a DIFFERENT filesystem. Config should allow jsonl_fallback_path (e.g., /dev/shm/sbh.jsonl for RAM-backed fallback)\n3. DEGRADED-2: If fallback path also fails, write to stderr with [SBH-JSONL] prefix so journald/syslog captures events\n4. DEGRADED-3: If stderr write fails (shouldn't happen but defensive), discard events silently — the daemon MUST NOT crash for logging failures\n\nEach degradation level transition must be logged at the NEXT available level.\n\nAlso add: Write JSONL lines using write_all() for a complete buffer (line + newline assembled first), NOT separate write() calls for line and newline. This prevents interleaved partial lines when the file is being tailed by another process.","created_at":"2026-02-14T17:13:46Z"}]}
{"id":"bd-395","title":"Agent/process disk usage attribution (sbh blame)","description":"## Deliverable\nTrack and display which running processes or agents are consuming the most disk space in build artifacts. sbh status --blame and sbh blame commands.\n\n## Usage\n  sbh blame                    # Show disk usage by agent/process\n  sbh blame --json             # Machine-readable output\n\n## Output\n```\nDisk Usage by Agent/Process:\n\n  Agent/Process          │ Build Dirs │ Total Size │ Oldest   │ Newest\n  claude-code (PID 1234) │    23      │   45.2 GB  │ 12h ago  │ 15m ago\n  codex-cli (PID 5678)   │     8      │   12.1 GB  │  4h ago  │ 30m ago\n  pi-agent (PID 9012)    │    14      │   28.7 GB  │  8h ago  │  1h ago\n  (orphaned)             │    31      │   67.4 GB  │  3d ago  │  6h ago\n\n  Total: 76 build dirs, 153.4 GB\n  Orphaned dirs (no running process) are the safest to clean.\n```\n\n## Implementation\n1. Scan /proc/*/cwd to find which processes have CWD in project directories\n2. Correlate discovered artifacts with the project directory of running processes\n3. \"Orphaned\" artifacts belong to projects where no process currently has CWD\n4. Use process binary name (from /proc/*/comm) as agent identifier\n5. Cache /proc scan results (refreshed every 30s or on demand)\n\n## Design Rationale\nHelps users understand which agents are the worst disk offenders. \"Orphaned\" artifacts (no running process) are the safest to clean and can be prioritized. This is a diagnostic tool that makes sbh's decisions more transparent.\n\n## Acceptance Criteria\n- Correctly attributes artifacts to running processes\n- Orphaned artifacts identified and highlighted\n- Works when many agents run simultaneously\n- Performance: <2s even with 100+ processes and 10K+ artifacts\n- JSON output for agent consumption\n- Handles /proc access gracefully (permission denied on some PIDs)","acceptance_criteria":"1. Unit tests cover PID-to-path attribution, orphan detection, and aggregation correctness. 2. Integration tests validate /proc scanning behavior with permission denials and stale process state. 3. E2E scenarios validate blame output across concurrent agent workloads and orphan cleanup opportunities. 4. JSON output schema remains stable for agent consumption. 5. Detailed attribution logs include process identity, matched directories, bytes totals, and unresolved-candidate reasons.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T18:34:05.574878016Z","created_by":"ubuntu","updated_at":"2026-02-14T23:22:55.232016968Z","closed_at":"2026-02-14T23:22:55.231952347Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","scanner"],"dependencies":[{"issue_id":"bd-395","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T18:34:33.619364723Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-395","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T18:34:33.704968788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-395","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T18:34:33.538065900Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-395","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T18:53:48.452379923Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":31,"issue_id":"bd-395","author":"Dicklesworthstone","text":"REVIEW: sbh blame is Linux-specific (/proc/*/cwd, /proc/*/comm). Added bd-sth (PAL) as dependency. Implementation: On Linux, use /proc scanning. On macOS, use lsof -c or similar. On other platforms, return an error 'blame command requires Linux or macOS' with exit code 3. The PAL layer provides platform_supports_process_attribution() -> bool and get_process_cwd(pid) -> Option<PathBuf>.","created_at":"2026-02-14T18:53:52Z"}]}
{"id":"bd-39b","title":"Rewrite AGENTS.md for storage_ballast_helper project","description":"## Deliverable\nReplace the current dcg-focused AGENTS.md with one tailored for the storage_ballast_helper (sbh) project.\n\n## Technical Approach\n### Structure (following dcg's excellent pattern)\n1. Rule 0: Fundamental override prerogative\n2. Rule 1: No file deletion without permission\n3. Git branch conventions\n4. Toolchain section (Rust, Cargo, edition, dependencies)\n5. Code editing discipline\n6. Compiler checks (cargo check, clippy, fmt)\n7. Testing section (unit tests, integration, E2E, stress)\n8. CI/CD pipeline overview\n9. Release process\n10. Architecture overview:\n    - Module diagram\n    - Key files table\n    - Data flow description\n11. Configuration reference\n12. CLI quick reference\n13. Scoring engine documentation\n14. Beads/bv workflow integration (keep existing sections)\n15. Session completion protocol\n\n### Key Differences from dcg's AGENTS.md\n- Architecture section describes sbh's monitoring/scanning/cleanup pipeline\n- No \"hook protocol\" section (sbh is a standalone daemon, not a hook)\n- Add scoring engine documentation\n- Add ballast system documentation\n- Add pressure levels and PID controller overview\n- Add build artifact pattern reference\n\n## Acceptance Criteria\n- Comprehensive and self-contained (agent can work from AGENTS.md alone)\n- All key files documented\n- CLI commands documented\n- Architecture clearly explained\n- Testing instructions complete\n- No references to dcg left","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:55:04.827031477Z","created_by":"ubuntu","updated_at":"2026-02-15T00:18:49.279774512Z","closed_at":"2026-02-15T00:18:49.279705413Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-39b","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T18:59:56.978218667Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":40,"issue_id":"bd-39b","author":"Dicklesworthstone","text":"MINOR: AGENTS.md rewrite depends only on bd-2rq (CLI framework) to know the final command structure. It does NOT need to wait for every bead to be implemented — AGENTS.md documents intended behavior, not implemented behavior. Current dep (bd-2rq) is sufficient.","created_at":"2026-02-14T18:54:47Z"}]}
{"id":"bd-3cj","title":"Statistics and metrics query engine with time-window aggregation","description":"## Deliverable\nQuery engine that provides statistics about sbh's recent behavior across multiple time windows, enabling agents and users to understand what sbh has been doing.\n\n## Technical Approach\n### Time Windows\nReport statistics for: 10 minutes, 30 minutes, 1 hour, 6 hours, 24 hours, 3 days, 7 days\n\n### Statistics Available\n```rust\npub struct WindowStats {\n    pub window: Duration,\n    pub deletions: DeletionStats,\n    pub ballast: BallastStats,\n    pub pressure: PressureStats,\n}\n\npub struct DeletionStats {\n    pub count: u64,                      // number of items deleted\n    pub total_bytes_freed: u64,          // total bytes freed\n    pub avg_size: u64,                   // average size of deleted items\n    pub median_size: u64,\n    pub largest_deletion: Option<PathInfo>,\n    pub most_common_category: String,    // e.g., \"RustTarget\"\n    pub most_common_pattern: String,     // e.g., \"cargo-target-*\"\n    pub avg_score: f64,                  // average candidacy score\n    pub avg_age_hours: f64,              // average age at time of deletion\n    pub failures: u64,                   // failed deletions\n}\n\npub struct BallastStats {\n    pub files_released: u64,\n    pub files_replenished: u64,\n    pub current_inventory: u64,\n    pub bytes_available: u64,\n}\n\npub struct PressureStats {\n    pub time_in_green_pct: f64,\n    pub time_in_yellow_pct: f64,\n    pub time_in_orange_pct: f64,\n    pub time_in_red_pct: f64,\n    pub time_in_critical_pct: f64,\n    pub transitions: u64,                // number of pressure level changes\n    pub worst_level_reached: PressureLevel,\n    pub current_level: PressureLevel,\n    pub current_free_pct: f64,\n}\n```\n\n### Query Implementation\n```rust\npub struct StatsEngine {\n    sqlite: Arc<SqliteLogger>,\n}\n\nimpl StatsEngine {\n    /// Get stats for all standard windows\n    pub fn summary(&self) -> Result<Vec<WindowStats>, SbhError>;\n    \n    /// Get stats for a specific window\n    pub fn window_stats(&self, window: Duration) -> Result<WindowStats, SbhError>;\n    \n    /// Get top-N most deleted patterns\n    pub fn top_patterns(&self, n: usize, window: Duration) -> Result<Vec<PatternStat>, SbhError>;\n    \n    /// Get top-N largest deletions\n    pub fn top_deletions(&self, n: usize, window: Duration) -> Result<Vec<DeletionDetail>, SbhError>;\n    \n    /// Export stats as JSON (for agent consumption)\n    pub fn export_json(&self) -> Result<serde_json::Value, SbhError>;\n}\n```\n\n### SQL Queries\nEfficient aggregation using SQLite's date functions:\n```sql\nSELECT \n    COUNT(*) as count,\n    SUM(size_bytes) as total_bytes,\n    AVG(size_bytes) as avg_size,\n    AVG(score) as avg_score\nFROM activity_log\nWHERE event_type = 'artifact_delete'\n  AND timestamp > datetime('now', '-1 hour')\n  AND success = 1;\n```\n\n### Output Format\nBoth human-readable (colored table for terminal) and JSON (for agents):\n```\nsbh stats\n┌─────────┬────────┬──────────────┬──────────────┬────────────┐\n│ Window  │ Deleted│ Space Freed  │ Avg Size     │ Avg Score  │\n├─────────┼────────┼──────────────┼──────────────┼────────────┤\n│ 10 min  │     3  │       4.2 GB │       1.4 GB │      0.87  │\n│ 30 min  │    12  │      18.7 GB │       1.6 GB │      0.82  │\n│  1 hour │    28  │      42.1 GB │       1.5 GB │      0.79  │\n│  6 hours│    85  │     124.3 GB │       1.5 GB │      0.76  │\n│ 24 hours│   312  │     467.8 GB │       1.5 GB │      0.74  │\n│  3 days │   891  │    1284.2 GB │       1.4 GB │      0.73  │\n│  7 days │  2104  │    3012.7 GB │       1.4 GB │      0.72  │\n└─────────┴────────┴──────────────┴──────────────┴────────────┘\n\nMost deleted patterns: cargo-target-* (42%), .target_* (28%), target/ (18%)\nCurrent pressure: GREEN (23.4% free)\nBallast: 8/10 files available (8 GB reclaimable)\n```\n\n## Acceptance Criteria\n- All time windows produce correct statistics\n- Statistics match actual activity log data (verified by cross-checking)\n- JSON output is well-formed and complete\n- Human-readable output is clear and informative\n- Queries are efficient (< 100ms even with 100K+ log entries)\n- Most common patterns correctly computed\n- Handles empty database gracefully\n- Unit tests with pre-populated test data","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:50:15.096388928Z","created_by":"ubuntu","updated_at":"2026-02-14T19:59:45.809126758Z","closed_at":"2026-02-14T19:59:45.809098235Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["logger"],"dependencies":[{"issue_id":"bd-3cj","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T16:55:55.240799474Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":41,"issue_id":"bd-3cj","author":"Dicklesworthstone","text":"MINOR: Stats engine schema coupling with SQLite logger (bd-2f8): The stats engine queries the SQLite database directly. The schema is defined in bd-2f8 and the stats engine reads it. This is intentional tight coupling — they share the same database. Any schema changes in bd-2f8 must be reflected in bd-3cj queries. Document the schema contract in both beads.","created_at":"2026-02-14T18:54:53Z"}]}
{"id":"bd-3i3","title":"launchd plist generation and service installation (macOS)","description":"## Deliverable\nGenerate and install a launchd plist for the sbh daemon on macOS.\n\n## Technical Approach\n### Generated Plist\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.sbh.daemon</string>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/usr/local/bin/sbh</string>\n        <string>daemon</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n    <key>KeepAlive</key>\n    <dict>\n        <key>SuccessfulExit</key>\n        <false/>\n    </dict>\n    <key>ThrottleInterval</key>\n    <integer>10</integer>\n    <key>Nice</key>\n    <integer>19</integer>\n    <key>LowPriorityIO</key>\n    <true/>\n    <key>StandardOutPath</key>\n    <string>/usr/local/var/log/sbh/sbh.log</string>\n    <key>StandardErrorPath</key>\n    <string>/usr/local/var/log/sbh/sbh.err</string>\n</dict>\n</plist>\n```\n\n### Installation Flow\n```bash\nsbh install --launchd\n```\n1. Generate plist from template\n2. Copy to ~/Library/LaunchAgents/com.sbh.daemon.plist (user) or /Library/LaunchDaemons/ (system)\n3. launchctl load plist_path\n4. Verify service is running\n\n### macOS-specific Considerations\n- No sd_notify equivalent; use different health check\n- macOS paths differ (~/Library/Application Support/sbh/)\n- Need to handle SIP restrictions (System Integrity Protection)\n- macOS doesn't have /dev/shm or tmpfs in the same way\n\n## Acceptance Criteria\n- Generated plist is valid XML\n- Service loads and starts via launchctl\n- Auto-restart on crash (KeepAlive)\n- Low priority scheduling (Nice=19, LowPriorityIO)\n- Works as user agent (no root required)\n- Integration test on macOS: install → start → verify → stop → uninstall","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:51:18.776325044Z","created_by":"ubuntu","updated_at":"2026-02-14T23:12:18.513244001Z","closed_at":"2026-02-14T23:12:18.513213604Z","close_reason":"Implemented launchd plist generation and service installation for macOS. 11 launchd unit tests plus 15 systemd tests (26 total) all passing. CLI handlers wired up for install/uninstall --launchd.","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon","platform"],"dependencies":[{"issue_id":"bd-3i3","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:04:03.366978480Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3i3","depends_on_id":"bd-2j5.3","type":"blocks","created_at":"2026-02-14T21:04:07.693568312Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3i3","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T16:56:03.559688994Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3il","title":"Deep audit hardening: merkle/protection/update/jsonl/self-monitor/assets","description":"Root-cause fixes from deep audit across reserved non-overlap files","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-15T16:38:00.095920032Z","created_by":"ubuntu","updated_at":"2026-02-15T16:38:14.035495393Z","closed_at":"2026-02-15T16:38:14.035474063Z","close_reason":"Completed and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","reliability","scanner","security"],"comments":[{"id":113,"issue_id":"bd-3il","author":"Dicklesworthstone","text":"Implemented and validated fixes: (1) Merkle budget now defers only changed/new paths after exhaustion and rehashes ancestors on remove_paths; (2) protection glob/path matching normalizes Windows separators; (3) self_monitor reconstructs last_scan_at Instant from RFC3339 timestamp; (4) assets cleanup uses same sanitized path components as cache layout; (5) update tempdir uses random per-invocation unique dirs; (6) JSONL writer performs throttled automatic recovery attempts while degraded. Validation: rch exec 'cargo test --lib', rch exec 'cargo check --all-targets', rch exec 'cargo clippy --lib -- -D warnings', rch exec 'cargo test --test integration_tests'.","created_at":"2026-02-15T16:38:10Z"}]}
{"id":"bd-3po","title":"PID-based storage pressure controller","description":"## Deliverable\nA PID (Proportional-Integral-Derivative) controller that determines how aggressively sbh should respond to storage pressure, replacing simplistic threshold-based reactions with smooth, oscillation-free behavior.\n\n## Technical Approach\n### Theory (Alien Graveyard: PID control from control theory)\nInstead of simple \"if free < 5% then panic\", use a feedback controller:\n\n- **P (Proportional)**: Response proportional to how far we are from target free%\n  - Large deviation → aggressive cleanup\n  - Small deviation → gentle cleanup\n  \n- **I (Integral)**: Accumulated error over time\n  - Persistent pressure (free% stayed low for minutes) → escalate response\n  - Prevents the system from being \"stuck\" at a slightly-too-low level\n  - Anti-windup clamp prevents integral term from growing unboundedly\n  \n- **D (Derivative)**: Rate of change (from EWMA estimator)\n  - Rapid disk consumption → preemptive aggressive response\n  - Rapid recovery → dampen response to prevent over-correction\n\n### Implementation\n```rust\npub struct PressureController {\n    // PID gains (configurable)\n    kp: f64,  // proportional gain (default 1.0)\n    ki: f64,  // integral gain (default 0.1)\n    kd: f64,  // derivative gain (default 0.5)\n    \n    // State\n    setpoint: f64,           // target free% (default 15.0)\n    integral: f64,           // accumulated error\n    integral_clamp: f64,     // anti-windup limit\n    previous_error: f64,     // for derivative calculation\n    last_update: Instant,\n    \n    // Hysteresis (Alien Graveyard: prevent oscillation)\n    hysteresis_band: f64,    // ±2% band around setpoint\n    last_action: PressureAction,\n}\n\npub struct PressureReading {\n    pub free_pct: f64,\n    pub rate_estimate: RateEstimate,\n    pub timestamp: Instant,\n}\n\npub enum PressureLevel {\n    Green,     // > setpoint + hysteresis → no action needed\n    Yellow,    // within hysteresis band → maintain current action\n    Orange,    // below setpoint → moderate cleanup\n    Red,       // significantly below setpoint → aggressive cleanup\n    Critical,  // near 0% → emergency: delete ballast + all safe artifacts\n}\n\npub struct PressureResponse {\n    pub level: PressureLevel,\n    pub urgency: f64,              // 0.0-1.0 (PID output, clamped)\n    pub recommended_actions: Vec<PressureAction>,\n    pub scan_interval: Duration,   // adaptive scan interval\n    pub max_delete_batch: usize,   // how many items to delete per cycle\n}\n\npub enum PressureAction {\n    NoAction,\n    IncreaseScanFrequency,\n    ReleaseBallast(usize),  // release N ballast files\n    DeleteArtifacts { min_score: f64 },\n    EmergencyCleanup,\n}\n```\n\n### Hysteresis (Alien Graveyard: prevent oscillation)\nWithout hysteresis, the controller would oscillate:\n1. Free drops to 14% → start cleaning\n2. Free rises to 16% → stop cleaning\n3. Free drops to 14% → start cleaning again\n4. (repeat forever)\n\nWith a 2% hysteresis band around the 15% setpoint:\n- Start cleaning when free < 13%\n- Stop cleaning when free > 17%\n- This creates stable behavior\n\n### Adaptive Scan Intervals\nThe PID output also controls scan frequency:\n- Green: scan every 60s\n- Yellow: scan every 30s\n- Orange: scan every 10s\n- Red: scan every 5s\n- Critical: scan every 1s\n\n## Design Rationale\nSimple threshold systems lead to oscillation, delayed response, or over-reaction. A PID controller with hysteresis provides:\n1. Smooth, proportional responses (not all-or-nothing)\n2. Faster reaction to rapid changes (derivative term)\n3. Persistent pressure handled by integral buildup\n4. No oscillation thanks to hysteresis band\n5. Tunable behavior via gains (users can adjust aggressiveness)\n\n## Acceptance Criteria\n- PID output is smooth and bounded (no oscillation in test scenarios)\n- Hysteresis prevents chattering between states\n- Anti-windup prevents integral runaway\n- Response proportional to pressure severity\n- Scan intervals adapt correctly to pressure level\n- Unit tests with simulated pressure scenarios\n- Test: sustained 10% pressure → integral buildup → escalated response\n- Test: sudden spike → derivative kicks in → fast response\n- Test: recovery → smooth return to normal scanning","acceptance_criteria":"1. Unit tests validate PID term calculations, anti-windup, clamps, and hysteresis transitions. 2. Closed-loop integration tests verify stable behavior with noisy EWMA pressure input. 3. E2E pressure scenarios validate action selection under burst, sustained, and recovery phases. 4. Controller output is monotonic with urgency and respects hard safety caps. 5. Detailed control-loop logs include P/I/D components, setpoint error, selected action, and guard/fallback reason codes.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:45:54.009722683Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:22.203475921Z","closed_at":"2026-02-14T19:45:22.203448800Z","close_reason":"PID pressure controller implemented in monitor/pid.rs (359 lines): PidPressureController with PressureLevel enum (Green-Critical), hysteresis, anti-windup integral, predictive boosting (time-to-red signals), response_policy with adaptive scan intervals, unit tests for escalation/hysteresis/prediction.","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","monitoring"],"dependencies":[{"issue_id":"bd-3po","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T19:02:40.641526689Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po","depends_on_id":"bd-x1k","type":"blocks","created_at":"2026-02-14T16:55:39.500558971Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":50,"issue_id":"bd-3po","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-2pj dependency, added bd-3uk dependency. PID controller receives PressureReading from the caller (main loop). It does not import or call FsStatsCollector. Data flow: main loop calls FsStatsCollector -> creates PressureReading -> passes to PressureController.update(). PID controller is pure computation on top of the reading. Needs bd-3uk for config validation errors (invalid PID gains).","created_at":"2026-02-14T19:03:06Z"}]}
{"id":"bd-3qm","title":"Project protection system with marker files and CLI","description":"## Deliverable\nUser-controllable project protection that prevents sbh from ever deleting artifacts in protected directories.\n\n## The Problem\nThe scoring engine has hard vetoes (.git, system paths, age), but there is NO way for users to say \"never touch THIS specific project.\" Users WILL have projects with 6-hour builds they cannot afford to lose. Without explicit protection, they will never trust sbh on production machines.\n\n## The Solution\n\n### Marker Files\nDrop a .sbh-protect file in any directory to protect it and all children from sbh cleanup:\n  touch /data/projects/critical-app/.sbh-protect\n\nThe walker checks for .sbh-protect during traversal. If found, the entire directory subtree is skipped — not scored, not walked, not considered.\n\n### CLI Commands\n  sbh protect <path>        # Creates .sbh-protect marker file\n  sbh protect --list        # Shows all protected paths (marker files + config)\n  sbh unprotect <path>      # Removes .sbh-protect marker file\n  sbh scan --show-protected # Includes protected paths in output with [PROTECTED] label\n\n### Config-Level Protection\nFor system-wide or pattern-based protection:\n  [scanner.protected_paths]\n  paths = [\"/data/projects/production-*\", \"/home/*/critical-builds\"]\n\nConfig protection works via glob patterns and does NOT require marker files.\n\n### Implementation\n1. ProtectionRegistry: loaded at startup from config + discovered marker files\n2. Walker integration: during traversal, check each directory for .sbh-protect before descending\n3. Scoring integration: protected paths get hard veto (score = 0.0, veto_reason = \"Protected by .sbh-protect\")\n4. CLI commands: trivial file create/delete operations\n5. Protection persists across restarts (marker files are on disk, config is in TOML)\n\n### Data Structure\n```rust\npub struct ProtectionRegistry {\n    marker_paths: HashSet<PathBuf>,      // discovered .sbh-protect locations\n    config_patterns: Vec<GlobPattern>,   // from config file\n}\n\nimpl ProtectionRegistry {\n    pub fn is_protected(&self, path: &Path) -> bool;\n    pub fn protection_reason(&self, path: &Path) -> Option<String>;\n    pub fn discover_markers(&mut self, root: &Path);\n}\n```\n\n### .sbh-protect File Format\nThe file can be empty (presence is sufficient) OR contain optional JSON metadata:\n```json\n{\n  \"reason\": \"Production build - 6 hour compile time\",\n  \"protected_by\": \"jeff\",\n  \"protected_at\": \"2026-02-14T10:00:00Z\"\n}\n```\nThe metadata is displayed in sbh scan --show-protected and sbh protect --list.\n\n## Design Rationale\nThis is the #1 trust-building feature. Without it, sophisticated users won't deploy sbh on production machines. With it, they feel in control. Implementation cost is near-zero: one additional check in the walker + two CLI commands. The marker file approach is elegant because: (1) it's discoverable (ls shows it), (2) it's portable (survives config changes), (3) it's git-committable (team-wide protection), (4) it requires zero sbh knowledge to create (just touch a file).\n\n## Acceptance Criteria\n- .sbh-protect marker file prevents all cleanup in that directory and children\n- sbh protect <path> creates marker file correctly\n- sbh unprotect <path> removes marker file\n- sbh protect --list shows all protections (marker + config) with metadata\n- Config-level glob patterns work correctly\n- Protected paths appear in sbh scan with [PROTECTED] label\n- Walker skips protected directories entirely (no unnecessary traversal)\n- Protection survives daemon restart\n- Empty .sbh-protect file works (metadata optional)\n- Unit tests: protection check logic, glob patterns, nested protection\n- Integration test: protect dir -> scan -> verify not scored -> unprotect -> scan -> verify scored","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:31:09.551558875Z","created_by":"ubuntu","updated_at":"2026-02-14T21:33:45.424324194Z","closed_at":"2026-02-14T21:33:45.424302945Z","close_reason":"Implemented ProtectionRegistry with dual-mode (full/marker-only), marker file discovery, config glob patterns, 26 unit tests all passing","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","scanner"],"dependencies":[{"issue_id":"bd-3qm","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T18:34:15.856890261Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qm","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T18:34:15.940345692Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qm","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T18:54:27.172976749Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":23,"issue_id":"bd-3qm","author":"Dicklesworthstone","text":"IMPORTANT: The protection system must work in TWO modes:\n1. FULL MODE (with config): reads scanner.protected_paths from config AND discovers .sbh-protect marker files\n2. MARKER-ONLY MODE (without config): only discovers .sbh-protect marker files during walker traversal\n\nThis is critical because the emergency mode (bd-2qa) uses the walker, and the walker depends on the protection system. Emergency mode operates WITHOUT any config file. The protection system must gracefully handle the 'no config available' case by falling back to marker-only mode.\n\nImplementation: ProtectionRegistry::new() takes Optional<Config>. When None, skip config-level glob patterns and only use marker file discovery.","created_at":"2026-02-14T18:36:31Z"},{"id":46,"issue_id":"bd-3qm","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-2rq dependency. The ProtectionRegistry is a LIBRARY component (marker file discovery, glob matching, is_protected() API). It does NOT import CLI framework code. CLI commands (sbh protect/unprotect) are wired into bd-2rq when that bead is implemented. bd-3qm deps: bd-3qw (config for glob patterns), bd-3uk (error types), bd-1kn (scaffolding).","created_at":"2026-02-14T19:02:48Z"}]}
{"id":"bd-3qw","title":"Configuration system with TOML + env vars + smart defaults","description":"## Deliverable\nComprehensive configuration system supporting layered config: defaults → system config → user config → env vars → CLI flags.\n\n## Technical Approach\n### Config File Location (XDG-compliant)\n- Linux: ~/.config/sbh/config.toml, /etc/sbh/config.toml\n- macOS: ~/Library/Application Support/sbh/config.toml\n- Windows: %APPDATA%\\sbh\\config.toml\n\n### Config Structure\n```toml\n[general]\nlog_level = \"info\"                  # trace/debug/info/warn/error\nscan_interval_seconds = 30          # base scan interval\ncritical_scan_interval_seconds = 5  # scan interval when pressure is high\ndry_run = false                     # log what would be deleted without deleting\n\n[ballast]\nenabled = true\nlocation = \"/var/lib/sbh/ballast\"   # or auto-detect\nfile_count = 10                     # number of 1GB ballast files\nfile_size_mb = 1024                 # size of each ballast file\nmin_free_after_release_pct = 15.0   # target free% after releasing ballast\n\n[pressure]\ngreen_threshold_pct = 20.0          # > 20% free = green\nyellow_threshold_pct = 15.0         # 10-20% = yellow (warning)\norange_threshold_pct = 10.0         # 5-10% = orange (alert)\nred_threshold_pct = 5.0             # < 5% = red (critical)\ncritical_threshold_pct = 2.0        # < 2% = emergency\n\n# PID controller tuning\npid_kp = 1.0                        # proportional gain\npid_ki = 0.1                        # integral gain (accumulated pressure)\npid_kd = 0.5                        # derivative gain (rate of change)\n\n[scanner]\nmax_depth = 10                      # max directory recursion depth\nmin_file_age_minutes = 30           # never delete files newer than this\nsweet_spot_min_hours = 2.0          # ideal deletion age range start\nsweet_spot_max_hours = 10.0         # ideal deletion age range end\nmin_size_mb = 10                    # skip files smaller than this\n\n[scanner.watched_paths]\n# paths to actively scan for build artifacts\npaths = [\"/data/projects\", \"/tmp\", \"/data/tmp\", \"/home\"]\n\n[scanner.excluded_paths]\n# paths to NEVER touch\npaths = [\"/\", \"/boot\", \"/etc\", \"/usr\", \"/bin\", \"/sbin\", \"/var/log\"]\n\n[scanner.special_locations]\n# RAM-backed locations requiring hawk-like monitoring\npaths = [\"/tmp\", \"/dev/shm\", \"/run/shm\"]\nbuffer_pct = 15.0                   # maintain this % free minimum\n\n[logger]\nsqlite_path = \"/var/lib/sbh/sbh.db\"\njsonl_path = \"/var/lib/sbh/sbh.jsonl\"\nmax_jsonl_size_mb = 100             # rotate JSONL at this size\nretention_days = 30                 # keep logs for this many days\n```\n\n### Environment Variable Override Pattern\nEvery config key maps to an env var: `SBH_BALLAST_FILE_COUNT=20` overrides `ballast.file_count`.\n\n### Validation\nAll config values validated on load with clear error messages. Invalid values → SBH-2xxx errors with suggestions.\n\n## Design Rationale\nLayered configuration with env var overrides is essential for containerized/systemd environments where you want to customize behavior without editing files. Smart defaults mean sbh works out of the box with zero configuration on typical Linux systems. The PID controller tuning parameters allow advanced users to tune pressure response behavior.\n\n## Acceptance Criteria\n- Default config works out of the box (no config file needed)\n- Config loads from correct platform-specific locations\n- Env vars override file config\n- All values validated with helpful error messages on invalid input\n- Config serializes to/from TOML round-trip perfectly\n- Unit tests for default values, overrides, validation","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:44:16.034562214Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:09.678739039Z","closed_at":"2026-02-14T19:45:09.678714423Z","close_reason":"Configuration system implemented in core/config.rs (481 lines): full TOML parsing, env var overrides (SBH_*), PressureConfig with thresholds, ScannerConfig, ScoringConfig with weights, BallastConfig, TelemetryConfig, PathsConfig, comprehensive validation, unit tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"],"dependencies":[{"issue_id":"bd-3qw","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T16:55:33.603504416Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qw","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T16:55:33.679832826Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":12,"issue_id":"bd-3qw","author":"Dicklesworthstone","text":"REVIEW: Two additions needed:\n\n1. PID CONTROLLER CONFIG: The config bead's TOML example includes pid_kp, pid_ki, pid_kd, hysteresis_band parameters, but the bead description doesn't mention exposing these through the config structure. The Config struct must include a PidConfig section:\n  [pressure.pid]\n  kp = 1.0\n  ki = 0.1\n  kd = 0.5\n  hysteresis_band_pct = 2.0\n  setpoint_free_pct = 15.0\n  integral_clamp = 10.0\n\n2. SPECIAL LOCATIONS CONFIG: The config should include a [scanner.special_locations] section for configuring special location monitoring parameters beyond just the paths:\n  [scanner.special_locations]\n  paths = [\"/tmp\", \"/dev/shm\", \"/run/shm\"]\n  buffer_pct = 15.0\n  scan_interval_seconds = 5\n\nThis ensures the special location registry (bd-1td) can read its configuration from the same config system instead of having hardcoded defaults.","created_at":"2026-02-14T17:14:36Z"},{"id":21,"issue_id":"bd-3qw","author":"Dicklesworthstone","text":"ENHANCEMENT (idea-wizard): Add per-category retention policies in config. Different artifact types should have different age thresholds:\n  [scanner.retention]\n  rust_target.min_age_minutes = 60      # Rust builds are expensive\n  node_modules.min_age_minutes = 30     # npm install is fast\n  python_cache.min_age_minutes = 15     # __pycache__ is trivially regenerated\n  cache_dirs.min_age_minutes = 120      # ~/.cache might have important state\n  agent_workspace.min_age_minutes = 45  # Agent workspaces need moderate protection\n\nThe scoring engine (bd-x9z) would check the category-specific min_age rather than the global min_file_age_minutes. This makes sbh smarter about what to clean first: cheap-to-rebuild artifacts get cleaned before expensive-to-rebuild ones.","created_at":"2026-02-14T18:35:03Z"}]}
{"id":"bd-3s5","title":"Live TUI dashboard with real-time pressure visualization","description":"## Deliverable\nsbh dashboard — a live terminal UI showing real-time pressure gauges, EWMA trends, scan activity, and deletion history. Uses frankentui reference patterns from /dp/frankentui.\n\n## Technical Approach\n### CLI\n  sbh dashboard              # Launch interactive TUI dashboard\n  sbh dashboard --refresh 2  # Custom refresh interval (default 1s)\n\n### Layout\n```\n┌─ Storage Ballast Helper v0.1.0 ──────────────────────────── uptime: 3d 14h ─┐\n│                                                                               │\n│  Pressure Gauges                                                              │\n│  /data    [████████████░░░░░░░░] 62% used (684 GB free)  GREEN               │\n│  /tmp     [██████████████████░░] 90% used (3.2 GB free)  ORANGE  ⚠ 18m left  │\n│  /dev/shm [████░░░░░░░░░░░░░░░░] 23% used (198 GB free) GREEN               │\n│                                                                               │\n│  EWMA Trends (last 30 min)                                                    │\n│  /data  ▁▁▂▃▄▅▆▅▄▃▂▁▁▁▁  -12 MB/s (recovering)                             │\n│  /tmp   ▁▂▃▅▇█████████   +245 MB/s (accelerating) ⚠                         │\n│                                                                               │\n│  Recent Activity                                          Ballast             │\n│  16:30:01 DEL cargo-target-mvcc (4.2 GB, 0.94)          /data: 8/10 avail   │\n│  16:29:55 DEL .target_opus_main (8.1 GB, 0.91)          /tmp:  3/3 avail    │\n│  16:29:40 SCAN /data/projects (47 candidates)            /: 5/5 avail        │\n│  16:29:00 ALERT /tmp approaching threshold                                    │\n│                                                                               │\n│  PID: P=0.3 I=0.1 D=0.8 | Scans: 1542 | Deleted: 312 (467 GB) | Errs: 2   │\n└───────────────────────────────────────────────────────────────────────────────┘\n```\n\n### Implementation\n- Refresh via polling state.json + live fs stats\n- Sparkline charts for EWMA trends (unicode block characters)\n- Color-coded pressure levels (green/yellow/orange/red with ANSI colors)\n- Scrollable activity log\n- PID controller state at bottom\n- Uses crossterm for terminal manipulation (already used by frankentui)\n- Graceful exit on q/Ctrl-C\n\n### Reference\nFollow patterns from /dp/frankentui for rendering approach, color schemes, layout primitives.\n\n## Acceptance Criteria\n- Live updating display (1s default refresh)\n- All monitored volumes shown with pressure gauges\n- EWMA trend sparklines for each volume\n- Recent activity log with timestamps\n- Ballast inventory per volume\n- PID controller state summary\n- Graceful exit, restores terminal state\n- Works when daemon is not running (degraded mode showing static fs stats)\n- Handles terminal resize","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T18:33:16.217664974Z","created_by":"ubuntu","updated_at":"2026-02-15T00:33:25.250169770Z","closed_at":"2026-02-15T00:33:25.250006787Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-3s5","depends_on_id":"bd-2np","type":"blocks","created_at":"2026-02-14T19:02:37.881609914Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3s5","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T18:34:32.946021429Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3s5","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T18:34:32.784013165Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3s5","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T18:34:32.865473451Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3sb","title":"Stress test and load simulator for extreme pressure scenarios","description":"## Deliverable\nStress and load simulator for extreme pressure scenarios, including adaptive-policy guard/fallback paths.\n\n## Scenario Matrix\nA. Rapid fill burst under compile-like load\nB. Sustained low-free-space pressure\nC. Flash fill in RAM-backed locations\nD. Recovery under ongoing write pressure\nE. High CPU + high I/O overload condition\nF. Decision-plane drift event triggering fallback\nG. Index integrity failure forcing full-scan mode\n\n## Metrics Collection\n- time to detection and first action\n- reclaimed vs consumed bytes\n- controller mode transitions and guard states\n- fallback trigger counts and recovery windows\n- sbh CPU/memory/IO overhead\n\n## Detailed Logging Contract\n- per-phase trace identifiers\n- action selection rationale\n- fallback reason codes and guard metrics\n- before/after pressure snapshots\n\n## Acceptance Criteria\n- all scenarios pass with deterministic checks\n- fallback behavior triggered and validated where expected\n- no crashes, deadlocks, or unbounded resource growth\n- machine-readable stress report emitted\n- stress harness integrated with CI/nightly path","acceptance_criteria":"1. Stress scenarios validate behavior under bursty load, sustained pressure, and recovery. 2. Resource ceilings and stability invariants are enforced (no deadlock/leak/unbounded growth). 3. Adaptive-policy guard/fallback transitions are triggered and verified under induced drift/faults. 4. Stress logs include per-phase metrics, action timelines, and failure snapshots for replay. 5. Reports are emitted in machine-readable form and archived by CI/nightly workflows.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:54:41.876853351Z","created_by":"ubuntu","updated_at":"2026-02-15T01:57:23.279207139Z","closed_at":"2026-02-15T01:57:23.279106831Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["testing"],"dependencies":[{"issue_id":"bd-3sb","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-14T16:56:23.030160027Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-2np","type":"blocks","created_at":"2026-02-14T19:02:47.225791425Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-2q9","type":"blocks","created_at":"2026-02-14T19:02:46.960117763Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:59:01.017697145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T19:02:47.135165934Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-izu.1","type":"blocks","created_at":"2026-02-14T18:59:01.106821052Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-izu.3","type":"blocks","created_at":"2026-02-14T18:59:01.192645082Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-izu.4","type":"blocks","created_at":"2026-02-14T18:59:01.281970436Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-izu.5","type":"blocks","created_at":"2026-02-14T18:59:01.372609911Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sb","depends_on_id":"bd-x1k","type":"blocks","created_at":"2026-02-14T19:02:47.046963629Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":22,"issue_id":"bd-3sb","author":"Dicklesworthstone","text":"ENHANCEMENT (idea-wizard): Add realistic agent swarm simulation as Scenario F:\n\n**Scenario F: Multi-Agent Swarm Simulation**\n- Spawn N simulated agents (default 10, configurable) as separate threads\n- Each agent: creates a uniquely-named target dir, writes 500MB-2GB of fake build data over 30-60 seconds, pauses 10-30s, then starts another build\n- Agent naming follows real patterns: .target_opus_main, cargo-target-quietwillow, pi_agent_cyanrobin\n- Some agents finish and stop, others run continuously\n- Verify sbh correctly handles: overlapping builds, rapid creation, varying sizes, some targets with .git (should be vetoed), some with active processes (should be vetoed via is_open)\n- Metrics: total disk consumed by agents over 10 min, total freed by sbh, peak disk usage, number of false positives (vetoed items that were correctly protected)\n- This is the single most realistic test — it directly simulates the production scenario sbh exists for.","created_at":"2026-02-14T18:35:03Z"}]}
{"id":"bd-3t9","title":"Clippy debt burn-down slice: daemon signals + pid test lint blockers","description":"Fix immediate clippy -D warnings blockers in src/daemon/signals.rs and src/monitor/pid.rs (unchecked_time_subtraction and float_cmp in tests) while preserving behavior; validate via rch.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:30:57.245084878Z","created_by":"ubuntu","updated_at":"2026-02-15T16:33:55.224079431Z","closed_at":"2026-02-15T16:33:55.224057039Z","close_reason":"Completed targeted signals/pid clippy blocker slice with passing rch tests/check; remaining clippy failures are unrelated backlog.","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":109,"issue_id":"bd-3t9","author":"Dicklesworthstone","text":"Validated and completed targeted lint blockers for daemon signals/pid tests in current tree. Relevant lines now use checked Instant subtraction in src/daemon/signals.rs watchdog test and epsilon-based zero comparison in src/monitor/pid.rs free_pct test. Validation: cargo fmt --check failed globally due unrelated files; targeted cargo fmt --check on src/daemon/signals.rs and src/monitor/pid.rs PASS. rch exec 'cargo test --lib signals' PASS. rch exec 'cargo test --lib pid' PASS. rch exec 'cargo check --all-targets' PASS. rch exec 'cargo clippy --all-targets -- -D warnings' FAIL with unrelated backlog, with no reported clippy entries for src/daemon/signals.rs or src/monitor/pid.rs in output and targeted grep check.","created_at":"2026-02-15T16:33:52Z"}]}
{"id":"bd-3uk","title":"Core error types with SBH-prefixed error codes","description":"## Deliverable\nDefine the comprehensive error type system for sbh with machine-parseable error codes.\n\n## Technical Approach\nFollowing dcg's pattern of prefixed error codes (DCG-XXXX), create SBH-XXXX codes:\n\n### Error Categories\n- SBH-1xxx: Storage monitoring errors (statvfs failures, mount detection errors)\n- SBH-2xxx: Configuration errors (parse failures, invalid values, missing config)\n- SBH-3xxx: Runtime/daemon errors (signal handling, service install failures)\n- SBH-4xxx: Scanner errors (permission denied, broken symlinks, walk failures)\n- SBH-5xxx: Logger errors (SQLite failures, JSONL write errors, disk full during logging)\n- SBH-6xxx: Ballast errors (creation failures, integrity check failures)\n\n### Error Traits\n- All errors implement std::error::Error via thiserror\n- All errors carry structured context (path, filesystem, operation)\n- Errors are serializable to JSON for machine consumption\n- Recoverability classification: Transient (retry), Permanent (fail), Unknown\n\n### Key Types\n```rust\npub enum SbhError {\n    Storage(StorageError),\n    Config(ConfigError),\n    Runtime(RuntimeError),\n    Scanner(ScannerError),\n    Logger(LoggerError),\n    Ballast(BallastError),\n}\n\npub enum Recoverability {\n    Transient,  // Safe to retry\n    Permanent,  // Unrecoverable\n    Unknown,    // Context-dependent\n}\n```\n\n## Design Rationale\nInspired by asupersync's three-tier error system and dcg's DCG-XXXX codes. Machine-parseable error codes let agents programmatically handle failures. Recoverability classification enables the daemon to auto-retry transient failures (disk temporarily busy) while surfacing permanent failures (permissions wrong) immediately.\n\n## Acceptance Criteria\n- All error variants have unique SBH-XXXX codes\n- All errors implement Display with human-readable messages\n- All errors serialize to JSON with code, category, message, context fields\n- Unit tests verify error code uniqueness and serialization round-trip","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:43:52.126552163Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:09.504253089Z","closed_at":"2026-02-14T19:45:09.504233472Z","close_reason":"Core error types implemented in core/errors.rs (138 lines): SBH-prefixed error codes (1001-3900), thiserror derives, Recoverability classification (is_retryable), Display/From impls for rusqlite/serde_json/toml errors.","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"],"dependencies":[{"issue_id":"bd-3uk","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T16:55:33.528952612Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-48o","title":"Main monitoring loop with tiered scan frequencies","description":"## Deliverable\nThe central daemon monitoring loop that orchestrates all sbh components: pressure monitoring, ballast management, artifact scanning, and cleanup.\n\n## Technical Approach\n### Architecture\nThe daemon runs as a single process with multiple threads:\n1. **Monitor thread**: Polls filesystem stats, updates EWMA, runs PID controller\n2. **Scanner thread**: Walks directories, scores candidates (triggered by monitor)\n3. **Executor thread**: Deletes candidates from the ranked queue\n4. **Logger thread**: Writes to SQLite and JSONL (receives events via channel)\n\nCommunication via crossbeam channels (no tokio, following asupersync's no-tokio philosophy):\n```\nMonitor → (PressureEvent) → Scanner\nScanner → (CandidateList) → Executor\nAll    → (ActivityEvent)  → Logger\n```\n\n### Tiered Scan Frequencies\nDifferent locations get different scan intervals based on criticality:\n\n**Tier 1: Special locations (5-second cycle)**\n- /tmp (tmpfs)\n- /dev/shm\n- Any RAM-backed mount\n- Check free% and alert if below buffer threshold\n\n**Tier 2: Primary project locations (30-second cycle)**\n- /data/projects/*\n- /home/*/projects/*\n- Full artifact scanning with scoring\n\n**Tier 3: Cache and misc locations (120-second cycle)**\n- ~/.cache/*\n- /data/tmp/*\n- Less aggressive scanning\n\n### Adaptive Frequency\nThe PID controller output adjusts these base intervals:\n- Green pressure: use base intervals\n- Yellow: halve intervals\n- Orange: quarter intervals\n- Red: minimum intervals (1s for special, 5s for primary, 15s for cache)\n- Critical: maximum frequency on all tiers\n\n### Main Loop Pseudocode\n```rust\nloop {\n    // 1. Collect filesystem stats (all monitored paths)\n    let stats = fs_collector.collect_all();\n    \n    // 2. Update EWMA rate estimator\n    rate_estimator.update(stats);\n    \n    // 3. Run PID pressure controller\n    let response = pressure_controller.update(stats, rate_estimator.estimate());\n    \n    // 4. Handle pressure response\n    match response.level {\n        Green => { /* normal operation, maybe replenish ballast */ },\n        Yellow => { /* increase scan frequency */ },\n        Orange => { /* start scanning + gentle cleanup */ },\n        Red => { /* release ballast + aggressive scan + delete */ },\n        Critical => { /* emergency: release all ballast + delete everything safe */ },\n    }\n    \n    // 5. Check special locations independently\n    for loc in special_locations.iter() {\n        if loc.needs_attention(stats) {\n            handle_special_location_pressure(loc, stats);\n        }\n    }\n    \n    // 6. Sleep for adaptive interval\n    thread::sleep(response.scan_interval);\n}\n```\n\n### Graceful Degradation Under Load\nWhen the system is already overloaded (high load average, heavy I/O wait):\n- Reduce parallelism in directory walker\n- Use ionice to lower I/O priority\n- Increase sleep between operations\n- Prefer ballast release over scanning (instant vs. slow)\n\nThe daemon must NEVER make an already-bad situation worse.\n\n### Startup Sequence\n1. Load/validate configuration\n2. Initialize platform abstraction\n3. Open/create SQLite database\n4. Open JSONL log file\n5. Discover special locations\n6. Verify/provision ballast files\n7. Initial pressure check (are we already in trouble?)\n8. Enter main loop\n\n### Shutdown Sequence (on SIGTERM/SIGINT)\n1. Stop accepting new scan requests\n2. Complete in-progress deletions (bounded timeout: 30s)\n3. Flush logger buffers\n4. Close SQLite connection\n5. Log shutdown event\n6. Exit 0\n\n## Design Rationale\nThe tiered scanning approach is essential because not all locations are equally critical. RAM-backed locations (tmpfs) need near-real-time monitoring because they can fill up in seconds during a cargo build. Disk-backed locations can be checked less frequently. The adaptive frequency ensures sbh responds quickly to pressure without wasting resources during calm periods.\n\nUsing threads with crossbeam channels instead of async/tokio follows the asupersync project's no-tokio philosophy and keeps the daemon simple, predictable, and debuggable. The daemon is fundamentally a polling-based system, not a network server, so async isn't needed.\n\n## Acceptance Criteria\n- Daemon starts up correctly and enters monitoring loop\n- Tiered scanning respects configured intervals\n- Adaptive frequency responds to pressure changes\n- Special locations monitored independently and more frequently\n- All events logged to both SQLite and JSONL\n- Graceful shutdown on SIGTERM/SIGINT\n- Startup detects and handles \"already in trouble\" scenarios\n- Thread communication via channels is reliable\n- Integration test: simulate full lifecycle (start → pressure → cleanup → stop)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:50:47.398076489Z","created_by":"ubuntu","updated_at":"2026-02-14T20:19:18.616968954Z","closed_at":"2026-02-14T20:19:18.616881681Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["daemon"],"dependencies":[{"issue_id":"bd-48o","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T16:56:03.228380163Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-48o","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T16:56:03.142386967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-48o","depends_on_id":"bd-1td","type":"blocks","created_at":"2026-02-14T16:56:03.393921357Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-48o","depends_on_id":"bd-224","type":"blocks","created_at":"2026-02-14T16:56:03.053498070Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-48o","depends_on_id":"bd-2s9","type":"blocks","created_at":"2026-02-14T16:56:03.311356440Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-48o","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T16:56:02.969269368Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":5,"issue_id":"bd-48o","author":"Dicklesworthstone","text":"REVIEW: Three critical design details added:\n\n1. BOUNDED CHANNELS & BACKPRESSURE: All crossbeam channels must be bounded to prevent unbounded memory growth under load:\n  - Monitor → Scanner channel: bounded(16) — if scanner falls behind, monitor drops old pressure events (latest-wins)\n  - Scanner → Executor channel: bounded(64) — if executor is slow, scanner blocks (natural backpressure). This prevents scoring thousands of candidates that will never be executed.\n  - All → Logger channel: bounded(1024) — generous buffer since logging is fast. If buffer fills, events are dropped with a stderr warning (never block the monitoring loop for logging).\n\n2. THREAD PANIC RECOVERY: If any worker thread panics, the main monitoring thread must detect it and restart it:\n  - Store JoinHandle for each thread\n  - Check thread health every 10 seconds (is_finished())\n  - If a thread has exited unexpectedly, log the error and respawn it\n  - If the SAME thread panics 3 times in 5 minutes, stop respawning and escalate to error reporting\n  - The monitoring thread itself is the \"last line of defense\" — if it panics, systemd's WatchdogSec will restart the process\n\n3. EXECUTOR SLOWER THAN SCANNER: When the executor is slower than the scanner (e.g., deleting large directory trees while new candidates arrive), the bounded channel (64 slots) provides natural backpressure. The scanner blocks on send(), which slows down scanning. This is CORRECT behavior — there's no point discovering more candidates if we can't delete them fast enough. Under extreme pressure, the executor should prioritize the highest-scoring candidates first (the channel should be a priority queue or the executor should sort its batch before processing).","created_at":"2026-02-14T17:13:07Z"},{"id":26,"issue_id":"bd-48o","author":"Dicklesworthstone","text":"REVIEW: After removing bd-112 and bd-2up as hard dependencies, the main loop should accept these as optional integrations wired in at startup: (1) Option<NotificationSender> — if notification system (bd-112) is compiled in and configured, send pressure transition events. If None, notifications are simply skipped. (2) Option<PredictiveActionPolicy> — if predictive pipeline (bd-2up) is available, evaluate EWMA predictions each cycle and pre-empt if confidence is high. If None, rely solely on reactive PID response. This means bd-48o can ship and work correctly WITHOUT either feature. They are layered on top, not load-bearing. The startup sequence checks config to decide whether to construct these Option<T> values.","created_at":"2026-02-14T18:53:12Z"},{"id":55,"issue_id":"bd-48o","author":"Dicklesworthstone","text":"REVIEW-2: DaemonArgs specification: The 'sbh daemon' command starts the monitoring loop. DaemonArgs struct: --foreground (default, systemd manages backgrounding), --pidfile <path> (optional, for non-systemd setups). The daemon subcommand handler calls the startup sequence defined in this bead. Scanner thread data flow: (1) Walker produces Vec<WalkEntry>. (2) Scanner thread constructs ScoringInput for each (adding pressure_urgency, is_open, now). (3) ScoringEngine.score(input) produces CandidacyScore. (4) Candidates above threshold collected into ranked Vec sorted by score desc. (5) Ranked batch sent to Executor thread via bounded channel. All scoring happens inline in the scanner thread — scoring is cheap pure computation.","created_at":"2026-02-14T19:03:28Z"}]}
{"id":"bd-4hh","title":"Clippy debt burn-down slice: dual logger and ewma test lint blockers","description":"Fix immediate clippy -D warnings blockers in src/logger/dual.rs and src/monitor/ewma.rs (needless collect in tests, unnecessary casts in Duration::from_secs loops), preserving behavior with focused rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:27:42.345825352Z","created_by":"ubuntu","updated_at":"2026-02-15T16:30:26.462039818Z","closed_at":"2026-02-15T16:30:26.462021985Z","close_reason":"Completed targeted dual/ewma clippy lint blockers with passing rch tests/check; remaining clippy failures are unrelated backlog outside this slice.","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":106,"issue_id":"bd-4hh","author":"Dicklesworthstone","text":"Completed narrow clippy slice in reserved files. Changes: src/logger/dual.rs replaced two needless collect+len assertions with lines().count(); src/monitor/ewma.rs changed two test loops to u64 iteration and removed unnecessary i as u64 casts in Duration::from_secs. Validation: cargo fmt --check PASS; rch exec 'cargo test --lib dual' PASS; rch exec 'cargo test --lib ewma' PASS; rch exec 'cargo check --all-targets' PASS; rch exec 'cargo clippy --all-targets -- -D warnings' still FAIL due broad unrelated repo lint backlog; targeted grep on rch clippy --lib output shows no remaining mentions of src/logger/dual.rs or src/monitor/ewma.rs.","created_at":"2026-02-15T16:30:23Z"}]}
{"id":"bd-5vm","title":"CI/CD pipeline with GitHub Actions","description":"## Deliverable\nGitHub Actions CI/CD pipeline for automated linting, comprehensive testing, reproducibility artifact capture, and release.\n\n## Technical Approach\n### Workflow: ci.yml\nJobs:\n1. check: cargo fmt --check + cargo clippy --all-targets -- -D warnings\n2. unit: full unit/property suite (including invariant tests)\n3. integration: full pipeline integration suite\n4. e2e: scripts/e2e_test.sh plus decision-plane e2e scenarios\n5. stress: stress/load scenarios with bounded runtime profile\n6. coverage: cargo llvm-cov thresholds with module-specific minimums\n7. provenance: emit env/manifest/repro.lock style metadata for benchmark/test runs\n\n### Logging and Artifact Contract\nEach CI stage publishes:\n- machine-readable test summary\n- detailed logs for failed cases\n- trace bundles for decision-plane failures\n- timing metrics and flaky-test diagnostics\n\n### Workflow: release.yml\nTriggered by version bump:\n1. Run full quality gate (all ci.yml stages)\n2. Build release binaries for Linux/macOS targets\n3. Create archives + SHA256 checksums\n4. Attach test/provenance summary to release artifacts\n\n## Acceptance Criteria\n- CI runs on PR and push to main\n- Unit, integration, e2e, and stress gates are enforced\n- Decision-plane logs/artifacts are retained on failures\n- Coverage and lint thresholds are enforced\n- Release jobs require successful quality gates and attach checksums/provenance","acceptance_criteria":"1. CI enforces fmt/clippy/tests/coverage gates on PR and push to main. 2. Unit/integration/e2e/stress suites execute with artifact retention for failures. 3. Decision-plane trace bundles and provenance metadata are attached to failing runs. 4. Release workflow requires passing quality gates and emits checksummed artifacts. 5. CI logging remains structured and queryable for root-cause analysis.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:54:52.150598426Z","created_by":"ubuntu","updated_at":"2026-02-15T01:58:23.045969744Z","closed_at":"2026-02-15T01:58:23.045902228Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["infrastructure","testing"],"dependencies":[{"issue_id":"bd-5vm","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-14T19:00:07.276215747Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-2j5","type":"parent-child","created_at":"2026-02-14T21:04:03.456476500Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-2q9","type":"blocks","created_at":"2026-02-14T16:56:23.114193133Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-3sb","type":"blocks","created_at":"2026-02-14T19:00:07.425946352Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-7ls","type":"blocks","created_at":"2026-02-14T18:54:58.146836919Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-izu.7","type":"blocks","created_at":"2026-02-14T19:00:07.598997398Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5vm","depends_on_id":"bd-izu.8","type":"blocks","created_at":"2026-02-14T19:00:07.511573643Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-7ls","title":"Comprehensive unit test suite for all modules","description":"## Deliverable\nComprehensive unit and property tests co-located with each module, with deterministic fixtures and detailed failure logging.\n\n## Scope Expansion (Plan-Space Revision)\nThe unit-test matrix must cover both baseline and advanced tracks:\n- core config/errors/platform\n- monitor (EWMA, PID, predictive policy)\n- scanner (walker, patterns, scoring, deletion)\n- ballast and release logic\n- logger and stats layers\n- decision-plane guards, scheduler, and evidence schema\n\n## Test Families\n### Core/Foundation\n- error code stability and serialization compatibility\n- config parsing/validation/override precedence\n- platform trait behavior with mocks\n\n### Monitoring and Control\n- EWMA trend/forecast correctness\n- PID hysteresis and anti-windup\n- predictive policy mode transitions\n- conformal/e-process guard behavior\n\n### Scanner and Cleanup\n- walker safety invariants (symlink/cross-device/exclusions)\n- scoring determinism + veto invariants\n- deletion safety/circuit-breaker behavior\n- incremental-index equivalence properties\n\n### Logging/Explainability\n- SQLite/JSONL dual-write consistency\n- evidence-ledger schema compatibility\n- explain rendering stability and JSON schema checks\n\n## Detailed Logging Contract\nEvery failing unit/property test logs seed, minimized fixture, invariant delta, and trace IDs when available.\n\n## Acceptance Criteria\n- deterministic and non-flaky unit/property tests\n- explicit coverage for advanced decision-plane features\n- regression fixtures for discovered edge cases\n- CI hard-fail on invariant violations\n- machine-readable failure logs suitable for replay","acceptance_criteria":"1. Every core module has deterministic unit coverage including error and boundary paths. 2. Property/invariant tests cover scoring, control logic, and safety veto semantics. 3. Golden fixtures remain stable across runs and are versioned for reproducibility. 4. Test logs include case identifiers, seed info, and failure context sufficient for replay. 5. Coverage report maps each feature bead to at least one unit/property test group.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:53:49.351806776Z","created_by":"ubuntu","updated_at":"2026-02-15T00:44:52.008429831Z","closed_at":"2026-02-15T00:44:52.008351444Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["testing"],"dependencies":[{"issue_id":"bd-7ls","depends_on_id":"bd-112","type":"blocks","created_at":"2026-02-14T18:58:58.368267831Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T19:02:43.886949722Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T19:02:43.970928039Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-1sw","type":"blocks","created_at":"2026-02-14T19:02:44.056823115Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-1td","type":"blocks","created_at":"2026-02-14T19:02:44.143620650Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T19:02:44.229415187Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-224","type":"blocks","created_at":"2026-02-14T19:02:44.316651985Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T19:02:44.400742953Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2f8","type":"blocks","created_at":"2026-02-14T19:02:44.488140982Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2it","type":"blocks","created_at":"2026-02-14T18:58:58.996635198Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2m9","type":"blocks","created_at":"2026-02-14T18:58:58.819725759Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2np","type":"blocks","created_at":"2026-02-14T19:02:44.744901027Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T19:02:44.573859287Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2qa","type":"blocks","created_at":"2026-02-14T18:58:58.102402669Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2s9","type":"blocks","created_at":"2026-02-14T19:02:44.660334619Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:58:58.015416644Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-394","type":"blocks","created_at":"2026-02-14T19:02:44.834646601Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-395","type":"blocks","created_at":"2026-02-14T18:58:58.642065915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T19:02:44.921680909Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T16:56:23.366023907Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3qm","type":"blocks","created_at":"2026-02-14T18:58:58.280083613Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T19:02:45.009769771Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3s5","type":"blocks","created_at":"2026-02-14T18:58:58.726968529Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T19:02:45.120537679Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T16:56:23.196955649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-7vl","type":"blocks","created_at":"2026-02-14T18:58:58.547475122Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-g0c","type":"blocks","created_at":"2026-02-14T18:58:58.457572116Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-p2u","type":"blocks","created_at":"2026-02-14T18:58:58.907240183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-sth","type":"blocks","created_at":"2026-02-14T19:02:45.217454006Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-u92","type":"blocks","created_at":"2026-02-14T18:58:58.190491539Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-x1k","type":"blocks","created_at":"2026-02-14T19:02:45.330111251Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7ls","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T16:56:23.280502905Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":6,"issue_id":"bd-7ls","author":"Dicklesworthstone","text":"REVIEW: Clarifying the role of this bead vs module-level tests:\n\nEach implementation bead (bd-x9z, bd-3po, bd-25g, etc.) MUST include its own unit tests as part of its deliverable — every bead's acceptance criteria already requires tests.\n\nThis bead (bd-7ls) serves three distinct purposes that go BEYOND individual module tests:\n\n1. CROSS-CUTTING TEST INFRASTRUCTURE: Create shared test helpers used by many modules:\n   - MockPlatform (comprehensive: configurable mount points, memory, fs stats)\n   - TestEnvironment builder (creates realistic directory trees with varied ages, sizes, patterns)\n   - create_fake_rust_target() — builds a realistic target dir with incremental/, deps/, .fingerprint/\n   - SyntheticTimeSeries — generates pressure patterns for EWMA/PID testing\n   These helpers live in a tests/common/ module and are imported by all tests.\n\n2. COVERAGE VERIFICATION: Run cargo-llvm-cov and verify coverage thresholds:\n   - Overall: > 80%\n   - Scoring engine (bd-x9z): > 90%\n   - Pattern matching (bd-1sw): > 95%\n   - PID controller (bd-3po): > 90%\n\n3. GAP-FILLING TESTS: After all modules are implemented, review coverage reports and add tests for any uncovered edge cases, especially:\n   - Cross-module interactions not caught by unit tests\n   - Error paths that require multiple components to fail simultaneously\n   - Boundary conditions at the intersections of modules","created_at":"2026-02-14T17:13:22Z"}]}
{"id":"bd-7vl","title":"Post-event config analysis and auto-tuning recommendations","description":"## Deliverable\nAfter each pressure event, automatically analyze what happened and generate actionable config recommendations. Makes PID controller tuning approachable for non-control-theory users.\n\n## The Problem\nsbh exposes PID gains (kp, ki, kd), hysteresis bands, scoring weights, ballast sizing, and threshold percentages. These are powerful but intimidating. Most users will never tune them. Suboptimal defaults for a specific machine's workload pattern make sbh either too aggressive or too passive.\n\n## The Solution\n\n### Automatic Post-Event Analysis\nAfter each pressure event (transition from Green to elevated level and back), analyze:\n1. Duration of the event\n2. Peak pressure level reached\n3. Ballast files released vs available\n4. Number of artifacts deleted, their scores, and categories\n5. Time from threshold breach to resolution\n6. PID controller integral term behavior (did it reach clamp?)\n7. EWMA prediction accuracy (did we see it coming?)\n\n### Generated Recommendations\nStored in SQLite activity_log, surfaced via CLI:\n\n**Ballast sizing:**\n- \"Ballast exhausted 8 min before pressure resolved on /data. Suggest: ballast.file_count = 15 (currently 10)\"\n- \"Ballast was never fully used — 8/10 files always available. Suggest: ballast.file_count = 6 to save 4 GB\"\n\n**PID tuning:**\n- \"PID integral reached clamp 4 times this week without resolving. Suggest: pid_ki = 0.15 (currently 0.1)\"\n- \"Response oscillated between Orange and Yellow 12 times. Suggest: hysteresis_band_pct = 3.0 (currently 2.0)\"\n\n**Scoring:**\n- \"12 artifacts were deleted during active compilation (age < 45 min). Suggest: min_file_age_minutes = 45 (currently 30)\"\n- \"95% of freed space came from RustTarget category. Other patterns rarely match — scan is efficient.\"\n\n**Threshold:**\n- \"Pressure stayed Yellow for 4+ hours without escalation. Suggest: yellow_threshold_pct = 18.0 (currently 15.0)\"\n\n### CLI Interface\n  sbh tune                    # Show current recommendations\n  sbh tune --apply            # Apply all recommendations (with confirmation)\n  sbh tune --apply --yes      # Apply without confirmation\n  sbh tune --history          # Show past recommendations and whether they were applied\n  sbh tune --json             # Machine-readable output\n\n### Implementation\n```rust\npub struct TuningEngine {\n    stats: Arc<StatsEngine>,\n    config: Config,\n}\n\npub struct Recommendation {\n    pub category: TuningCategory,    // Ballast, PID, Scoring, Threshold\n    pub config_key: String,          // e.g., \"ballast.file_count\"\n    pub current_value: String,\n    pub suggested_value: String,\n    pub rationale: String,           // Plain English explanation\n    pub confidence: f64,             // 0.0-1.0\n    pub evidence: Vec<String>,       // Supporting data points\n    pub risk: TuningRisk,            // Low, Medium, High\n}\n\npub enum TuningRisk {\n    Low,    // Safe to apply automatically\n    Medium, // Review recommended\n    High,   // Manual review required\n}\n```\n\n### Analysis Triggers\n- After each pressure event resolution (Green → Elevated → Green)\n- On weekly schedule (for long-term trend analysis)\n- On demand via sbh tune --analyze\n\n### Learning from User Feedback\nTrack which recommendations users accept/reject:\n- Accepted: reinforce similar recommendations\n- Rejected: reduce confidence for that category\n- Applied then reverted: strong negative signal\n\n## Design Rationale\nThis is the difference between \"works OK with defaults\" and \"gets better over time.\" The data exists in the SQLite activity database — this is pure analysis. Users who don't understand PID controllers get concrete, actionable suggestions. Self-improving systems feel magical.\n\n## Acceptance Criteria\n- Generates recommendations after pressure events\n- Recommendations include plain-English rationale\n- sbh tune shows current recommendations with confidence levels\n- sbh tune --apply modifies config file correctly\n- Recommendations based on actual historical data (not hypothetical)\n- Handles edge cases: no pressure events yet, insufficient data, conflicting signals\n- Risk level assigned to each recommendation\n- Unit tests: recommendation generation with synthetic event data\n- Integration test: simulate pressure event -> verify recommendation generated\n- Test: apply recommendation -> verify config updated -> verify new behavior","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:31:38.926752400Z","created_by":"ubuntu","updated_at":"2026-02-14T23:26:41.497060116Z","closed_at":"2026-02-14T23:26:41.496960990Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","daemon"],"dependencies":[{"issue_id":"bd-7vl","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T19:02:34.637774913Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7vl","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T18:34:24.989658056Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7vl","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T18:53:45.398971591Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7vl","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T18:34:25.064960011Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":35,"issue_id":"bd-7vl","author":"Dicklesworthstone","text":"REVIEW: Test coverage requirements: (1) Unit test: generate synthetic PressureEvent sequence -> verify TuningEngine produces correct recommendations for ballast sizing, PID gains, scoring weights. (2) Unit test: conflicting signals (oscillation + integral clamp) -> verify recommendation priority/risk. (3) Integration test: populate SQLite with realistic event data -> run TuningEngine -> verify recommendations match expected output. (4) Test: sbh tune --apply modifies config file atomically (write to .tmp, rename). (5) Test: insufficient data (<3 events) -> TuningEngine returns empty recommendations with 'insufficient data' message.","created_at":"2026-02-14T18:54:14Z"},{"id":45,"issue_id":"bd-7vl","author":"Dicklesworthstone","text":"MINOR: Config-write capability — sbh tune --apply needs to modify the TOML config file. Implementation: (1) Read existing config file as raw TOML string. (2) Use toml_edit crate (preserves formatting, comments) to modify specific values. (3) Write to .tmp file, then atomic rename. (4) Log each changed value with old and new values. NOTE: toml_edit must be added as a dependency in bd-1kn scaffolding. It is NOT the same as the toml crate (toml parses, toml_edit preserves structure).","created_at":"2026-02-14T18:55:13Z"}]}
{"id":"bd-g0c","title":"Pre-build disk space check command (sbh check)","description":"## Deliverable\nA fast (<50ms) command that checks if there is enough disk space for a build, designed for use as a pre-build gate.\n\n## Usage\n  sbh check                     # Check default paths, exit 0 if OK\n  sbh check /data               # Check specific path\n  sbh check --need 20G          # Check if 20GB available\n  sbh check --predict 60        # Check if EWMA predicts enough space for 60 min\n  sbh check && cargo build      # Pre-build gate\n  alias cb='sbh check && cargo build'  # Shell alias\n\n## Exit Codes\n  0 = OK (enough space)\n  1 = WARNING (space available but predicted to run out within --predict window)\n  2 = CRITICAL (insufficient space right now)\n\n## Output (stderr, non-blocking)\n  On success: nothing (silent)\n  On warning: \"sbh: /data has 12.3 GB free but predicted full in 45 min\"\n  On critical: \"sbh: /data has 1.2 GB free (0.8%). Run: sbh emergency /data\"\n\n## Implementation\n- Read state.json for daemon data (EWMA predictions) if available\n- Fallback: direct statvfs call if daemon not running\n- Must complete in <50ms (no scanning, no database, just fs stats + prediction check)\n- JSON output with --json flag for agent consumption\n\n## Design Rationale\nPrevents the problem entirely rather than cleaning up after. Users can integrate into their workflow with a one-line alias. Agents can use it as a pre-build check in their workflows. The <50ms latency requirement means it adds negligible overhead to build commands.\n\n## Acceptance Criteria\n- Completes in <50ms (benchmarked)\n- Correct exit codes for all scenarios\n- Works without daemon running (degraded mode, no prediction)\n- Works with daemon running (uses EWMA prediction from state.json)\n- --need flag checks absolute space requirement\n- --predict flag checks time-to-exhaustion\n- --json output for agent consumption\n- Silent on success (no stdout, no stderr)","acceptance_criteria":"1. Unit tests cover threshold math, target-free calculations, and exit-code semantics. 2. Integration tests validate coupling with fs stats, notifier, and CLI formatting layers. 3. E2E scenarios simulate low-space and healthy-space runs with --json and human outputs. 4. Command never mutates disk state and returns deterministic recommendations for fixed inputs. 5. Detailed logs include free/used snapshots, threshold comparisons, and recommended next actions.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:33:31.760342120Z","created_by":"ubuntu","updated_at":"2026-02-14T21:55:02.889097942Z","closed_at":"2026-02-14T21:55:02.889020908Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-g0c","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T18:34:33.116687834Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g0c","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T18:34:33.031161234Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":28,"issue_id":"bd-g0c","author":"Dicklesworthstone","text":"REVIEW: Removed bd-x1k (EWMA) dependency. sbh check reads daemon state from state.json (bd-2np) which already contains EWMA predictions. When daemon is not running, sbh check falls back to direct statvfs() — no EWMA needed. The check command does NOT import or run the EWMA module; it only reads pre-computed predictions from state.json. Dependencies: bd-2rq (CLI framework) + bd-2pj (fs stats for fallback mode).","created_at":"2026-02-14T18:53:30Z"},{"id":52,"issue_id":"bd-g0c","author":"Dicklesworthstone","text":"REVIEW-2: state.json format contract is defined in bd-2np. sbh check reads it opportunistically — if the file exists and is fresh (<30s old), use EWMA predictions for --predict flag; otherwise fall back to direct statvfs(). No hard dep on bd-2np needed since the read is optional with graceful fallback.","created_at":"2026-02-14T19:03:14Z"}]}
{"id":"bd-izu","title":"Alien Artifact Decision Plane v1 (shadow-first, explainable, calibrated)","description":"## Background\nsbh has strong foundational beads, but advanced decision-plane capabilities (shadow rollout, calibrated uncertainty, explainability contracts, and reproducible proof artifacts) were not represented as a cohesive executable plan.\n\n## Goal\nShip a mathematically-rigorous decision plane that stays conservative by default, exposes full evidence for every action, and provides deterministic fallbacks under uncertainty and drift.\n\n## Scope\nThis epic coordinates six implementation tracks plus dedicated verification tracks:\n1. Shadow mode + progressive rollout controller\n2. Evidence ledger schema + explain surfaces\n3. Incremental Merkle scan index\n4. VOI-based scan budget scheduler\n5. Conformal/e-process guardrails for adaptive actions\n6. Proof-grade benchmark/replay/fault-injection harness\n7. Decision-plane unit/property tests\n8. Decision-plane e2e scenarios with trace logging\n\n## Constraints\n- Safe under partial failure and low-confidence prediction\n- Deterministic conservative fallback always available\n- Comprehensive unit and e2e validation with detailed logs\n- Reproducibility artifacts attached to performance and safety claims\n\n## Adoption Wedge\nobserve-only (shadow) -> canary -> controlled ramp -> default-on with kill-switch.\n\n## Budgeted Mode Requirement\nEvery adaptive component must define explicit resource caps (time/memory/work budget) and exact on-exhaustion behavior.\n\n## Success Criteria\n- No dependency cycles introduced\n- Every track has explicit fallback semantics and recovery criteria\n- Decision traces are explainable and queryable via CLI/API\n- Unit and e2e tracks cover all decision-plane features\n- Performance and safety claims have replayable evidence packs","acceptance_criteria":"1. All decision-plane tracks define deterministic fallback-safe behavior and recovery criteria. 2. Every adaptive decision is explainable via evidence ledger and trace surfaces. 3. Unit/property/e2e/stress/proof beads collectively cover all scoped decision-plane features. 4. Detailed structured logging and trace bundles are mandatory for failures and regressions. 5. No dependency cycles or unverifiable performance/safety claims remain.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-14T18:48:13.506612829Z","created_by":"ubuntu","updated_at":"2026-02-15T01:48:18.977804533Z","closed_at":"2026-02-15T01:48:18.977699466Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","alien-graveyard","decision-plane"]}
{"id":"bd-izu.1","title":"Shadow-mode policy engine with progressive delivery gates","description":"## Deliverable\\nA decision-policy engine that runs in shadow mode first (observe-only), computes the same cleanup actions as enforce mode, and logs decision deltas without deleting anything.\\n\\n## Background and Rationale\\nThe project already has predictive and scoring beads, but production-safe rollout needs an explicit policy lifecycle. Shadow-first rollout is the highest-confidence way to prevent destructive false positives while still learning from real workload behavior.\\n\\n## Adoption Wedge\\nobserve -> canary -> enforce, with immediate fallback_safe at any time.\\n\\n## Budgeted Mode\\n- Max evaluations per loop: 100 candidates\\n- Max hypothetical delete set per loop: 25\\n- Max canary deletes per hour: configurable cap\\n- On budget exhaustion: stop action planning and remain in observe mode\\n\\n## Expected-Loss Model\\nStates: useful, abandoned\\nActions: keep, delete, review\\nLoss defaults: L(delete,useful)=100, L(keep,abandoned)=30, L(review,*)=5\\nPolicy applies delete only when expected_loss_delete + guard_penalty is strictly below expected_loss_keep.\\n\\n## Calibration and Fallback Trigger\\nFallback_safe trigger when any condition holds:\\n- calibration_score below configured floor for N windows\\n- guardrail breach (delete budget, drift alarm, policy error)\\n- evidence serialization failure\\nRecovery requires M consecutive clean windows.\\n\\n## Implementation Plan\\n1. Add policy modes and mode transitions to daemon loop.\\n2. Execute scoring/planning in observe mode with side effects disabled.\\n3. Emit decision_trace events for hypothetical and actual paths.\\n4. Enforce rollout guardrails and kill-switches.\\n5. Add automatic fallback transitions and recovery gates.\\n\\n## Proof Artifacts\\n- Shadow-vs-enforce replay diff report\\n- Drift and guardrail activation report\\n- Deterministic trace corpus with fixed seeds\\n\\n## Rollback\\nSingle config/env switch forces fallback_safe and disables enforce actions.\\n\\n## Baseline Comparator\\nCurrent threshold-plus-scoring reactive execution without progressive delivery gates.\\n\\n## Acceptance Criteria\\n- Observe mode produces no filesystem mutation\\n- Guardrails cap canary impact deterministically\\n- Fallback_safe engages automatically on trigger conditions\\n- Recovery requires explicit clean-window criterion\\n- Unit tests for transition graph and guardrails\\n- E2E scenario validates shadow, canary, fallback with detailed logs","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:48:21.719089634Z","created_by":"ubuntu","updated_at":"2026-02-15T00:47:04.752757737Z","closed_at":"2026-02-15T00:47:04.752667117Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","daemon","safety"],"dependencies":[{"issue_id":"bd-izu.1","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T18:49:29.684471170Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-1hh","type":"blocks","created_at":"2026-02-14T18:49:29.466487241Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:49:29.803716781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T18:49:29.574440511Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:21.719089634Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-izu.2","type":"blocks","created_at":"2026-02-14T18:51:02.593569428Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.1","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T18:49:29.342719283Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.2","title":"Evidence ledger schema + explain command (galaxy-brain levels)","description":"## Deliverable\\nA unified evidence-ledger schema recording why each cleanup decision was made, with explain surfaces for operators and agents.\\n\\n## Background and Rationale\\nAdvanced policy behavior is unusable without transparent decision provenance. This bead provides machine-readable and human-readable explainability that can be audited and replayed.\\n\\n## Schema Contract\\nEach decision record must include:\\n- decision_id, trace_id, policy_mode, timestamp\\n- candidate features and factor contributions\\n- posterior estimates and expected-loss values\\n- selected action and guard outcomes\\n- calibration_score, fallback_active, fallback_reason\\n- comparator_action (for shadow/canary diffing)\\n\\n## Explain Surfaces\\n- sbh explain --id <decision_id>\\n- sbh scan --explain <path>\\n- dashboard integration with level-0 to level-3 detail\\n\\n## Galaxy-Brain Levels\\nLevel 0: concise recommendation\\nLevel 1: weighted factor table\\nLevel 2: posterior, loss, calibration values\\nLevel 3: full serialized trace payload for replay/debug\\n\\n## Budgeted Mode\\nIf evidence rendering is expensive, cap expanded explain payload generation per request; return compact mode with pointer to stored trace.\\n\\n## Fallback Trigger\\nAny ledger write/serialization failure triggers safe policy fallback and emits explicit error events.\\n\\n## Proof Artifacts\\n- Snapshot corpus for explain outputs\\n- Compatibility tests for schema evolution\\n- Replay verifier reading ledger-only traces\\n\\n## Baseline Comparator\\nCurrent ad-hoc scan explanation and log records without unified decision provenance.\\n\\n## Acceptance Criteria\\n- Every decision has a persistent evidence record\\n- Explain output deterministic for identical inputs\\n- Stats engine can aggregate on evidence fields\\n- Unit tests for schema and rendering stability\\n- E2E test covers scan/clean/explain path with verbose logs","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:48:29.946253205Z","created_by":"ubuntu","updated_at":"2026-02-15T00:42:26.056843967Z","closed_at":"2026-02-15T00:42:26.056775899Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","cli","logger"],"dependencies":[{"issue_id":"bd-izu.2","depends_on_id":"bd-1gm","type":"blocks","created_at":"2026-02-14T18:49:30.215088931Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.2","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T18:49:30.111896574Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.2","depends_on_id":"bd-3s5","type":"blocks","created_at":"2026-02-14T18:51:02.934448491Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.2","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:29.946253205Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.2","depends_on_id":"bd-nhm","type":"blocks","created_at":"2026-02-14T18:49:30.009594605Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.2","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T18:49:29.898962059Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.3","title":"Incremental Merkle scan index with full-scan fallback","description":"## Deliverable\\nAn incremental scan index keyed by path and metadata hashes so repeated daemon cycles avoid full recursive scans when nothing changed.\\n\\n## Background and Rationale\\nFull recursive scanning is expected to dominate runtime cost as monitored trees grow. Incremental Merkle indexing provides an asymptotic and practical latency reduction while preserving deterministic behavior via reference fallback.\\n\\n## Adoption Wedge\\n- Phase A: index build + shadow comparison only\\n- Phase B: canary incremental reads on subset of paths\\n- Phase C: full incremental default with periodic full-scan verification\\n\\n## Budgeted Mode\\n- Max subtree hash updates per loop\\n- Max checkpoint write bytes per interval\\n- On budget exhaustion: partial updates deferred and full scan used for affected roots\\n\\n## Isomorphism Proof Plan\\nFor the same filesystem snapshot, incremental and full scan must yield identical candidate set/order.\\n- Candidate-set equality checks\\n- Stable ordering checks\\n- Mismatch auto-fallback and trace emission\\n\\n## Safety and Fallback\\nAny index checksum mismatch, corruption, or replay inconsistency forces immediate full-scan mode and marks index unhealthy until rebuilt.\\n\\n## Proof Artifacts\\n- Replay fixtures for churn/no-churn workloads\\n- p50/p95/p99 scan latency report before/after\\n- mismatch incident traces with root-cause fields\\n\\n## Baseline Comparator\\nCurrent full recursive walk on each scan cycle.\\n\\n## Acceptance Criteria\\n- Incremental and full paths are behaviorally isomorphic\\n- Automatic fallback works for all detected integrity failures\\n- Demonstrable low-churn speedup without safety regression\\n- Unit tests for Merkle updates/checkpoint recovery\\n- E2E tests for healthy/degraded/index-rebuild scenarios","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:48:36.299300341Z","created_by":"ubuntu","updated_at":"2026-02-14T23:40:23.865001004Z","closed_at":"2026-02-14T23:40:23.864978842Z","close_reason":"Implemented incremental Merkle scan index with full-scan fallback: 20 unit tests, SHA-256 metadata hashing, checkpoint persistence with integrity verification, budget-aware diff, automatic corruption detection","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","performance","scanner"],"dependencies":[{"issue_id":"bd-izu.3","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T18:49:30.454270606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.3","depends_on_id":"bd-3qw","type":"blocks","created_at":"2026-02-14T18:49:30.658494964Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.3","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T18:49:30.554835844Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.3","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:36.299300341Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.4","title":"Value-of-Information scheduler for scan budget allocation","description":"## Deliverable\\nA Value-of-Information scheduler that allocates scan budget to paths with highest expected reclaimed-bytes-per-IO while maintaining safety and exploration guarantees.\\n\\n## Background and Rationale\\nFixed-frequency scanning wastes cycles under pressure and large tree counts. VOI scheduling prioritizes high-yield scan targets while preserving deterministic safety behavior.\\n\\n## Utility Model\\nutility(path) = expected_reclaim_bytes - io_cost_penalty - false_positive_risk_penalty\\nwith explicit uncertainty discount and exploration bonus for under-sampled paths.\\n\\n## Budgeted Mode\\n- Fixed scan budget per interval\\n- Per-path minimum exploration quota\\n- On budget exhaustion: deterministic round-robin fallback for remaining paths\\n\\n## Expected-Loss Framing\\nStates: high-yield, low-yield, risky\\nActions: scan_now, defer, fallback_scan\\nLoss discourages starvation and risky over-scanning.\\n\\n## Calibration and Fallback\\nIf forecast-error exceeds threshold across N windows, disable VOI prioritization and revert to baseline tiered scheduler until recalibrated.\\n\\n## Proof Artifacts\\n- Forecast vs realized utility report\\n- reclaim-per-IO comparison against fixed-tier baseline\\n- starvation guard audit for low-activity paths\\n\\n## Baseline Comparator\\nCurrent tiered fixed-interval scan scheduling.\\n\\n## Acceptance Criteria\\n- Higher reclaim-per-IO than baseline in benchmark scenarios\\n- No path starvation under exploration constraints\\n- Deterministic fallback path tested and documented\\n- Unit tests for ranking, quotas, and forecast error handling\\n- E2E mixed-workload scenario with full scheduler telemetry","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:48:44.768366852Z","created_by":"ubuntu","updated_at":"2026-02-14T23:56:04.375601382Z","closed_at":"2026-02-14T23:56:04.375580853Z","close_reason":"Implemented VOI scheduler: utility model (reclaim/IO/FP-risk/exploration), budgeted mode with exploration quotas, calibration with automatic round-robin fallback, 14 unit tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","monitoring","scheduler"],"dependencies":[{"issue_id":"bd-izu.4","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:49:30.960359915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.4","depends_on_id":"bd-48o","type":"blocks","created_at":"2026-02-14T18:49:31.044019061Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.4","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:44.768366852Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.4","depends_on_id":"bd-izu.3","type":"blocks","created_at":"2026-02-14T18:49:30.773230792Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.4","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T18:49:30.874438403Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.5","title":"Conformal and e-process guardrails for adaptive controller actions","description":"## Deliverable\\nA statistical guard layer for predictive and PID-driven adaptive decisions using conformal-style calibration checks plus anytime-valid e-process alarms.\\n\\n## Background and Rationale\\nAdaptive controllers can become overconfident under distribution shift. This bead provides explicit finite-sample guardrails so high-impact actions are blocked when confidence is not justified.\\n\\n## Guard Contract\\n- Compute rolling calibration diagnostics for forecast quality\\n- Maintain e-process statistic for drift/overconfidence detection\\n- Gate aggressive cleanup and emergency escalation on guard pass\\n- Trigger fallback-safe policy on guard fail\\n\\n## Budgeted Mode\\n- Bounded historical window for guard computation\\n- Bounded per-loop statistic updates\\n- On compute budget exhaustion: mark guard unknown and force conservative fallback\\n\\n## Expected-Loss and Safety\\nExpected-loss controller output is considered valid only when guard status is PASS.\\nStates: calibrated, uncalibrated\\nActions: adaptive_allowed, adaptive_blocked\\nLoss strongly penalizes adaptive actions under uncalibrated state.\\n\\n## Recovery Policy\\nRequire M consecutive clean windows plus low e-process evidence before re-enabling adaptive mode.\\n\\n## Proof Artifacts\\n- Synthetic drift injections with trigger timestamps\\n- False-alarm and miss-rate tradeoff report\\n- Replay traces showing guard-driven action suppression\\n\\n## Baseline Comparator\\nCurrent controller behavior without formal calibration gate.\\n\\n## Acceptance Criteria\\n- Guard catches induced drift and overconfidence cases\\n- High-impact actions blocked when guard != PASS\\n- Recovery flow requires explicit clean-window criterion\\n- Unit tests for sequential guard math and thresholds\\n- E2E drift scenario validates fallback activation with detailed logs","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:48:53.315900870Z","created_by":"ubuntu","updated_at":"2026-02-15T00:33:12.424765560Z","closed_at":"2026-02-15T00:33:12.424694608Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","monitoring","safety"],"dependencies":[{"issue_id":"bd-izu.5","depends_on_id":"bd-2up","type":"blocks","created_at":"2026-02-14T18:49:31.302889348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.5","depends_on_id":"bd-3po","type":"blocks","created_at":"2026-02-14T18:49:31.214234388Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.5","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:53.315900870Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.5","depends_on_id":"bd-x1k","type":"blocks","created_at":"2026-02-14T18:49:31.129652584Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.6","title":"Decision-plane proof harness: replay, fault-injection, and reproducibility pack","description":"## Deliverable\\nA proof harness validating decision-plane behavior with deterministic replay, fault-injection, and reproducibility artifacts suitable for CI and local triage.\\n\\n## Background and Rationale\\nAdvanced policy work must ship with evidence, not anecdotes. This harness is the contract that turns optimization/safety claims into reproducible proof artifacts.\\n\\n## Scope\\n1. Deterministic replay engine for decision traces and mode transitions\\n2. Fault-injection suite (IO errors, stale stats, lock contention, delayed telemetry, serializer failures)\\n3. Reproducibility pack emission: env.json, manifest.json, repro.lock, trace bundle\\n4. Statistical benchmark report: p50/p95/p99 with baseline comparator and variance context\\n\\n## Budgeted Mode\\nTest harness supports fast and full modes:\\n- Fast mode: reduced scenario set for pre-commit\\n- Full mode: complete matrix for CI/nightly\\n\\n## Fallback Validation\\nEvery fault scenario must assert expected fallback behavior and explicit reason codes.\\n\\n## Proof Artifacts\\n- Replay report with pass/fail by invariant\\n- Fault matrix with expected vs observed fallback action\\n- Benchmark delta report against baseline\\n\\n## Baseline Comparator\\nCurrent ad-hoc tests without formal replay/fault/provenance packaging.\\n\\n## Acceptance Criteria\\n- Replay outcomes deterministic across repeated runs\\n- Fault matrix validates all fallback invariants\\n- Repro packs generated and consumable by tooling\\n- Unit tests validate parser/invariant checks\\n- E2E workflow integrates into CI with verbose logging outputs","acceptance_criteria":"1. Proof harness reproduces decision outcomes from evidence traces across runs. 2. Fault-injection suite validates conservative fallback on serialization/index/guard failures. 3. Benchmark and replay artifacts include manifest + seed + environment provenance. 4. Harness emits machine-readable verdicts and differential reports for regression detection. 5. Logging captures full proof chain from input trace through policy decision assertions.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:48:59.044222337Z","created_by":"ubuntu","updated_at":"2026-02-15T01:11:55.110735548Z","closed_at":"2026-02-15T01:11:55.110632946Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","infrastructure","testing"],"dependencies":[{"issue_id":"bd-izu.6","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-14T18:51:02.761141777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-7ls","type":"blocks","created_at":"2026-02-14T18:51:02.845849657Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:48:59.044222337Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu.1","type":"blocks","created_at":"2026-02-14T18:49:31.384815790Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu.2","type":"blocks","created_at":"2026-02-14T18:49:31.468966587Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu.3","type":"blocks","created_at":"2026-02-14T18:49:31.556404177Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu.4","type":"blocks","created_at":"2026-02-14T18:49:31.642828411Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.6","depends_on_id":"bd-izu.5","type":"blocks","created_at":"2026-02-14T18:49:31.728867813Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.7","title":"Decision-plane e2e scenario pack with verbose trace logging","description":"## Deliverable\nEnd-to-end scenario pack for shadow, canary, enforce, and fallback behavior under realistic pressure and failure modes.\n\n## Scenario Matrix\n1. Burst growth with safe shadow recommendations\n2. Canary pass with bounded impact and trace capture\n3. Calibration drift causing guard fail and fallback\n4. Index corruption causing full-scan fallback\n5. Injected IO/serializer faults causing safe degradation\n6. Progressive recovery from fallback after clean windows\n\n## Logging Contract\nEach step emits:\n- trace_id and decision_id\n- policy mode and guard status\n- pressure before/after, bytes reclaimed, action reason\n- fallback reason code when relevant\n- scan budget and scheduler decision details\n\n## Determinism Contract\nScenario outcomes must be deterministic under fixed seeds and fixture inputs.\n\n## Artifacts\n- machine-readable run report\n- per-scenario logs\n- summary pass/fail table\n- per-scenario replay bundle references\n\n## Acceptance Criteria\n- deterministic scenario outcomes\n- asserted fallback triggers and reasons\n- integrated into CI e2e workflow\n- verbose logs sufficient for postmortem debugging\n- explicit checks for no unintended deletions in shadow mode","acceptance_criteria":"1. E2E scenarios cover observe/canary/enforce/fallback/recovery decision-plane lifecycle. 2. Scenarios include drift, budget exhaustion, evidence failures, and index fallback cases. 3. Verbose logging captures action decisions, guard statuses, and remediation triggers at each step. 4. Machine-readable outputs include trace IDs and expected-vs-actual comparisons. 5. Scenario set is deterministic under fixed seeds and reproducible in CI.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:49:11.597026399Z","created_by":"ubuntu","updated_at":"2026-02-15T01:47:43.086344525Z","closed_at":"2026-02-15T01:47:43.086283Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","e2e","testing"],"dependencies":[{"issue_id":"bd-izu.7","depends_on_id":"bd-2q9","type":"blocks","created_at":"2026-02-14T18:49:31.896496167Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.7","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:49:11.597026399Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.7","depends_on_id":"bd-izu.6","type":"blocks","created_at":"2026-02-14T18:49:31.811121118Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.7","depends_on_id":"bd-izu.8","type":"blocks","created_at":"2026-02-14T18:51:02.678560689Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-izu.8","title":"Decision-plane unit-test matrix with invariant/property checks","description":"## Deliverable\nComprehensive unit and property-test matrix for decision-plane math, invariants, and safe-mode transitions.\n\n## Invariant Families\n- deterministic ranking and tie-break stability\n- posterior/loss monotonicity under stronger evidence\n- guard state machine safety (no unsafe transitions)\n- Merkle incremental equivalence properties\n- fallback dominance under uncertainty/error states\n\n## Property Testing Requirements\n- randomized fixtures with reproducible seeds\n- shrink to minimal counterexample\n- include seed and reduced case in failure logs\n- capture decision trace snapshot for each failing property\n\n## Detailed Logging Contract\nEach test failure must log:\n- seed and minimized input fixture\n- expected vs actual invariant expression\n- relevant policy mode and guard state\n- trace identifiers for replay reproduction\n\n## Acceptance Criteria\n- invariant coverage across all core decision components\n- deterministic re-run reproducibility for failures\n- regression fixtures for discovered edge cases\n- CI gating on invariant/property failures\n- explicit tests for shadow/canary/fallback mode transition safety","acceptance_criteria":"1. Unit/property matrix covers all decision-plane components and invariants. 2. Determinism, calibration, and fallback contracts are validated under seeded replay fixtures. 3. Statistical guard math and scheduler/index interactions have boundary and adversarial tests. 4. Verbose traces include seed, rule IDs, posterior/guard values, and assertion provenance. 5. Failures produce replay-ready artifacts consumed by proof/e2e beads.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T18:49:11.608749397Z","created_by":"ubuntu","updated_at":"2026-02-15T00:52:14.963835038Z","closed_at":"2026-02-15T00:52:14.963770507Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","testing"],"dependencies":[{"issue_id":"bd-izu.8","depends_on_id":"bd-7ls","type":"blocks","created_at":"2026-02-14T18:49:32.408982973Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu","type":"parent-child","created_at":"2026-02-14T18:49:11.608749397Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu.1","type":"blocks","created_at":"2026-02-14T18:49:31.982181667Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu.2","type":"blocks","created_at":"2026-02-14T18:49:32.066012515Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu.3","type":"blocks","created_at":"2026-02-14T18:49:32.151646108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu.4","type":"blocks","created_at":"2026-02-14T18:49:32.235443182Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-izu.8","depends_on_id":"bd-izu.5","type":"blocks","created_at":"2026-02-14T18:49:32.322015693Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nhm","title":"sbh scan command: manual artifact scan with scoring report","description":"## Deliverable\nCommand to manually trigger a build artifact scan, showing all candidates with their scores and classification, without deleting anything.\n\n## Technical Approach\n### Usage\n```bash\nsbh scan                     # Scan all configured paths\nsbh scan /data/projects      # Scan specific path\nsbh scan --min-score 0.5     # Only show candidates above threshold\nsbh scan --top 20            # Show top 20 candidates\nsbh scan --json              # Machine-readable output\nsbh scan --explain <path>    # Detailed scoring breakdown for specific path\n```\n\n### Output\n```\nBuild Artifact Scan Results\n  Scanned: 15,234 directories in 3.2 seconds\n  Candidates found: 47 (above threshold 0.3)\n\n  ┌─────┬──────────────────────────────────────────┬──────────┬──────────┬────────┬────────┐\n  │  #  │ Path                                     │ Size     │ Age      │ Score  │ Type   │\n  ├─────┼──────────────────────────────────────────┼──────────┼──────────┼────────┼────────┤\n  │  1  │ /tmp/cargo-target-quietwillow-mvcc       │   4.2 GB │ 6h 15m   │  0.94  │ Rust   │\n  │  2  │ /data/projects/pi/.target_opus_main      │   8.1 GB │ 4h 02m   │  0.91  │ Rust   │\n  │  3  │ /data/tmp/cargo-target                   │  12.4 GB │ 3h 45m   │  0.89  │ Rust   │\n  │  4  │ /tmp/target-ubuntu-am                    │   2.8 GB │ 8h 30m   │  0.87  │ Rust   │\n  │  ...│                                          │          │          │        │        │\n  └─────┴──────────────────────────────────────────┴──────────┴──────────┴────────┴────────┘\n\n  Total reclaimable: 142.7 GB\n  Use 'sbh clean' to delete these candidates.\n```\n\n### Explain Mode\nFor debugging scoring decisions:\n```\nsbh scan --explain /data/projects/pi/.target_opus_main\n\nScoring Breakdown for: /data/projects/pi/.target_opus_main\n  Type: Directory (8.1 GB, 14,523 files)\n  Modified: 4 hours 2 minutes ago\n  Classification: RustTarget (confidence: 0.95)\n\n  Factor        │ Weight │ Raw Value │ Weighted\n  ──────────────┼────────┼───────────┼─────────\n  Location      │  0.25  │    0.80   │   0.200\n  Name/Pattern  │  0.25  │    0.95   │   0.238\n  Age           │  0.20  │    0.90   │   0.180\n  Size          │  0.15  │    0.95   │   0.143\n  Structure     │  0.15  │    0.90   │   0.135\n  ──────────────┼────────┼──────────┼─────────\n  Base Score    │        │           │   0.895\n  Pressure ×    │        │     1.0   │\n  Final Score   │        │           │   0.895\n\n  Structural markers found: incremental/, deps/, .fingerprint/\n  No vetoes triggered.\n  Verdict: HIGH confidence deletion candidate\n```\n\n## Acceptance Criteria\n- Scans configured paths and reports all candidates\n- Scoring matches the scoring engine output exactly\n- --explain provides full factor breakdown\n- --json output includes all scoring data\n- Performance: scans /data/projects in < 10 seconds\n- Results sorted by score descending\n- No modifications to filesystem (read-only scan)\n- Unit tests for output formatting","acceptance_criteria":"1. Unit tests cover filter parsing, ranking limits, and deterministic ordering output. 2. Integration tests validate scanner + scorer + protection interactions and veto visibility. 3. E2E scripts cover dry-run style scans, min-score, top-N, explain output, and JSON contracts. 4. Output schemas remain stable and machine-parseable under empty and high-volume scan sets. 5. Detailed scan traces include candidate factors, veto reasons, and summary counters.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:53:01.202437634Z","created_by":"ubuntu","updated_at":"2026-02-14T21:47:22.615744673Z","closed_at":"2026-02-14T21:47:22.615657640Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-nhm","depends_on_id":"bd-1w9","type":"blocks","created_at":"2026-02-14T19:02:35.617481915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-nhm","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:13.626065954Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-nhm","depends_on_id":"bd-x9z","type":"blocks","created_at":"2026-02-14T16:56:13.523209855Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-p1b","title":"Clippy debt burn-down slice: logger stats numeric/cast cleanup","description":"Reduce top clippy blockers in src/logger/stats.rs (readable literals, safer casts, arithmetic style) with behavior-preserving changes and rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:22:14.616844350Z","created_by":"ubuntu","updated_at":"2026-02-15T16:24:16.646299722Z","closed_at":"2026-02-15T16:24:16.646272551Z","close_reason":"Completed","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":103,"issue_id":"bd-p1b","author":"Dicklesworthstone","text":"Completed logger stats clippy slice in src/logger/stats.rs with behavior-preserving fixture cleanup: removed risky casts by using explicit typed tuples, replaced index-based casts in loops, and fixed unreadable duration literals (259_200, 604_800). Validation: rch exec \"cargo test --lib stats\" PASS; rch exec \"cargo check --all-targets\" PASS; cargo fmt --check PASS; rch exec \"cargo clippy --all-targets -- -D warnings\" still fails with broader backlog, now reporting 68 lib-test errors in final summary plus non-lib/example/test issues.","created_at":"2026-02-15T16:24:11Z"}]}
{"id":"bd-p2u","title":"sbh stats command: activity statistics with time-window aggregation","description":"## Deliverable\nCommand that queries the activity database and displays comprehensive statistics about sbh's recent behavior across multiple time windows.\n\n## Technical Approach\n### Usage\n```bash\nsbh stats                    # Full summary across all windows\nsbh stats --window 1h        # Stats for specific window\nsbh stats --top-patterns 10  # Top 10 most-deleted patterns\nsbh stats --top-deletions 5  # 5 largest individual deletions\nsbh stats --pressure-history # Pressure level history\nsbh stats --json             # Machine-readable output\n```\n\n### Output\nUses the StatsEngine from the statistics query engine bead. Formats the WindowStats data into colored tables for terminal or JSON for agents.\n\n### Additional Views\n- **Pressure timeline**: ASCII chart showing pressure level over time\n- **Deletion heatmap**: Which hours of day see the most deletions\n- **Pattern breakdown**: Pie-chart-like breakdown of deleted patterns\n\n## Acceptance Criteria\n- All time windows produce correct, verifiable statistics\n- --json output is complete and well-formed\n- Works with empty database (shows zeros, not errors)\n- Performance: < 200ms even with large databases\n- Human output is clear and scannable","acceptance_criteria":"1. Unit tests validate window aggregation, percentile math, and top-N ranking determinism. 2. Integration tests validate query engine interaction with realistic activity database volumes. 3. E2E scripts cover empty DB, normal workload, and heavy-history scenarios with JSON and human outputs. 4. Performance remains within target latency under large datasets. 5. Detailed stats logs include query window selection, aggregation counts, and fallback/empty-result reasoning.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-14T16:52:37.134261855Z","created_by":"ubuntu","updated_at":"2026-02-14T23:20:04.532795883Z","closed_at":"2026-02-14T23:20:04.532729178Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"],"dependencies":[{"issue_id":"bd-p2u","depends_on_id":"bd-2rq","type":"blocks","created_at":"2026-02-14T16:56:13.421817779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-p2u","depends_on_id":"bd-3cj","type":"blocks","created_at":"2026-02-14T16:56:13.314467055Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-r1b","title":"Clippy debt burn-down slice: integration/common test helper lint blockers","description":"Fix immediate clippy -D warnings blockers in tests/common/mod.rs and tests/integration_tests.rs (format_push_string and needless_raw_string_hashes), preserving behavior with focused rch validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-15T16:29:05.571196609Z","created_by":"ubuntu","updated_at":"2026-02-15T16:32:45.743979591Z","closed_at":"2026-02-15T16:32:45.743960826Z","close_reason":"Completed: integration/common test clippy blockers resolved and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["clippy","lint","quality"],"comments":[{"id":107,"issue_id":"bd-r1b","author":"Dicklesworthstone","text":"Completed non-overlap clippy slice in tests/common + integration test target.\\n\\nChanges:\\n- tests/common/mod.rs: replaced format!+push_str with writeln! to clear format_push_string.\\n- tests/integration_tests.rs: removed needless raw-string hashes; fixed redundant clone; inlined format args in asserts; replaced float assert_eq with epsilon comparisons; removed lossy casts/suboptimal flops in calibration helpers using checked u32 conversion + mul_add; switched candidate loops to u64 ranges to avoid sign-loss casts.\\n\\nValidation:\\n- rch exec \"cargo clippy --test integration_tests -- -D warnings\" ✅\\n- rch exec \"cargo check --test integration_tests\" ✅\\n- rch exec \"cargo check --all-targets\" ✅\\n- cargo fmt --check ✅\\n- rch exec \"cargo clippy --all-targets -- -D warnings\" ❌ (remaining repo-wide lint backlog outside this slice; e.g., src/scanner/merkle.rs and other files)","created_at":"2026-02-15T16:32:39Z"}]}
{"id":"bd-sth","title":"Platform abstraction layer (PAL) for Linux/macOS/Windows","description":"## Deliverable\nCross-platform abstraction traits that encapsulate OS-specific operations.\n\n## Technical Approach\n### Core PAL Trait\n```rust\npub trait Platform: Send + Sync {\n    /// Get filesystem statistics for a path (free space, total space, filesystem type)\n    fn fs_stats(&self, path: &Path) -> Result<FsStats, SbhError>;\n    \n    /// List all mount points with their filesystem types\n    fn mount_points(&self) -> Result<Vec<MountPoint>, SbhError>;\n    \n    /// Check if a path is on a RAM-backed filesystem (tmpfs, devshm, ramfs)\n    fn is_ram_backed(&self, path: &Path) -> Result<bool, SbhError>;\n    \n    /// Get the platform-specific service management interface\n    fn service_manager(&self) -> Box<dyn ServiceManager>;\n    \n    /// Get the platform-specific default paths\n    fn default_paths(&self) -> PlatformPaths;\n    \n    /// Get current system memory info (total, available, used)\n    fn memory_info(&self) -> Result<MemoryInfo, SbhError>;\n}\n```\n\n### Platform Implementations\n- **LinuxPlatform**: statvfs for fs stats, /proc/mounts for mounts, /proc/meminfo for memory, systemd for service\n- **MacosPlatform**: statvfs for fs stats, getmntinfo for mounts, sysctl for memory, launchd for service\n- **WindowsPlatform**: GetDiskFreeSpaceEx, volume enumeration, GlobalMemoryStatusEx, Windows Service API\n\n### Key Types\n```rust\npub struct FsStats {\n    pub total_bytes: u64,\n    pub free_bytes: u64,\n    pub available_bytes: u64,  // available to non-root users\n    pub fs_type: String,       // \"ext4\", \"tmpfs\", \"apfs\", \"ntfs\", etc.\n    pub mount_point: PathBuf,\n    pub is_readonly: bool,\n}\n\npub struct MountPoint {\n    pub path: PathBuf,\n    pub device: String,\n    pub fs_type: String,\n    pub is_ram_backed: bool,\n}\n\npub struct MemoryInfo {\n    pub total_bytes: u64,\n    pub available_bytes: u64,\n    pub swap_total_bytes: u64,\n    pub swap_free_bytes: u64,\n}\n```\n\n### Conditional Compilation\nUse `#[cfg(target_os = \"linux\")]`, `#[cfg(target_os = \"macos\")]`, `#[cfg(target_os = \"windows\")]` with a factory function `Platform::detect() -> Box<dyn Platform>`.\n\n## Design Rationale\nFollowing asupersync's reactor pattern: abstract the OS-specific bits behind a trait so the rest of the codebase is platform-agnostic. Linux is the primary target (where agent swarms run), macOS for development, Windows for completeness. The trait-based design also enables testing with a MockPlatform.\n\n## Acceptance Criteria\n- Compiles on Linux, macOS, and Windows (may need CI matrix)\n- LinuxPlatform correctly reads /proc/mounts and statvfs\n- MockPlatform allows deterministic testing\n- All FsStats fields populated correctly\n- is_ram_backed correctly identifies tmpfs, devshm, ramfs\n- Unit tests with mock platform","acceptance_criteria":"1. PAL APIs compile and behave consistently across Linux/macOS/Windows adapters. 2. Unit tests cover error mapping, permission failures, and platform edge cases with mock adapters. 3. Integration tests validate fs/process operations used by scanner, monitor, and daemon subsystems. 4. E2E smoke scripts verify core commands run correctly on each supported platform matrix target. 5. Structured platform logs include adapter, syscall result, normalized error code, and fallback path.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:44:33.564848054Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:09.592465840Z","closed_at":"2026-02-14T19:45:09.592443658Z","close_reason":"Platform abstraction layer implemented in platform/pal.rs (385 lines): Platform trait with fs_stats/mount_points/is_ram_backed/default_paths/memory_info/service_manager, LinuxPlatform with /proc/mounts parsing and statvfs, MockPlatform for testing, FsStats/MountPoint/MemoryInfo structs, unit tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"],"dependencies":[{"issue_id":"bd-sth","depends_on_id":"bd-1kn","type":"blocks","created_at":"2026-02-14T16:55:33.757771541Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-sth","depends_on_id":"bd-3uk","type":"blocks","created_at":"2026-02-14T16:55:33.839911101Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":19,"issue_id":"bd-sth","author":"Dicklesworthstone","text":"ENHANCEMENT (idea-wizard): Centralize filesystem type detection and strategy selection. Currently ad-hoc across beads (bd-25g uses fs_type for fallocate, bd-1w9 has cross-device checks). Add to Platform trait:\n  fn fs_strategy(&self, path: &Path) -> FsStrategy\nWhere FsStrategy encodes:\n  - ballast_method: Fallocate | RandomData | Skip (for tmpfs/NFS)\n  - deletion_method: Standard | CowAware (btrfs/zfs may not free space immediately)\n  - scan_strategy: Full | ReadOnly | Skip (NFS mounts: read-only scanning, skip deletion, warn user)\n  - size_estimation: StatBased | DuBased (CoW filesystems lie about size via stat)\nThis prevents each bead from independently querying fs_type.","created_at":"2026-02-14T18:35:03Z"},{"id":56,"issue_id":"bd-sth","author":"Dicklesworthstone","text":"REVIEW-2: Windows support is STUB ONLY for v1. WindowsPlatform implements the trait with NotImplemented returns for service_manager(). Core fs_stats() and mount_points() work via GetDiskFreeSpaceEx / GetVolumeInformation. Full Windows support is a future release target. #[cfg(target_os = 'windows')] compiles but methods return SbhError::NotSupported except for basic fs stats.","created_at":"2026-02-14T19:03:32Z"}]}
{"id":"bd-u92","title":"Multi-volume ballast pool coordinator","description":"## Deliverable\nSupport ballast files distributed across multiple volumes so each monitored filesystem has its own relief pool. Fixes the fundamental gap where ballast on /var can't help /data.\n\n## The Problem\nThe current design has a single ballast.location. On machines with multiple data volumes (/data, /scratch, /home), ballast on /var/lib/sbh/ballast can only relieve pressure on the root filesystem. If /data fills up, deleting ballast on /var does nothing. This breaks the ballast feature for the most common production setup.\n\n## The Solution: Per-Volume Ballast Pools\n\n### Architecture\nEach monitored filesystem gets its own ballast pool:\n- /data has ballast at /data/.sbh/ballast/\n- /scratch has ballast at /scratch/.sbh/ballast/\n- /home has ballast at /home/.sbh/ballast/\n- Root filesystem uses /var/lib/sbh/ballast/ (default)\n\n### Configuration\n```toml\n[ballast]\nauto_provision = true           # Auto-create pools on each monitored mount\nper_volume_file_count = 5       # Default files per volume\nper_volume_file_size_mb = 1024  # Default size per file\n\n# Override for specific mounts\n[ballast.overrides.\"/data\"]\nfile_count = 10\nfile_size_mb = 2048             # /data is large, bigger ballast\n\n[ballast.overrides.\"/scratch\"]\nfile_count = 3\nfile_size_mb = 512              # /scratch is small\n\n[ballast.overrides.\"/tmp\"]\nenabled = false                 # Never ballast tmpfs (defeats purpose)\n```\n\n### BallastPoolCoordinator\n```rust\npub struct BallastPoolCoordinator {\n    pools: HashMap<MountPoint, BallastPool>,\n}\n\npub struct BallastPool {\n    mount_point: PathBuf,\n    ballast_dir: PathBuf,          // e.g., /data/.sbh/ballast/\n    manager: BallastManager,       // existing manager, per-pool instance\n    fs_type: String,               // ext4, xfs, btrfs, etc.\n}\n\nimpl BallastPoolCoordinator {\n    /// Provision all pools (auto-detect from watched_paths + overrides)\n    pub fn provision_all(&mut self) -> Result<Vec<ProvisionReport>, SbhError>;\n    \n    /// Release ballast for a SPECIFIC filesystem under pressure\n    pub fn release_for_mount(&mut self, mount: &Path, count: usize) -> Result<ReleaseReport, SbhError>;\n    \n    /// Replenish a specific pool when its filesystem recovers\n    pub fn replenish_for_mount(&mut self, mount: &Path) -> Result<ProvisionReport, SbhError>;\n    \n    /// Get inventory across all pools\n    pub fn inventory(&self) -> Vec<PoolInventory>;\n    \n    /// Total releasable bytes across all pools\n    pub fn total_releasable(&self) -> u64;\n}\n```\n\n### Auto-Detection Logic\nOn startup:\n1. Enumerate all mount points from scanner.watched_paths\n2. Resolve each path to its mount point (e.g., /data/projects -> /data)\n3. Deduplicate by mount point\n4. For each unique mount: check if ballast pool exists, provision if needed\n5. Apply overrides from config\n6. Skip tmpfs/ramfs mounts (ballast on RAM defeats the purpose)\n7. Skip read-only mounts\n\n### Filesystem Type Awareness\nDifferent provisioning strategies per filesystem type:\n- ext4/xfs: use fallocate() for instant provisioning\n- btrfs/zfs: write random data (CoW prevents fallocate from reserving actual blocks)\n- tmpfs: skip entirely (warn in logs)\n- NFS: skip by default (warn: network ballast is unreliable)\n\n### Integration with Pressure Response\nWhen PID controller says ReleaseBallast for mount /data:\n1. Coordinator routes to /data's pool\n2. Releases N files from that specific pool\n3. Verifies space freed on /data (not on /var or anywhere else)\n4. If /data has no pool (e.g., not provisioned), log warning and skip to artifact scanning\n\n### sbh ballast status (enhanced)\n```\nVolume    │ Pool Location           │ Files │ Available │ Reclaimable\n/data     │ /data/.sbh/ballast      │ 10/10 │ 10        │ 20.0 GB\n/scratch  │ /scratch/.sbh/ballast   │ 3/3   │ 3         │ 1.5 GB\n/         │ /var/lib/sbh/ballast    │ 5/5   │ 5         │ 5.0 GB\n/tmp      │ (skipped: tmpfs)        │  -    │ -         │ -\nTotal:    │                         │ 18/18 │ 18        │ 26.5 GB\n```\n\n## Design Rationale\nThis is a genuine architectural gap. The single-volume ballast breaks on the most common production setup (separate data and system volumes). Agent swarms often use machines with /data on a large partition and / on a smaller root — ballast on root can't help when /data fills up. Implementation extends BallastManager from a single inventory to a HashMap<MountPoint, BallastPool> — significant but well-contained.\n\n## Acceptance Criteria\n- Each monitored filesystem gets its own ballast pool\n- Auto-detection correctly identifies mount points from watched_paths\n- Per-volume overrides work correctly\n- Filesystem type detection selects correct provisioning strategy\n- tmpfs and NFS mounts are skipped with warnings\n- Release targets the correct volume's pool\n- sbh ballast status shows per-volume inventory\n- Provisioning respects per-volume free space (don't provision if < 20% free)\n- Unit tests: pool coordination, mount point resolution, override merging\n- Integration test: two mock volumes -> provision both -> release on specific volume -> verify\n- Test: volume with no pool -> graceful fallback to artifact scanning","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T18:32:15.733168537Z","created_by":"ubuntu","updated_at":"2026-02-14T22:54:16.411247394Z","closed_at":"2026-02-14T22:54:16.411216887Z","close_reason":"Implemented BallastPoolCoordinator with per-volume pool management, auto-detection, config overrides, targeted release, and 12 comprehensive tests. All 36 ballast tests pass.","source_repo":".","compaction_level":0,"original_size":0,"labels":["ballast"],"dependencies":[{"issue_id":"bd-u92","depends_on_id":"bd-25g","type":"blocks","created_at":"2026-02-14T18:34:25.143389052Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-u92","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T18:34:25.221843249Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":39,"issue_id":"bd-u92","author":"Dicklesworthstone","text":"MINOR: Ballast file index design — each BallastPool (per-filesystem) maintains its OWN index of ballast files (Vec<BallastFile>). The BallastPoolCoordinator holds a HashMap<PathBuf, BallastPool> keyed by mount point. There is NO global ballast file index. This keeps pools independent and avoids cross-pool coordination complexity.","created_at":"2026-02-14T18:54:44Z"}]}
{"id":"bd-x1k","title":"EWMA disk usage rate estimator","description":"## Deliverable\nExponentially Weighted Moving Average estimator that tracks the RATE of disk consumption, enabling predictive intervention before storage actually runs out.\n\n## Technical Approach\n### Theory (Alien Graveyard: EWMA from time-series analysis)\nEWMA smooths noisy measurements while giving more weight to recent observations:\n  EWMA_t = α × x_t + (1 - α) × EWMA_{t-1}\nwhere α ∈ (0, 1) is the smoothing factor (higher = more reactive).\n\nFor sbh, we track:\n- bytes_consumed_per_second: how fast disk is being consumed\n- time_to_exhaustion: estimated seconds until disk is full at current rate\n- acceleration: is the consumption rate itself increasing? (second derivative)\n\n### Implementation\n```rust\npub struct DiskRateEstimator {\n    alpha: f64,                     // smoothing factor (default 0.3)\n    ewma_bytes_per_sec: f64,        // smoothed consumption rate\n    ewma_acceleration: f64,         // smoothed rate of rate change\n    last_free_bytes: u64,\n    last_sample_time: Instant,\n    samples_count: u64,\n    min_samples_for_prediction: u64, // need at least N samples (default 3)\n}\n\npub struct RateEstimate {\n    pub bytes_per_second: f64,       // current smoothed rate (positive = consuming)\n    pub seconds_to_exhaustion: f64,  // predicted time until 0% free\n    pub seconds_to_threshold: f64,   // predicted time until red threshold\n    pub acceleration: f64,           // rate change (positive = accelerating)\n    pub confidence: f64,             // 0.0-1.0 based on sample count and variance\n    pub trend: Trend,\n}\n\npub enum Trend {\n    Stable,        // consumption rate is constant\n    Accelerating,  // consumption rate is increasing (agent swarm ramping up)\n    Decelerating,  // consumption rate is decreasing (builds finishing)\n    Recovering,    // disk is being freed (negative consumption rate)\n}\n```\n\n### Prediction\ntime_to_exhaustion = free_bytes / bytes_per_second (when rate > 0)\nWith acceleration correction: t = (-v + sqrt(v² + 2*a*d)) / a (quadratic model)\nwhere v = current rate, a = acceleration, d = free bytes remaining\n\n### Adaptive Alpha\nWhen disk consumption is very bursty (e.g., cargo build starts), increase alpha temporarily to be more responsive. When stable, decrease alpha for smoother estimates.\n\n### Why This Matters\nWithout rate estimation, sbh can only react AFTER thresholds are crossed. With EWMA prediction, sbh can start preparing BEFORE the threshold is crossed:\n- If time_to_exhaustion < 5 minutes → start preemptive cleanup\n- If acceleration is positive → start monitoring more frequently\n- If trend is Recovering → reduce scan frequency to save CPU\n\n## Design Rationale\nEWMA is the right tool because: (1) disk consumption is noisy (builds start/stop), (2) we need recency-biased smoothing, (3) it's O(1) memory and O(1) per update, (4) the alpha parameter lets us tune reactivity. The acceleration term (second derivative) catches the \"10 agents started compiling simultaneously\" scenario before it fills the disk.\n\n## Acceptance Criteria\n- EWMA correctly smooths noisy input sequences\n- Predictions are reasonable for linear consumption patterns\n- Quadratic prediction handles acceleration correctly\n- Confidence increases with sample count\n- Trend detection correctly identifies all four states\n- Thread-safe (the monitoring loop updates this from a single thread, but readers may be concurrent)\n- Unit tests with synthetic consumption patterns","acceptance_criteria":"1. Unit tests validate EWMA smoothing, acceleration estimates, and confidence computation across synthetic traces. 2. Integration tests verify estimator outputs feed controller thresholds without instability. 3. E2E pressure timelines validate early-warning trigger quality against known workloads. 4. Estimator remains deterministic for fixed sample streams and alpha settings. 5. Detailed estimator logs include raw samples, EWMA value, derivative estimates, confidence, and trend state.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-14T16:45:29.069195094Z","created_by":"ubuntu","updated_at":"2026-02-14T19:45:22.110276563Z","closed_at":"2026-02-14T19:45:22.110251165Z","close_reason":"EWMA rate estimator implemented in monitor/ewma.rs (255 lines): DiskRateEstimator with adaptive alpha, EWMA rate/acceleration/confidence, Trend classification (Stable/Accelerating/Decelerating/Recovering), quadratic projection with linear fallback, unit tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-graveyard","monitoring"],"dependencies":[{"issue_id":"bd-x1k","depends_on_id":"bd-2pj","type":"blocks","created_at":"2026-02-14T16:55:39.336766482Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-x9z","title":"Multi-factor deterministic candidacy scoring engine","description":"The scoring engine that computes a file_deletion_candidacy_score for every discovered artifact, combining multiple independent factors into a single deterministic score with hard safety vetoes.\n\n## CRITICAL CONTRACT: Pure Computation, Zero I/O\nThe scoring engine is a PURE FUNCTION. Given identical inputs, it produces identical outputs. It NEVER performs filesystem I/O, network calls, or any side effects. All data it needs is provided by callers:\n- Path + metadata → from DirectoryWalker (bd-1w9)\n- Artifact classification → from PatternRegistry (bd-1sw)\n- pressure_multiplier → from PID controller output (bd-3po), passed in by main loop\n- Structural markers → pre-collected by walker as part of WalkEntry.structural_markers: Vec<String>\n\nThis means the walker must collect structural markers (checking for subdirs like incremental/, deps/, .fingerprint/) during traversal, and attach them to WalkEntry. The scoring engine then uses these pre-collected markers without touching the filesystem.\n\n## Scoring Formula (Alien Artifact: information-theoretic multi-factor scoring)\n\n```\ncandidacy_score = pressure_multiplier × Σ(weight_i × factor_i)\n```\n\nWith hard vetoes that force score to 0.0 regardless of other factors.\n\n## Factor Functions (each normalized to [0.0, 1.0])\n\n**f_location(path)**: Location safety factor\n```\n/tmp, /data/tmp, /dev/shm     → 0.95  (temp dirs, very safe)\n/data/projects/*/target        → 0.80  (build output, safe)\n/data/projects/*/.target*      → 0.85  (hidden build output, safe)\n/data/projects/*/.tmp_*        → 0.90  (explicitly temporary)\n~/.cache/*                     → 0.60  (caches, usually safe)\n/home/*/projects/*             → 0.40  (user projects, moderate risk)\n/home/*/Documents/*            → 0.10  (user data, high risk)\n/ system directories           → 0.00  (never touch)\n```\n\n**f_name(entry, classification)**: Name/type classification factor\n```\nclassification.combined_confidence directly (0.0-1.0)\nBoost +0.1 if name contains \"tmp\", \"temp\", \"cache\"\nBoost +0.15 if classification category is RustTarget\nPenalty -0.3 if name contains \"backup\", \"save\", \"important\"\n```\n\n**f_age(modified_time, now)**: Age sweet-spot factor (bell curve, not linear)\nNote: \"now\" is passed explicitly for determinism (never call SystemTime::now())\n```\n< 30 minutes      → 0.00  (too recent, may be in active use)\n30 min - 2 hours   → 0.20  (probably still in use)\n2 - 4 hours        → 0.70  (cooling off)\n4 - 10 hours       → 1.00  (sweet spot: old enough to be stale)\n10 - 24 hours      → 0.85  (definitely stale)\n1 - 7 days         → 0.60  (old, but user might remember it)\n7 - 30 days        → 0.40  (very old, ambiguous)\n> 30 days          → 0.25  (ancient - might be intentional)\n```\n\n**f_size(bytes)**: Size-efficiency factor (prefer deleting large items)\n```\n< 1 MB             → 0.05  (not worth the syscall overhead)\n1 - 10 MB          → 0.20\n10 - 100 MB        → 0.40\n100 MB - 1 GB      → 0.70\n1 - 10 GB          → 1.00  (maximum value - huge space recovery)\n10 - 50 GB         → 0.90  (enormous, slight extra caution)\n> 50 GB            → 0.75  (suspiciously large, extra caution)\n```\n\n**f_structure(structural_markers)**: Internal structure indicators\nUses pre-collected structural_markers from WalkEntry (NO filesystem I/O here):\n```\nContains .fingerprint/ or incremental/  → 0.95  (definitely build artifacts)\nContains deps/ + build/                 → 0.85  (very likely build artifacts)\nContains .git/                          → 0.00  (VETO: this is a repo!)\nContains Cargo.toml at root             → 0.05  (this is a project, not a target)\nContains only .d, .o, .rlib files       → 0.90  (build outputs)\n```\n\n## Weights\nDefault weights (configurable):\n```\nw_location  = 0.25\nw_name      = 0.25\nw_age       = 0.20\nw_size      = 0.15\nw_structure = 0.15\n```\n\n## Pressure Multiplier\nFrom PID controller urgency (0.0-1.0), passed in by caller:\n```\nurgency 0.0    → multiplier 1.0  (normal scoring)\nurgency 0.3    → multiplier 1.3  (slightly lower bar)\nurgency 0.5    → multiplier 1.5  (moderate pressure)\nurgency 0.8    → multiplier 2.0  (aggressive)\nurgency 1.0    → multiplier 3.0  (emergency - almost anything goes)\n```\n\n## Hard Vetoes (override score to 0.0)\nThese CANNOT be overridden even under maximum pressure:\n1. structural_markers contains \".git\" → VETO (never delete repos)\n2. Path matches system directory pattern → VETO\n3. File age < min_file_age_minutes (default 30) → VETO (age computed from passed-in \"now\")\n4. Path matches any user-configured exclusion → VETO\n5. File is currently open by a process → VETO (this is the ONE check that requires I/O — but it's done by the CALLER before invoking the scoring engine, and passed as an `is_open: bool` flag)\n\n## Implementation\n```rust\npub struct ScoringEngine {\n    weights: ScoringWeights,\n    pattern_registry: ArtifactPatternRegistry,\n    excluded_paths: Vec<PathBuf>,\n    min_file_age: Duration,\n}\n\n/// All inputs needed to score a single candidate. No I/O needed.\npub struct ScoringInput {\n    pub path: PathBuf,\n    pub size_bytes: u64,\n    pub modified_at: SystemTime,\n    pub structural_markers: Vec<String>,  // pre-collected by walker\n    pub is_open: bool,                     // pre-checked by caller\n    pub now: SystemTime,                   // explicit timestamp for determinism\n    pub pressure_urgency: f64,             // from PID controller\n}\n\npub struct CandidacyScore {\n    pub path: PathBuf,\n    pub total_score: f64,\n    pub factors: ScoreFactors,\n    pub vetoed: bool,\n    pub veto_reason: Option<String>,\n    pub classification: ArtifactClassification,\n    pub size_bytes: u64,\n    pub age: Duration,\n}\n```\n\n## Determinism\nGiven the same ScoringInput, the scoring engine MUST produce identical CandidacyScore. No randomness, no implicit time-dependency (all time-dependent computations use the passed-in `now` field). This enables testing and reproducibility.\n\n\n## Acceptance Criteria\n- Deterministic scoring and ordering for identical inputs\n- Hard vetoes always dominate computed score\n- Pressure scaling monotonic with urgency\n- Boundary-value tests for all factors\n- Pure-computation contract (no I/O in scorer)\n- Integration ranking test with realistic fixtures","acceptance_criteria":"1. Deterministic scoring: identical ScoringInput yields identical CandidacyScore and ordering. 2. Hard vetoes always override score for git/system/excluded/recent/open-file paths. 3. Pressure multiplier mapping is monotonic in urgency. 4. Factor boundary values are unit-tested. 5. Pure-function contract holds: no filesystem or network I/O inside scoring engine. 6. Explainability fields are always populated for decision traces. 7. Integration test validates realistic ranking and veto behavior.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-14T16:48:21.317243458Z","created_by":"ubuntu","updated_at":"2026-02-14T19:46:14.739828846Z","closed_at":"2026-02-14T19:46:14.739801955Z","close_reason":"Implemented in scanner/scoring.rs","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","scanner"],"dependencies":[{"issue_id":"bd-x9z","depends_on_id":"bd-1sw","type":"blocks","created_at":"2026-02-14T16:55:49.512159142Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":1,"issue_id":"bd-x9z","author":"Dicklesworthstone","text":"REVIEW: Removed dependency on bd-2pj (FsStatsCollector). The scoring engine is a PURE COMPUTATION — it takes pre-collected metadata (from walker) and a pressure_multiplier (from PID controller) as inputs. It should never call filesystem APIs directly. This was an erroneous dependency that added unnecessary critical path length. The pressure_multiplier the scoring engine needs comes from the PID controller's output, which the main loop passes in when invoking scoring.","created_at":"2026-02-14T17:11:18Z"},{"id":47,"issue_id":"bd-x9z","author":"Dicklesworthstone","text":"REVIEW-2: Removed bd-1w9 dependency. The scoring engine is pure computation with zero I/O. It takes ScoringInput struct populated by the caller. The walker PROVIDES data but the scorer does not import walker code. Data contract: walker produces WalkEntry with structural_markers, caller constructs ScoringInput adding pressure_urgency + is_open + now timestamp.","created_at":"2026-02-14T19:02:52Z"}]}
